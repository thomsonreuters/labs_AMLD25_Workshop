{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Exercise 2: Efficient Training with Resource Constraints and Capability Preservation üöÄ\n## üìò Prerequisites\nFrom Exercise 1, we learned how to create high-quality training sets using smart sampling techniques. Now we'll build upon this knowledge while considering resource constraints and model capabilities preservation.\n\n## üéØ New Challenges\n### 1. Capability Preservation\nEven with high-quality data, models can \"forget\" previously learned capabilities during fine-tuning. We need to:\n\n* Maintain general knowledge\n* Preserve essential capabilities\n* Balance new and existing skills\n\nGenerally, this is done by introducing different collections, and in this case, we will add a well-known collection for generic knowledge to our preference collection\n\n```\ntrl-lib/ultrafeedback_binarized\n```\n\n### 2. Resource Optimization\nTraining on full datasets, even high-quality ones, can be impractical due to:\n\n* Workshop time constraints\n* Cloud computing costs\n* In an enterprise environment, respect the development cycle duration","metadata":{}},{"cell_type":"markdown","source":"### Git Clone","metadata":{}},{"cell_type":"code","source":"! git clone https://github.com/thomsonreuters/labs_AMLD25_Workshop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:49:15.639088Z","iopub.execute_input":"2025-02-17T09:49:15.639373Z","iopub.status.idle":"2025-02-17T09:49:16.684595Z","shell.execute_reply.started":"2025-02-17T09:49:15.639341Z","shell.execute_reply":"2025-02-17T09:49:16.683742Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'labs_AMLD25_Workshop'...\nremote: Enumerating objects: 59, done.\u001b[K\nremote: Counting objects: 100% (59/59), done.\u001b[K\nremote: Compressing objects: 100% (45/45), done.\u001b[K\nremote: Total 59 (delta 8), reused 56 (delta 8), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (59/59), 3.92 MiB | 28.43 MiB/s, done.\nResolving deltas: 100% (8/8), done.\n","output_type":"stream"}],"execution_count":1},{"cell_type":"markdown","source":"### Install dependencies","metadata":{}},{"cell_type":"code","source":"! pip install -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt\n! pip install flash-attn==2.7.3 --no-build-isolation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:49:18.474380Z","iopub.execute_input":"2025-02-17T09:49:18.474774Z","iopub.status.idle":"2025-02-17T09:49:55.267468Z","shell.execute_reply.started":"2025-02-17T09:49:18.474738Z","shell.execute_reply":"2025-02-17T09:49:55.266401Z"}},"outputs":[{"name":"stdout","text":"Collecting accelerate==1.3.0 (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1))\n  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\nCollecting bitsandbytes==0.45.1 (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 2))\n  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\nRequirement already satisfied: datasets==3.2.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: peft==0.14.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 4)) (0.14.0)\nCollecting transformers==4.48.1 (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 5))\n  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting trl==0.13.0 (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 6))\n  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (0.28.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.11.11)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.1->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 5)) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.1->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 5)) (0.21.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl==0.13.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 6)) (13.9.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2025.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl==0.13.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 6)) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl==0.13.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 6)) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.13.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 6)) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2024.2.0)\nDownloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m10.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m105.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.13.0-py3-none-any.whl (293 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m16.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: transformers, accelerate, trl, bitsandbytes\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.2.1\n    Uninstalling accelerate-1.2.1:\n      Successfully uninstalled accelerate-1.2.1\nSuccessfully installed accelerate-1.3.0 bitsandbytes-0.45.1 transformers-4.48.1 trl-0.13.0\nCollecting flash-attn==2.7.3\n  Downloading flash_attn-2.7.3.tar.gz (3.2 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m48.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.7.3) (2.5.1+cu121)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.7.3) (0.8.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn==2.7.3) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn==2.7.3) (3.0.2)\nBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flash-attn: filename=flash_attn-2.7.3-cp310-cp310-linux_x86_64.whl size=191342964 sha256=cd0ead6ad92bff21f90cf0aecdf859bc7145bf4e7dc8ed1fdc3cc38299651ce4\n  Stored in directory: /root/.cache/pip/wheels/85/d7/10/a74c9fe5ffe6ff306b27a220b2bf2f37d907b68fdcd138cdda\nSuccessfully built flash-attn\nInstalling collected packages: flash-attn\nSuccessfully installed flash-attn-2.7.3\n","output_type":"stream"}],"execution_count":2},{"cell_type":"markdown","source":"### Testing GPU\nPlease check if python recognize that you have GPU allocated, if not please go in `Settings`>`Accelerator`>`GPU T4 x 2` ","metadata":{}},{"cell_type":"code","source":"import os, sys\n\n# from tensorflow.python.client import device_lib\nrepo_folder = os.getcwd().split('labs_AMLD25_Workshop')[0]+\"/labs_AMLD25_Workshop/src\" \nsys.path.append(repo_folder)\n\n# UNCOMMENT TO CHECK GPU HW\n# device_lib.list_local_devices()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:49:55.268700Z","iopub.execute_input":"2025-02-17T09:49:55.268940Z","iopub.status.idle":"2025-02-17T09:49:55.273241Z","shell.execute_reply.started":"2025-02-17T09:49:55.268919Z","shell.execute_reply":"2025-02-17T09:49:55.272357Z"}},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":"if you get two GPUs you can manually assign them using env variables. This step is optional since they should be automatically recognized by pytorch ","metadata":{}},{"cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\" ## turning off WandB logging\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n\nrl_foolder = \"labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:49:55.274666Z","iopub.execute_input":"2025-02-17T09:49:55.274940Z","iopub.status.idle":"2025-02-17T09:49:55.296230Z","shell.execute_reply.started":"2025-02-17T09:49:55.274920Z","shell.execute_reply":"2025-02-17T09:49:55.295461Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"import torch\n\nfrom typing import Optional, List, Dict\nimport datasets\nfrom datasets import (\n    load_dataset, \n    load_from_disk, \n    DatasetDict,\n    concatenate_datasets\n)\n\nfrom accelerate import Accelerator, PartialState\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom trl import (\n    ModelConfig,\n    DPOTrainer,\n    DPOConfig,\n    TrlParser,\n    get_kbit_device_map,\n    get_peft_config,\n    get_quantization_config,\n)\n\nfrom trlabs.rl.data import (\n    get_datasets, \n    DataArguments\n)\n\nfrom trlabs.utils import *\n\nfrom trl.trainer.utils import SIMPLE_CHAT_TEMPLATE","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:49:55.297343Z","iopub.execute_input":"2025-02-17T09:49:55.297591Z","iopub.status.idle":"2025-02-17T09:50:18.723015Z","shell.execute_reply.started":"2025-02-17T09:49:55.297571Z","shell.execute_reply":"2025-02-17T09:50:18.722350Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":5},{"cell_type":"markdown","source":"### Model Config","metadata":{}},{"cell_type":"code","source":"model_config = {\n    \"model_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n    \"torch_dtype\": \"bfloat16\",\n    \"use_peft\": True, \n    \"lora_r\": 64,        \n    \"lora_alpha\": 32,    # Stronger updates\n    \"lora_dropout\": 0.1, # Prevent overfitting\n}\n","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:50:18.723841Z","iopub.execute_input":"2025-02-17T09:50:18.724156Z","iopub.status.idle":"2025-02-17T09:50:18.728008Z","shell.execute_reply.started":"2025-02-17T09:50:18.724125Z","shell.execute_reply":"2025-02-17T09:50:18.727011Z"}},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":"### Data Config\nYou can also leverage the preference dataset located in HF Hub `trl-lib/ultrafeedback_binarized`. ","metadata":{}},{"cell_type":"code","source":"data_params = {\n  \"dataset_name\": \"Mix 2\",\n  \"dataset_mixer\": {\n    \"trl-lib/ultrafeedback_binarized\": 0.02, ## 1220 samples over 61000\n     f\"{rl_foolder}/data/AMLD25_reuters_gentitle_1k\": 1.,\n  },\n  \"dataset_splits\": [\"train\", \"test\"],\n  \"num_eval_samples\": 100,\n  \"seed\": 42\n}","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:50:18.729013Z","iopub.execute_input":"2025-02-17T09:50:18.729353Z","iopub.status.idle":"2025-02-17T09:50:18.776912Z","shell.execute_reply.started":"2025-02-17T09:50:18.729324Z","shell.execute_reply":"2025-02-17T09:50:18.776329Z"}},"outputs":[],"execution_count":7},{"cell_type":"markdown","source":"### Training Config","metadata":{}},{"cell_type":"code","source":"training_params =  {\n    ## General\n    \"output_dir\": f\"{model_config['model_name_or_path'].split('/')[0].lower()}_ex2_output\",\n     \"num_train_epochs\": 1,\n    \"beta\": 0.1,\n    \"eval_strategy\": \"steps\",\n    \"eval_steps\": 8,\n    \"per_device_train_batch_size\": 1,\n    \"per_device_eval_batch_size\": 1,\n    \"gradient_accumulation_steps\": 8,\n    #@ context length and max length (max_new_token = max_length - max_prompt_length)\n    \"max_length\": 768,\n    \"max_prompt_length\":512,\n    ## Optimizer\n    \"optim\": \"adamw_torch\",\n    \"learning_rate\": 2.0e-4,\n    \"weight_decay\": 0.001,\n    \"adam_epsilon\": 1.0e-8,\n    \"adam_beta1\": 0.9,\n    \"adam_beta2\": 0.999,\n    \"max_grad_norm\": 1.0,\n    ## Scheduler ##\n    \"warmup_steps\": 10,\n    \"lr_scheduler_type\": \"cosine\",\n    ## Logging\n    \"log_level\": \"info\",\n    \"logging_first_step\": True,\n    \"logging_steps\": 10\n}","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:52:45.953236Z","iopub.execute_input":"2025-02-17T09:52:45.953623Z","iopub.status.idle":"2025-02-17T09:52:45.958830Z","shell.execute_reply.started":"2025-02-17T09:52:45.953596Z","shell.execute_reply":"2025-02-17T09:52:45.958116Z"}},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":"## DPO Training Loop\n\nIf you launch the training as it is set up you will see that it will take about 1 hour!!! \n\nYou can get the same results in much less time. \n\n### Let's Play with our two collection!\nNow we have two collections, choosing **X** and **Y** fractions to balance the resulting contribution. But before you do that create collections with more smart sampling\n\n```python\ndata_params = {\n  \"dataset_name\": \"Mix 2\",\n  \"dataset_mixer\": {\n    \"trl-lib/ultrafeedback_binarized\": Y,\n    \"labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/data/AMLD25_reuters_gentitle_1k\": X,\n  },\n  \"dataset_splits\": [\"train\", \"test\"],\n  \"num_eval_samples\": 100,\n  \"seed\": 42\n}\n```\n\nIn order to achive the best performance. \n\n\n\n\n##### Note:\nA fraction of 0.1 for collection`\"trl-lib/ultrafeedback_binarized\"` means that we select 10% of the training size from this collection","metadata":{}},{"cell_type":"code","source":"from trlabs.rl.train import dpo\n\ndpo(data_params, training_params, model_config)","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:52:50.288099Z","iopub.execute_input":"2025-02-17T09:52:50.288411Z","iopub.status.idle":"2025-02-17T11:01:22.708268Z","shell.execute_reply.started":"2025-02-17T09:52:50.288388Z","shell.execute_reply":"2025-02-17T11:01:22.707528Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"494a9199712e46509cf9f78fbad8aa51"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fb6d1645645940e8aa5ffb0a99168c71"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dbc09bc459014a6f8ec8db6bb2be798f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11c9086b2b8b432eb978537684013d01"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"07cc83bafabf45b9bad548b9aa0dd008"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3f772ff88e1c4e36a3a3f547ca4e9c64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cc5571b99424858a5aa9c627b07fbf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/643 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"436d762bcfa3497eac236888c5457129"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"train-00000-of-00001.parquet:   0%|          | 0.00/131M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5dffe8289ca34813a36e277e61f14460"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"test-00000-of-00001.parquet:   0%|          | 0.00/2.14M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"27bc41772a6944138d9b4b88d7a80fa7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/62135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2b0b16a0be874d78af61a4614a5941af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2d20440104cb41f1a0efa975f448b36d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting prompt from train dataset:   0%|          | 0/2229 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8a7a7dd34bf14698b7055a2dc2a84fba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/2229 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"97dfce58ddea46d1997646d80e844080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting prompt from eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"08c338caabd841998d742bcdac31cafc"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2e063e1478f542fc9faf2b5db2e4486d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/2229 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6af455fd583c4816b21abbeb06a9be64"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85ce199c6a5e452db84c6d2768086863"}},"metadata":{}},{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 2,229\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Training with DataParallel so batch size has been adjusted to: 2\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 8\n  Total optimization steps = 139\n  Number of trainable parameters = 4,325,376\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='139' max='139' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [139/139 1:05:58, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rewards/chosen</th>\n      <th>Rewards/rejected</th>\n      <th>Rewards/accuracies</th>\n      <th>Rewards/margins</th>\n      <th>Logps/chosen</th>\n      <th>Logps/rejected</th>\n      <th>Logits/chosen</th>\n      <th>Logits/rejected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>8</td>\n      <td>5.531200</td>\n      <td>0.680938</td>\n      <td>0.005524</td>\n      <td>-0.024902</td>\n      <td>0.430000</td>\n      <td>0.030396</td>\n      <td>-256.000000</td>\n      <td>-233.000000</td>\n      <td>-2.953125</td>\n      <td>-3.031250</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>5.615000</td>\n      <td>0.651992</td>\n      <td>-0.096680</td>\n      <td>-0.207031</td>\n      <td>0.570000</td>\n      <td>0.110840</td>\n      <td>-258.000000</td>\n      <td>-235.000000</td>\n      <td>-2.937500</td>\n      <td>-3.000000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>5.277300</td>\n      <td>0.641797</td>\n      <td>-0.220703</td>\n      <td>-0.388672</td>\n      <td>0.660000</td>\n      <td>0.166992</td>\n      <td>-258.000000</td>\n      <td>-236.000000</td>\n      <td>-2.953125</td>\n      <td>-3.015625</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>5.219300</td>\n      <td>0.615547</td>\n      <td>-0.245117</td>\n      <td>-0.503906</td>\n      <td>0.660000</td>\n      <td>0.257812</td>\n      <td>-258.000000</td>\n      <td>-238.000000</td>\n      <td>-2.937500</td>\n      <td>-3.000000</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.040500</td>\n      <td>0.620625</td>\n      <td>-0.196289</td>\n      <td>-0.476562</td>\n      <td>0.650000</td>\n      <td>0.281250</td>\n      <td>-258.000000</td>\n      <td>-237.000000</td>\n      <td>-2.937500</td>\n      <td>-3.015625</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>5.040500</td>\n      <td>0.609453</td>\n      <td>-0.198242</td>\n      <td>-0.527344</td>\n      <td>0.660000</td>\n      <td>0.332031</td>\n      <td>-258.000000</td>\n      <td>-238.000000</td>\n      <td>-2.937500</td>\n      <td>-3.015625</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>4.873800</td>\n      <td>0.601562</td>\n      <td>-0.178711</td>\n      <td>-0.531250</td>\n      <td>0.640000</td>\n      <td>0.353516</td>\n      <td>-258.000000</td>\n      <td>-238.000000</td>\n      <td>-2.968750</td>\n      <td>-3.031250</td>\n    </tr>\n    <tr>\n      <td>64</td>\n      <td>4.735400</td>\n      <td>0.603516</td>\n      <td>-0.127930</td>\n      <td>-0.507812</td>\n      <td>0.640000</td>\n      <td>0.376953</td>\n      <td>-258.000000</td>\n      <td>-238.000000</td>\n      <td>-2.968750</td>\n      <td>-3.031250</td>\n    </tr>\n    <tr>\n      <td>72</td>\n      <td>4.729500</td>\n      <td>0.589590</td>\n      <td>-0.255859</td>\n      <td>-0.679688</td>\n      <td>0.660000</td>\n      <td>0.425781</td>\n      <td>-260.000000</td>\n      <td>-239.000000</td>\n      <td>-2.968750</td>\n      <td>-3.031250</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>5.018300</td>\n      <td>0.593574</td>\n      <td>-0.365234</td>\n      <td>-0.769531</td>\n      <td>0.660000</td>\n      <td>0.406250</td>\n      <td>-260.000000</td>\n      <td>-240.000000</td>\n      <td>-2.968750</td>\n      <td>-3.046875</td>\n    </tr>\n    <tr>\n      <td>88</td>\n      <td>5.018300</td>\n      <td>0.588008</td>\n      <td>-0.253906</td>\n      <td>-0.660156</td>\n      <td>0.650000</td>\n      <td>0.408203</td>\n      <td>-260.000000</td>\n      <td>-239.000000</td>\n      <td>-2.984375</td>\n      <td>-3.046875</td>\n    </tr>\n    <tr>\n      <td>96</td>\n      <td>5.175400</td>\n      <td>0.600020</td>\n      <td>-0.182617</td>\n      <td>-0.558594</td>\n      <td>0.600000</td>\n      <td>0.375000</td>\n      <td>-258.000000</td>\n      <td>-238.000000</td>\n      <td>-2.968750</td>\n      <td>-3.031250</td>\n    </tr>\n    <tr>\n      <td>104</td>\n      <td>4.594200</td>\n      <td>0.616445</td>\n      <td>-0.228516</td>\n      <td>-0.574219</td>\n      <td>0.610000</td>\n      <td>0.345703</td>\n      <td>-258.000000</td>\n      <td>-238.000000</td>\n      <td>-2.953125</td>\n      <td>-3.015625</td>\n    </tr>\n    <tr>\n      <td>112</td>\n      <td>4.293000</td>\n      <td>0.617773</td>\n      <td>-0.236328</td>\n      <td>-0.601562</td>\n      <td>0.630000</td>\n      <td>0.363281</td>\n      <td>-258.000000</td>\n      <td>-239.000000</td>\n      <td>-2.953125</td>\n      <td>-3.015625</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>4.906800</td>\n      <td>0.599023</td>\n      <td>-0.226562</td>\n      <td>-0.617188</td>\n      <td>0.620000</td>\n      <td>0.388672</td>\n      <td>-258.000000</td>\n      <td>-239.000000</td>\n      <td>-2.953125</td>\n      <td>-3.015625</td>\n    </tr>\n    <tr>\n      <td>128</td>\n      <td>4.906800</td>\n      <td>0.607344</td>\n      <td>-0.273438</td>\n      <td>-0.652344</td>\n      <td>0.610000</td>\n      <td>0.378906</td>\n      <td>-260.000000</td>\n      <td>-239.000000</td>\n      <td>-2.953125</td>\n      <td>-3.015625</td>\n    </tr>\n    <tr>\n      <td>136</td>\n      <td>4.379200</td>\n      <td>0.598516</td>\n      <td>-0.269531</td>\n      <td>-0.664062</td>\n      <td>0.650000</td>\n      <td>0.394531</td>\n      <td>-260.000000</td>\n      <td>-239.000000</td>\n      <td>-2.953125</td>\n      <td>-3.015625</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nSaving model checkpoint to qwen_ex2_output/checkpoint-139\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\ntokenizer config file saved in qwen_ex2_output/checkpoint-139/tokenizer_config.json\nSpecial tokens file saved in qwen_ex2_output/checkpoint-139/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 01:20]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to qwen_ex2_output\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n","output_type":"stream"},{"name":"stdout","text":"***** eval metrics *****\n  epoch                   =     0.9973\n  eval_logits/chosen      =    -2.9531\n  eval_logits/rejected    =    -3.0156\n  eval_logps/chosen       =     -258.0\n  eval_logps/rejected     =     -239.0\n  eval_loss               =     0.6063\n  eval_rewards/accuracies =       0.62\n  eval_rewards/chosen     =    -0.2539\n  eval_rewards/margins    =     0.3906\n  eval_rewards/rejected   =    -0.6445\n  eval_runtime            = 0:01:23.27\n  eval_samples_per_second =      1.201\n  eval_steps_per_second   =        0.6\n","output_type":"stream"},{"name":"stderr","text":"tokenizer config file saved in qwen_ex2_output/tokenizer_config.json\nSpecial tokens file saved in qwen_ex2_output/special_tokens_map.json\n","output_type":"stream"}],"execution_count":9},{"cell_type":"markdown","source":"## Give a look to the Model Generation","metadata":{}},{"cell_type":"code","source":"from trlabs.utils import dataset_creation, not_relevant_data\n\nSYSTEM_PROMPT = 'You are an advanced AI system specialised in providing Reuters News title given a body text of the news.'\nINSTRUCTION = \"The title should be in capital letters and between 6 and 8 words in length. Please provide only the title as output and no other text or explanation.\"\n\ndataset = load_dataset(\"ucirvine/reuters21578\", 'ModApte', trust_remote_code=True)\ndataset = dataset.filter(not_relevant_data).shuffle(seed=42).map(dataset_creation, fn_kwargs={\"system_prompt\": SYSTEM_PROMPT, \"instruction\": INSTRUCTION})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:01:22.709539Z","iopub.execute_input":"2025-02-17T11:01:22.709858Z","iopub.status.idle":"2025-02-17T11:01:31.652714Z","shell.execute_reply.started":"2025-02-17T11:01:22.709835Z","shell.execute_reply":"2025-02-17T11:01:31.651535Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/16.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f0fed78b990944589f09b6ba05468f1d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"reuters21578.py:   0%|          | 0.00/17.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6ce2a160568546999e03d6f822a9ed70"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"reuters21578.tar.gz:   0%|          | 0.00/8.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eea3eda76b1c4678bb1d5182182dfa2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3299 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f9e8970c07e44c196928838a3fe2348"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd288cb26ffd41c89cb817565a2d1cf7"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unused split:   0%|          | 0/722 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b44d39afc7134883bbdfa21c535b8b3f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/3299 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0a4bba8e31b44575aa6292e93c4ddaf8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/9603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"15a04758c63b46be8cdbc3d04ad59232"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/722 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f632844e96a9423db9e752b3011e3627"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3295 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4417ebb24a994fc3b160bd85882f7c4b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b193554a61fb4587930304b188d1b8a4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/722 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4ee760aa0ae743f0b161fcc1d94e36e6"}},"metadata":{}}],"execution_count":10},{"cell_type":"code","source":"from trlabs.rl.eval import setup_model_and_lora, generate\n\nindex =33\nprompt = dataset[\"test\"][index][\"system\"]+dataset[\"test\"][index][\"messages\"]\n\nmodel, tokenizer = setup_model_and_lora(\n    base_model_name = model_config[\"model_name_or_path\"], \n    lora_path = training_params[\"output_dir\"]\n)\n\nresponse = generate(prompt, model, tokenizer)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:01:31.654303Z","iopub.execute_input":"2025-02-17T11:01:31.654530Z","iopub.status.idle":"2025-02-17T11:01:36.471959Z","shell.execute_reply.started":"2025-02-17T11:01:31.654510Z","shell.execute_reply":"2025-02-17T11:01:36.471080Z"}},"outputs":[{"name":"stderr","text":"loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/merges.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/model.safetensors\nInstantiating Qwen2ForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\nAll model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.1,\n  \"temperature\": 0.7,\n  \"top_k\": 20,\n  \"top_p\": 0.8\n}\n\nloading file vocab.json\nloading file merges.txt\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading file chat_template.jinja\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nYou are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 151646. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"system\nYou are an advanced AI system specialised in providing Reuters News title given a body text of the news.\nuser\nCanada Development Corp <CDC.TO> said it agreed to sell its 25.2 pct interest in CDC Life Sciences Inc to Caisse de depot et placement du Quebec, the provincial pension fund manager, and Institut Merieux, a French biological laboratory company, for 169.2 mln dlrs. It said the caisse and Institut Merieux will each buy 2.75 mln common shares of the company for 30.76 dlrs a share. It said following the transaction the caisse will hold about 19.3 pct of CDC Life Sciences. Canada Development said the purchasers do not plan to acquire the remaining publicly-held shares.\n\nThe title should be in capital letters and between 6 and 8 words in length. Please provide only the title as output and no other text or explanation.\nassistant\n\"Canada Development Corp Sells 25.2% Interest in CDC Life Sciences to Caisse de Depot & Placement du Qu√©bec, Institut Merieux\"\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"#### Note: \nif you do not provide a lora_path you can check the base model output","metadata":{}},{"cell_type":"markdown","source":"## Solution","metadata":{}},{"cell_type":"code","source":"from trlabs.utils import *\n\ndataset = load_from_disk(\"/kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/data/AMLD25_reuters_gentitle_1k\").filter(reuters_cleaning_dataset)\ndataset.save_to_disk(\"AMLD25_reuters_gentitle_0.5k_cleaned\")\n\ndataset = load_dataset(\"trl-lib/ultrafeedback_binarized\").filter(ultrafeedback_cleaning_dataset)\ndataset.save_to_disk(\"trl-lib/ultrafeedback_binarized-cleaned_11k\")\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:01:36.473258Z","iopub.execute_input":"2025-02-17T11:01:36.473512Z","iopub.status.idle":"2025-02-17T11:01:40.089442Z","shell.execute_reply.started":"2025-02-17T11:01:36.473491Z","shell.execute_reply":"2025-02-17T11:01:40.088538Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/987 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1a8915899e2a498d87db0419a912b0dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/496 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"63c4ea27b8b947748a3e8eacb6fcd7bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/446 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"969b7a974ced479c9a2383b7587ed935"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/196 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6c55ceccf0e64291bd89161aa1ef7df1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/62135 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"23d4befb7a4547458db2bf8f5547e994"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"40a948c5895145bb9df012fa81250e6d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/11329 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e0071db53a4c4617ab4e330b8aad9e2f"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/188 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb6d44cc19ba4708a8daa369adff94be"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"data_params = {\n  \"dataset_name\": \"Mix 2\",\n  \"dataset_mixer\": {\n    \"trl-lib/ultrafeedback_binarized-cleaned_11k\": 0.05,\n    f\"AMLD25_reuters_gentitle_0.5k_cleaned\": 1.,\n  },\n  \"dataset_splits\": [\"train\", \"test\"],\n  \"num_eval_samples\": 100,\n  \"seed\": 42\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:01:40.090185Z","iopub.execute_input":"2025-02-17T11:01:40.090401Z","iopub.status.idle":"2025-02-17T11:01:40.094099Z","shell.execute_reply.started":"2025-02-17T11:01:40.090382Z","shell.execute_reply":"2025-02-17T11:01:40.093390Z"}},"outputs":[],"execution_count":13},{"cell_type":"code","source":"from trlabs.rl.train import dpo\n\ndpo(data_params, training_params, model_config)","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:01:40.094715Z","iopub.execute_input":"2025-02-17T11:01:40.094951Z","iopub.status.idle":"2025-02-17T11:29:39.243096Z","shell.execute_reply.started":"2025-02-17T11:01:40.094932Z","shell.execute_reply":"2025-02-17T11:29:39.242123Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/model.safetensors\nInstantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\nAll model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.1,\n  \"temperature\": 0.7,\n  \"top_k\": 20,\n  \"top_p\": 0.8\n}\n\nloading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/merges.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting prompt from train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"53b24f4c00fb44f7a023bfd90ae4ca81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b608f312f6ed42db969d0aaa3aa5bdf5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting prompt from eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"727efc152c3d4ec38a41dde82dad82aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"0d84f42184e6471e8ed8dcc09fb60ca2"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/1012 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d70d53b95be94c91b7af3cbffb29e493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"748e95786f2148f1bbe9c6260ad28858"}},"metadata":{}},{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 1,012\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Training with DataParallel so batch size has been adjusted to: 2\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 8\n  Total optimization steps = 63\n  Number of trainable parameters = 4,325,376\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='63' max='63' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [63/63 26:21, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rewards/chosen</th>\n      <th>Rewards/rejected</th>\n      <th>Rewards/accuracies</th>\n      <th>Rewards/margins</th>\n      <th>Logps/chosen</th>\n      <th>Logps/rejected</th>\n      <th>Logits/chosen</th>\n      <th>Logits/rejected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>8</td>\n      <td>5.531200</td>\n      <td>0.674531</td>\n      <td>-0.015869</td>\n      <td>-0.055664</td>\n      <td>0.520000</td>\n      <td>0.039795</td>\n      <td>-160.000000</td>\n      <td>-154.000000</td>\n      <td>-2.921875</td>\n      <td>-2.906250</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>5.534300</td>\n      <td>0.603633</td>\n      <td>0.012756</td>\n      <td>-0.223633</td>\n      <td>0.700000</td>\n      <td>0.236328</td>\n      <td>-160.000000</td>\n      <td>-155.000000</td>\n      <td>-2.859375</td>\n      <td>-2.843750</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>4.717400</td>\n      <td>0.539590</td>\n      <td>0.081543</td>\n      <td>-0.443359</td>\n      <td>0.790000</td>\n      <td>0.523438</td>\n      <td>-160.000000</td>\n      <td>-158.000000</td>\n      <td>-2.812500</td>\n      <td>-2.765625</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>4.681000</td>\n      <td>0.521533</td>\n      <td>0.057129</td>\n      <td>-0.546875</td>\n      <td>0.730000</td>\n      <td>0.601562</td>\n      <td>-160.000000</td>\n      <td>-159.000000</td>\n      <td>-2.812500</td>\n      <td>-2.781250</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>3.694100</td>\n      <td>0.509238</td>\n      <td>-0.033936</td>\n      <td>-0.699219</td>\n      <td>0.750000</td>\n      <td>0.664062</td>\n      <td>-161.000000</td>\n      <td>-160.000000</td>\n      <td>-2.828125</td>\n      <td>-2.796875</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>3.694100</td>\n      <td>0.508359</td>\n      <td>-0.086426</td>\n      <td>-0.773438</td>\n      <td>0.740000</td>\n      <td>0.687500</td>\n      <td>-161.000000</td>\n      <td>-161.000000</td>\n      <td>-2.843750</td>\n      <td>-2.812500</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>3.832700</td>\n      <td>0.493066</td>\n      <td>-0.151367</td>\n      <td>-0.878906</td>\n      <td>0.760000</td>\n      <td>0.726562</td>\n      <td>-162.000000</td>\n      <td>-162.000000</td>\n      <td>-2.843750</td>\n      <td>-2.812500</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nSaving model checkpoint to qwen_ex2_output/checkpoint-63\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\ntokenizer config file saved in qwen_ex2_output/checkpoint-63/tokenizer_config.json\nSpecial tokens file saved in qwen_ex2_output/checkpoint-63/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected. If origin_response_c_r, rejected, prompt, rejected_reward, chosen_reward, score_chosen, date, chosen, score_rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 01:10]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to qwen_ex2_output\n","output_type":"stream"},{"name":"stdout","text":"***** eval metrics *****\n  epoch                   =      0.996\n  eval_logits/chosen      =    -2.8438\n  eval_logits/rejected    =    -2.8125\n  eval_logps/chosen       =     -161.0\n  eval_logps/rejected     =     -162.0\n  eval_loss               =     0.4895\n  eval_rewards/accuracies =       0.76\n  eval_rewards/chosen     =    -0.1138\n  eval_rewards/margins    =     0.7305\n  eval_rewards/rejected   =    -0.8477\n  eval_runtime            = 0:01:12.60\n  eval_samples_per_second =      1.377\n  eval_steps_per_second   =      0.689\n","output_type":"stream"},{"name":"stderr","text":"loading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\ntokenizer config file saved in qwen_ex2_output/tokenizer_config.json\nSpecial tokens file saved in qwen_ex2_output/special_tokens_map.json\n","output_type":"stream"}],"execution_count":14},{"cell_type":"markdown","source":"## Give a look to the Model Generation","metadata":{}},{"cell_type":"code","source":"from trlabs.utils import dataset_creation, not_relevant_data\n\nSYSTEM_PROMPT = 'You are an advanced AI system specialised in providing Reuters News title given a body text of the news.'\nINSTRUCTION = \"The title should be in capital letters and between 6 and 8 words in length. Please provide only the title as output and no other text or explanation.\"\n\ndataset = load_dataset(\"ucirvine/reuters21578\", 'ModApte', trust_remote_code=True)\ndataset = dataset.filter(not_relevant_data).shuffle(seed=42).map(dataset_creation, fn_kwargs={\"system_prompt\": SYSTEM_PROMPT, \"instruction\": INSTRUCTION})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:29:39.244936Z","iopub.execute_input":"2025-02-17T11:29:39.245222Z","iopub.status.idle":"2025-02-17T11:29:40.000452Z","shell.execute_reply.started":"2025-02-17T11:29:39.245194Z","shell.execute_reply":"2025-02-17T11:29:39.999770Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"from trlabs.rl.eval import setup_model_and_lora, generate\n\nindex =33\nprompt = dataset[\"test\"][index][\"system\"]+dataset[\"test\"][index][\"messages\"]\n\nmodel, tokenizer = setup_model_and_lora(\n    base_model_name = model_config[\"model_name_or_path\"], \n    lora_path = training_params[\"output_dir\"]\n)\n\nresponse = generate(prompt, model, tokenizer)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T11:29:40.001956Z","iopub.execute_input":"2025-02-17T11:29:40.002180Z","iopub.status.idle":"2025-02-17T11:29:44.598743Z","shell.execute_reply.started":"2025-02-17T11:29:40.002161Z","shell.execute_reply":"2025-02-17T11:29:44.597966Z"}},"outputs":[{"name":"stderr","text":"loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/merges.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/model.safetensors\nInstantiating Qwen2ForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\nAll model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.1,\n  \"temperature\": 0.7,\n  \"top_k\": 20,\n  \"top_p\": 0.8\n}\n\nloading file vocab.json\nloading file merges.txt\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading file chat_template.jinja\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nYou are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 151646. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n","output_type":"stream"},{"name":"stdout","text":"system\nYou are an advanced AI system specialised in providing Reuters News title given a body text of the news.\nuser\nCanada Development Corp <CDC.TO> said it agreed to sell its 25.2 pct interest in CDC Life Sciences Inc to Caisse de depot et placement du Quebec, the provincial pension fund manager, and Institut Merieux, a French biological laboratory company, for 169.2 mln dlrs. It said the caisse and Institut Merieux will each buy 2.75 mln common shares of the company for 30.76 dlrs a share. It said following the transaction the caisse will hold about 19.3 pct of CDC Life Sciences. Canada Development said the purchasers do not plan to acquire the remaining publicly-held shares.\n\nThe title should be in capital letters and between 6 and 8 words in length. Please provide only the title as output and no other text or explanation.\nassistant\n\"Canada Development Corp Sells 25.2% Interest in CDC Life Sciences Inc to Caisse de Depot & Placement du Qu√©bec and Institut Merieux\"\n","output_type":"stream"}],"execution_count":16}]}