{"metadata":{"kernelspec":{"name":"python3","display_name":"Python 3","language":"python"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30887,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":5,"nbformat":4,"cells":[{"id":"ad34dcf5-1f4e-411f-a2a9-0f6ed9c54896","cell_type":"markdown","source":"# Exercise 1: Smart Dataset Sampling for Optimal Model Performance üéØ\nIn this exercise, we'll explore how intelligent dataset sampling can significantly impact model performance. We'll learn how to create high-quality training sets by implementing strategic sampling techniques.\n\nüåü The Challenge\nTraining on our entire dataset without proper sampling can lead to:\n\n* **Noisy Training Signals**: Not all data contributes equally to model learning\n* **Suboptimal Performance**: Quantity doesn't always mean quality\n* **Inefficient Learning**: Model might focus on redundant or low-quality examples","metadata":{}},{"id":"768a9999-e08a-4cf1-bc54-0d167e5e0fd1","cell_type":"markdown","source":"### Git Clone","metadata":{}},{"id":"9f5f1671-3827-4fa7-bf3a-96ed335eaec5","cell_type":"code","source":"! git clone https://github.com/thomsonreuters/labs_AMLD25_Workshop","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:11:53.648875Z","iopub.execute_input":"2025-02-17T09:11:53.649419Z","iopub.status.idle":"2025-02-17T09:11:54.689403Z","shell.execute_reply.started":"2025-02-17T09:11:53.649382Z","shell.execute_reply":"2025-02-17T09:11:54.688352Z"}},"outputs":[{"name":"stdout","text":"Cloning into 'labs_AMLD25_Workshop'...\nremote: Enumerating objects: 59, done.\u001b[K\nremote: Counting objects: 100% (59/59), done.\u001b[K\nremote: Compressing objects: 100% (45/45), done.\u001b[K\nremote: Total 59 (delta 8), reused 56 (delta 8), pack-reused 0 (from 0)\u001b[K\nReceiving objects: 100% (59/59), 3.92 MiB | 18.14 MiB/s, done.\nResolving deltas: 100% (8/8), done.\n","output_type":"stream"}],"execution_count":1},{"id":"cee0dbd1-ccbb-4940-91fa-a15fb113e682","cell_type":"markdown","source":"### Install dependencies","metadata":{}},{"id":"070a77fe-0f8a-4977-b183-cbc5d2653c04","cell_type":"code","source":"! pip install -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt\n! pip install flash-attn==2.7.3 --no-build-isolation","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:12:04.705492Z","iopub.execute_input":"2025-02-17T09:12:04.705824Z","iopub.status.idle":"2025-02-17T09:12:41.460705Z","shell.execute_reply.started":"2025-02-17T09:12:04.705780Z","shell.execute_reply":"2025-02-17T09:12:41.459705Z"}},"outputs":[{"name":"stdout","text":"Collecting accelerate==1.3.0 (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1))\n  Downloading accelerate-1.3.0-py3-none-any.whl.metadata (19 kB)\nCollecting bitsandbytes==0.45.1 (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 2))\n  Downloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl.metadata (5.8 kB)\nRequirement already satisfied: datasets==3.2.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.2.0)\nRequirement already satisfied: peft==0.14.0 in /usr/local/lib/python3.10/dist-packages (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 4)) (0.14.0)\nCollecting transformers==4.48.1 (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 5))\n  Downloading transformers-4.48.1-py3-none-any.whl.metadata (44 kB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m44.4/44.4 kB\u001b[0m \u001b[31m1.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting trl==0.13.0 (from -r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 6))\n  Downloading trl-0.13.0-py3-none-any.whl.metadata (11 kB)\nRequirement already satisfied: numpy<3.0.0,>=1.17 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (24.2)\nRequirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (5.9.5)\nRequirement already satisfied: pyyaml in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (6.0.2)\nRequirement already satisfied: torch>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2.5.1+cu121)\nRequirement already satisfied: huggingface-hub>=0.21.0 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (0.28.1)\nRequirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.10/dist-packages (from accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (0.4.5)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.17.0)\nRequirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (19.0.0)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (0.3.8)\nRequirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2.2.3)\nRequirement already satisfied: requests>=2.32.2 in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2.32.3)\nRequirement already satisfied: tqdm>=4.66.3 in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (4.67.1)\nRequirement already satisfied: xxhash in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.5.0)\nRequirement already satisfied: multiprocess<0.70.17 in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (0.70.16)\nRequirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /usr/local/lib/python3.10/dist-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2024.9.0)\nRequirement already satisfied: aiohttp in /usr/local/lib/python3.10/dist-packages (from datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.11.11)\nRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.1->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 5)) (2024.11.6)\nRequirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.10/dist-packages (from transformers==4.48.1->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 5)) (0.21.0)\nRequirement already satisfied: rich in /usr/local/lib/python3.10/dist-packages (from trl==0.13.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 6)) (13.9.4)\nRequirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2.4.4)\nRequirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (1.3.2)\nRequirement already satisfied: async-timeout<6.0,>=4.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (5.0.1)\nRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (25.1.0)\nRequirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (1.5.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (6.1.0)\nRequirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (0.2.1)\nRequirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (1.18.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.10/dist-packages (from huggingface-hub>=0.21.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (4.12.2)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2025.0.1)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2022.0.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.10/dist-packages (from numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2.4.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.4.1)\nRequirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (3.10)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2.3.0)\nRequirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests>=2.32.2->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2025.1.31)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (3.1.4)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch>=2.0.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch>=2.0.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.3.0)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2025.1)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.10/dist-packages (from pandas->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (2025.1)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl==0.13.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 6)) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich->trl==0.13.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 6)) (2.19.1)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich->trl==0.13.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 6)) (0.1.2)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas->datasets==3.2.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 3)) (1.17.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch>=2.0.0->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (3.0.2)\nRequirement already satisfied: intel-openmp>=2024 in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.10/dist-packages (from mkl->numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2022.0.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.10/dist-packages (from tbb==2022.*->mkl->numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.10/dist-packages (from mkl_umath->numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.10/dist-packages (from intel-openmp>=2024->mkl->numpy<3.0.0,>=1.17->accelerate==1.3.0->-r /kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/requirements.txt (line 1)) (2024.2.0)\nDownloading accelerate-1.3.0-py3-none-any.whl (336 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m336.6/336.6 kB\u001b[0m \u001b[31m9.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mta \u001b[36m0:00:01\u001b[0m\n\u001b[?25hDownloading bitsandbytes-0.45.1-py3-none-manylinux_2_24_x86_64.whl (69.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m69.7/69.7 MB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading transformers-4.48.1-py3-none-any.whl (9.7 MB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m9.7/9.7 MB\u001b[0m \u001b[31m112.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25hDownloading trl-0.13.0-py3-none-any.whl (293 kB)\n\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m293.4/293.4 kB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: transformers, accelerate, trl, bitsandbytes\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.47.0\n    Uninstalling transformers-4.47.0:\n      Successfully uninstalled transformers-4.47.0\n  Attempting uninstall: accelerate\n    Found existing installation: accelerate 1.2.1\n    Uninstalling accelerate-1.2.1:\n      Successfully uninstalled accelerate-1.2.1\nSuccessfully installed accelerate-1.3.0 bitsandbytes-0.45.1 transformers-4.48.1 trl-0.13.0\nCollecting flash-attn==2.7.3\n  Downloading flash_attn-2.7.3.tar.gz (3.2 MB)\n\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m3.2/3.2 MB\u001b[0m \u001b[31m33.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\nRequirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.7.3) (2.5.1+cu121)\nRequirement already satisfied: einops in /usr/local/lib/python3.10/dist-packages (from flash-attn==2.7.3) (0.8.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (3.17.0)\nRequirement already satisfied: typing-extensions>=4.8.0 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (4.12.2)\nRequirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (3.4.2)\nRequirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (3.1.4)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (2024.9.0)\nRequirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.10/dist-packages (from torch->flash-attn==2.7.3) (1.13.1)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy==1.13.1->torch->flash-attn==2.7.3) (1.3.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->torch->flash-attn==2.7.3) (3.0.2)\nBuilding wheels for collected packages: flash-attn\n  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for flash-attn: filename=flash_attn-2.7.3-cp310-cp310-linux_x86_64.whl size=191342964 sha256=cd0ead6ad92bff21f90cf0aecdf859bc7145bf4e7dc8ed1fdc3cc38299651ce4\n  Stored in directory: /root/.cache/pip/wheels/85/d7/10/a74c9fe5ffe6ff306b27a220b2bf2f37d907b68fdcd138cdda\nSuccessfully built flash-attn\nInstalling collected packages: flash-attn\nSuccessfully installed flash-attn-2.7.3\n","output_type":"stream"}],"execution_count":2},{"id":"08f4b693-0cd8-45d4-afaf-ee32b75ad9d5","cell_type":"markdown","source":"### Testing GPU\nPlease check if python recognize that you have GPU allocated, if not please go in `Settings`>`Accelerator`>`GPU T4 x 2` ","metadata":{}},{"id":"ff8e48b9-1001-42a5-8bfa-632b585584bb","cell_type":"code","source":"import os, sys\n\n# from tensorflow.python.client import device_lib\nrepo_folder = os.getcwd().split('labs_AMLD25_Workshop')[0]+\"/labs_AMLD25_Workshop/src\" \nsys.path.append(repo_folder)\n\n# UNCOMMENT TO CHECK GPU HW\n# device_lib.list_local_devices()","metadata":{"tags":[],"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:12:41.462086Z","iopub.execute_input":"2025-02-17T09:12:41.462352Z","iopub.status.idle":"2025-02-17T09:12:41.466908Z","shell.execute_reply.started":"2025-02-17T09:12:41.462330Z","shell.execute_reply":"2025-02-17T09:12:41.466109Z"}},"outputs":[],"execution_count":3},{"id":"af42e06e-970a-495d-893c-3ec156ece907","cell_type":"markdown","source":"if you get two GPUs you can manually assign them using env variables. This step is optional since they should be automatically recognized by pytorch ","metadata":{}},{"id":"0147144f-ee68-4958-b824-b1ec9ba9d87f","cell_type":"code","source":"os.environ[\"WANDB_DISABLED\"] = \"true\" ## turning off WandB logging\nos.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\"\n\nrl_foolder = \"labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:12:41.468356Z","iopub.execute_input":"2025-02-17T09:12:41.468678Z","iopub.status.idle":"2025-02-17T09:12:41.486246Z","shell.execute_reply.started":"2025-02-17T09:12:41.468649Z","shell.execute_reply":"2025-02-17T09:12:41.485565Z"}},"outputs":[],"execution_count":4},{"id":"c6b5f5d1-fed7-4e3d-9c26-9e702e98c518","cell_type":"code","source":"import torch\n\nfrom typing import Optional, List, Dict\nimport datasets\nfrom datasets import (\n    load_dataset, \n    load_from_disk, \n    DatasetDict,\n    concatenate_datasets\n)\n\nfrom accelerate import Accelerator, PartialState\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\n\nfrom trl import (\n    ModelConfig,\n    DPOTrainer,\n    DPOConfig,\n    TrlParser,\n    get_kbit_device_map,\n    get_peft_config,\n    get_quantization_config,\n)\n\nfrom trlabs.rl.data import (\n    get_datasets, \n    DataArguments\n)\n\nfrom trlabs.utils import *\n\nfrom trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:12:41.487448Z","iopub.execute_input":"2025-02-17T09:12:41.487872Z","iopub.status.idle":"2025-02-17T09:13:05.663245Z","shell.execute_reply.started":"2025-02-17T09:12:41.487841Z","shell.execute_reply":"2025-02-17T09:13:05.662627Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"}],"execution_count":5},{"id":"67c5edfd-ffd7-4d44-b44b-d02c71d53a65","cell_type":"markdown","source":"### Model Config","metadata":{}},{"id":"5cfac9f8-8a42-45ec-a885-997178768a99","cell_type":"code","source":"model_config = {\n    \"model_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n    \"torch_dtype\": \"bfloat16\",\n    \"use_peft\": True, \n    \"lora_r\": 64,        \n    \"lora_alpha\": 32,    # Stronger updates\n    \"lora_dropout\": 0.1, # Prevent overfitting\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:13:05.663971Z","iopub.execute_input":"2025-02-17T09:13:05.664183Z","iopub.status.idle":"2025-02-17T09:13:05.667901Z","shell.execute_reply.started":"2025-02-17T09:13:05.664163Z","shell.execute_reply":"2025-02-17T09:13:05.666887Z"}},"outputs":[],"execution_count":6},{"id":"c9b12b23-7568-481d-b367-c19adc2a9100","cell_type":"markdown","source":"### Data Config\nYou can leverage the preference dataset for this task located in `labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/data/AMLD25_reuters_gentitle_1k`. ","metadata":{}},{"id":"236d9b51-e84a-4f97-b9f9-c97dd6a7a4f0","cell_type":"code","source":"data_params = {\n  \"dataset_name\": \"Mix 1\",\n  \"dataset_mixer\": {\n    f\"{rl_foolder}/data/AMLD25_reuters_gentitle_1k\": 1.,\n  },\n  \"dataset_splits\": [\"train\", \"test\"],\n  \"num_eval_samples\": 100,\n  \"seed\": 42\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:13:05.668547Z","iopub.execute_input":"2025-02-17T09:13:05.668834Z","iopub.status.idle":"2025-02-17T09:13:05.729906Z","shell.execute_reply.started":"2025-02-17T09:13:05.668784Z","shell.execute_reply":"2025-02-17T09:13:05.729050Z"}},"outputs":[],"execution_count":7},{"id":"e67c3af0-2d92-4814-9aad-2039f79347ca","cell_type":"markdown","source":"### Training Config","metadata":{}},{"id":"a64d3765-75a0-4679-8d91-fe94a8c60b80","cell_type":"code","source":"training_params =  {\n    ## General\n    \"output_dir\": f\"{model_config['model_name_or_path'].split('/')[0].lower()}_ex1_output\",\n    \"num_train_epochs\": 1,\n    \"beta\": 0.1,\n    \"eval_strategy\": \"steps\",\n    \"eval_steps\": 8,\n    \"per_device_train_batch_size\": 1,\n    \"per_device_eval_batch_size\": 1,\n    \"gradient_accumulation_steps\": 8,\n    #@ context length and max length (max_new_token = max_length - max_prompt_length)\n    \"max_length\": 768,\n    \"max_prompt_length\":512,\n    ## Optimizer\n    \"optim\": \"adamw_torch\",\n    \"learning_rate\": 2.0e-4,\n    \"weight_decay\": 0.001,\n    \"adam_epsilon\": 1.0e-8,\n    \"adam_beta1\": 0.9,\n    \"adam_beta2\": 0.999,\n    \"max_grad_norm\": 1.0,\n    ## Scheduler ##\n    \"warmup_steps\": 10,\n    \"lr_scheduler_type\": \"cosine\",\n    ## Logging\n    \"log_level\": \"info\",\n    \"logging_first_step\": True,\n    \"logging_steps\": 10\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:13:05.730664Z","iopub.execute_input":"2025-02-17T09:13:05.730999Z","iopub.status.idle":"2025-02-17T09:13:06.323578Z","shell.execute_reply.started":"2025-02-17T09:13:05.730970Z","shell.execute_reply":"2025-02-17T09:13:06.322621Z"}},"outputs":[],"execution_count":8},{"id":"03223d88-b7cf-4326-a7c1-d908886b47fa","cell_type":"markdown","source":"### DPO Training Loop","metadata":{}},{"id":"0d7c6796-a16a-4fb5-bae8-e68f2c6028f1","cell_type":"code","source":"accelerator = Accelerator()\n\n\ndata_args = DataArguments(**data_params)\ntraining_args =  DPOConfig(**training_params)\nmodel_args = ModelConfig(**model_config)\n\n###################\n# Model & Tokenizer\n###################\ntorch_dtype = (\n    model_args.torch_dtype\n    if model_args.torch_dtype in [\"auto\", None]\n    else getattr(torch, model_args.torch_dtype)\n)\nquantization_config = get_quantization_config(model_args)\nmodel_kwargs = dict(\n    revision=model_args.model_revision,\n    attn_implementation=model_args.attn_implementation,\n    torch_dtype=torch_dtype,\n    use_cache=False if training_args.gradient_checkpointing else True,\n    device_map=get_kbit_device_map() if quantization_config is not None else None,\n    quantization_config=quantization_config,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, **model_kwargs\n)\npeft_config = get_peft_config(model_args)\nif peft_config is None:\n    ref_model = AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, **model_kwargs\n    )\nelse:\n    ref_model = None\ntokenizer = AutoTokenizer.from_pretrained(\n    model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code\n)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nif tokenizer.chat_template is None:\n    tokenizer.chat_template = SIMPLE_CHAT_TEMPLATE\n\n################\n# Dataset\n################\ndataset =  get_datasets(data_args, splits=data_args.dataset_splits)\n\n\n\n################\n# Training\n################\ntrainer = DPOTrainer(\n    model,\n    ref_model,\n    args=training_args,\n    train_dataset=dataset[data_args.dataset_train_split],\n    eval_dataset=dataset[data_args.dataset_test_split].shuffle(data_args.seed)\\\n        .take(min(len(dataset[data_args.dataset_test_split]), data_args.num_eval_samples))\\\n        if training_args.eval_strategy != \"no\" else None,\n    processing_class=tokenizer,\n    peft_config=peft_config,\n)\n\ntrainer.train()\n\nif training_args.eval_strategy != \"no\":\n    metrics = trainer.evaluate()\n    trainer.log_metrics(\"eval\", metrics)\n    trainer.save_metrics(\"eval\", metrics)\n\n# Save and push to hub\ntrainer.save_model(training_args.output_dir)\nif training_args.push_to_hub:\n    trainer.push_to_hub(dataset_name=data_args.dataset_name)","metadata":{"scrolled":true,"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:13:06.326091Z","iopub.execute_input":"2025-02-17T09:13:06.326346Z","iopub.status.idle":"2025-02-17T09:34:04.378224Z","shell.execute_reply.started":"2025-02-17T09:13:06.326324Z","shell.execute_reply":"2025-02-17T09:34:04.377476Z"}},"outputs":[{"name":"stderr","text":"Using the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/659 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c02fe176eeb642b98d0f18058ab0a4d4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/988M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2fe02808eb3b4d6089f19077edef3a20"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/242 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cad5f66186ba4ed98fead8b6cd095d19"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/1.29k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1f75f87bccd1415384dd58235bf99d4a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/2.78M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9396c98d9bf547428fc9d34be3b6467d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/1.67M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7f089ee40edb4b27815edbe1b5812d58"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/7.03M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f73c715d889a46c9be8d693bfd3f09c0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting prompt from train dataset:   0%|          | 0/987 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"aa545874781f41b28915950f1b4344b9"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/987 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"024ef76836a6407cb61bc034ce1f0188"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting prompt from eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e06bf4d5252b4374852130522b0476bf"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dfe6bb6acbd14ced95f7399d3b1293c1"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/987 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d86a89afa09d40e7b4c7fa67cf9e0cf6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9c041fedcef438c9a9ee15618455294"}},"metadata":{}},{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 987\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Training with DataParallel so batch size has been adjusted to: 2\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 8\n  Total optimization steps = 61\n  Number of trainable parameters = 4,325,376\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='61' max='61' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [61/61 19:20, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rewards/chosen</th>\n      <th>Rewards/rejected</th>\n      <th>Rewards/accuracies</th>\n      <th>Rewards/margins</th>\n      <th>Logps/chosen</th>\n      <th>Logps/rejected</th>\n      <th>Logits/chosen</th>\n      <th>Logits/rejected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>8</td>\n      <td>5.531200</td>\n      <td>0.662812</td>\n      <td>0.166992</td>\n      <td>0.089355</td>\n      <td>0.580000</td>\n      <td>0.077637</td>\n      <td>-54.750000</td>\n      <td>-53.250000</td>\n      <td>-3.062500</td>\n      <td>-3.062500</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>5.413600</td>\n      <td>0.609609</td>\n      <td>0.867188</td>\n      <td>0.601562</td>\n      <td>0.670000</td>\n      <td>0.265625</td>\n      <td>-47.750000</td>\n      <td>-48.250000</td>\n      <td>-3.109375</td>\n      <td>-3.109375</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>4.767300</td>\n      <td>0.553813</td>\n      <td>0.457031</td>\n      <td>-0.059082</td>\n      <td>0.730000</td>\n      <td>0.515625</td>\n      <td>-51.750000</td>\n      <td>-54.750000</td>\n      <td>-3.093750</td>\n      <td>-3.093750</td>\n    </tr>\n    <tr>\n      <td>32</td>\n      <td>4.307300</td>\n      <td>0.539946</td>\n      <td>0.227539</td>\n      <td>-0.486328</td>\n      <td>0.760000</td>\n      <td>0.714844</td>\n      <td>-54.000000</td>\n      <td>-59.000000</td>\n      <td>-3.093750</td>\n      <td>-3.093750</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>4.371500</td>\n      <td>0.501414</td>\n      <td>-0.371094</td>\n      <td>-1.164062</td>\n      <td>0.750000</td>\n      <td>0.792969</td>\n      <td>-60.000000</td>\n      <td>-66.000000</td>\n      <td>-3.078125</td>\n      <td>-3.078125</td>\n    </tr>\n    <tr>\n      <td>48</td>\n      <td>4.371500</td>\n      <td>0.498445</td>\n      <td>-0.349609</td>\n      <td>-1.140625</td>\n      <td>0.780000</td>\n      <td>0.792969</td>\n      <td>-59.750000</td>\n      <td>-65.500000</td>\n      <td>-3.078125</td>\n      <td>-3.078125</td>\n    </tr>\n    <tr>\n      <td>56</td>\n      <td>4.397500</td>\n      <td>0.490835</td>\n      <td>-0.253906</td>\n      <td>-1.070312</td>\n      <td>0.790000</td>\n      <td>0.812500</td>\n      <td>-58.750000</td>\n      <td>-65.000000</td>\n      <td>-3.093750</td>\n      <td>-3.078125</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nSaving model checkpoint to qwen_ex1_output/checkpoint-61\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\ntokenizer config file saved in qwen_ex1_output/checkpoint-61/tokenizer_config.json\nSpecial tokens file saved in qwen_ex1_output/checkpoint-61/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:53]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to qwen_ex1_output\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\n","output_type":"stream"},{"name":"stdout","text":"***** eval metrics *****\n  epoch                   =     0.9879\n  eval_logits/chosen      =    -3.0938\n  eval_logits/rejected    =    -3.0781\n  eval_logps/chosen       =     -58.75\n  eval_logps/rejected     =      -65.0\n  eval_loss               =     0.4905\n  eval_rewards/accuracies =       0.79\n  eval_rewards/chosen     =    -0.2383\n  eval_rewards/margins    =     0.8203\n  eval_rewards/rejected   =    -1.0625\n  eval_runtime            = 0:00:54.86\n  eval_samples_per_second =      1.823\n  eval_steps_per_second   =      0.911\n","output_type":"stream"},{"name":"stderr","text":"tokenizer config file saved in qwen_ex1_output/tokenizer_config.json\nSpecial tokens file saved in qwen_ex1_output/special_tokens_map.json\n","output_type":"stream"}],"execution_count":9},{"id":"3502d66e-7c3f-418e-9c12-1bb1a780602f","cell_type":"markdown","source":"### Your Turn!\nWe randomly selected a subset, choosing **X** as the fraction. See below\n\n```python\ndata_params = {\n  \"dataset_name\": \"Mix 1\",\n  \"dataset_mixer\": {\n    \"labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/data/AMLD25_reuters_gentitle_1k\": X,\n  },\n  \"dataset_splits\": [\"train\", \"test\"],\n  \"num_eval_samples\": 100,\n  \"seed\": 42\n}\n```\n\nCan we do a better selection? \n\n**Hint(s)**: \n1. Please give a deep look to the max context length (768) and `chosen` and `rejected` features\n2. Please check the other columns\n\n<details>\n<summary> <b>Solution Spoiler!</b> </summary>\n  Search in <code>src/trgpt/utils.py</code> for the solution (functions: <code>reuters_cleaning_dataset</code> and <code>not_relevant_data</code>) \n</details>\n","metadata":{"jp-MarkdownHeadingCollapsed":true}},{"id":"6e215ec9-a0c1-4336-8806-fe4c7829b7df","cell_type":"markdown","source":"## Give a look to the Model Generation","metadata":{}},{"id":"a52f7a8e-aeb6-42f4-ad81-4fe295434a85","cell_type":"code","source":"from trlabs.utils import dataset_creation, not_relevant_data\n\nSYSTEM_PROMPT = 'You are an advanced AI system specialised in providing Reuters News title given a body text of the news.'\nINSTRUCTION = \"The title should be in capital letters and between 6 and 8 words in length. Please provide only the title as output and no other text or explanation.\"\n\ndataset = load_dataset(\"ucirvine/reuters21578\", 'ModApte', trust_remote_code=True)\ndataset = dataset.filter(not_relevant_data).shuffle(seed=42).map(dataset_creation, fn_kwargs={\"system_prompt\": SYSTEM_PROMPT, \"instruction\": INSTRUCTION})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:34:04.379305Z","iopub.execute_input":"2025-02-17T09:34:04.379534Z","iopub.status.idle":"2025-02-17T09:34:13.157755Z","shell.execute_reply.started":"2025-02-17T09:34:04.379506Z","shell.execute_reply":"2025-02-17T09:34:13.156700Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/16.0k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"47e0f19e9ef949f68d37f3a7bdd63feb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"reuters21578.py:   0%|          | 0.00/17.9k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9ffa945691474258b8e93e2d2400c5aa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"reuters21578.tar.gz:   0%|          | 0.00/8.15M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"d9cd8ea9feb34115892bdbc2fd9960de"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/3299 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"851b84052fd94c32ab977f904decb312"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/9603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5e706e11775c43808b0db7d046b47ba4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating unused split:   0%|          | 0/722 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49030f97e63b48da8d73e17ecb7704fa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/3299 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9092c3bc7d5840dcaccdab94afd0ca72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/9603 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fc385b2cd5e6453dbd58a295f67c5a72"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/722 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b2e50e01498f44dabc5e4349cdbc3521"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/3295 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcac1a9bb0f74af1a678602f017d62b6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/9583 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"10834e2c37314d3dbbb0e62e2d03cc84"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/722 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3ff1d5b802a64bf4a3d8d1c9ff626eba"}},"metadata":{}}],"execution_count":10},{"id":"522d09df-b9d7-427e-8d14-71da31972d14","cell_type":"code","source":"from trlabs.rl.eval import setup_model_and_lora, generate\n\nindex =15\nprompt = dataset[\"test\"][index][\"system\"]+dataset[\"test\"][index][\"messages\"]\n\nmodel, tokenizer = setup_model_and_lora(\n    base_model_name = model_config[\"model_name_or_path\"], \n    lora_path = training_params[\"output_dir\"]\n)\n\nresponse = generate(prompt, model, tokenizer)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:34:13.158963Z","iopub.execute_input":"2025-02-17T09:34:13.159302Z","iopub.status.idle":"2025-02-17T09:34:17.658677Z","shell.execute_reply.started":"2025-02-17T09:34:13.159264Z","shell.execute_reply":"2025-02-17T09:34:17.657916Z"}},"outputs":[{"name":"stderr","text":"loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/merges.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/model.safetensors\nInstantiating Qwen2ForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\nAll model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.1,\n  \"temperature\": 0.7,\n  \"top_k\": 20,\n  \"top_p\": 0.8\n}\n\nloading file vocab.json\nloading file merges.txt\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading file chat_template.jinja\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nYou are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 151646. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\nThe attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n","output_type":"stream"},{"name":"stdout","text":"system\nYou are an advanced AI system specialised in providing Reuters News title given a body text of the news.\nuser\nChina's foreign debt is very low given its export capability, the size of its economy and its growth potential and the country is politically stable, Jean-Maxime Leveque, chairman of Credit Lyonnais, told reporters. Leveque, who has met the heads of most of China's banks including the president of its central bank during a visit here, said the Chinese authorities are very attentive to its foreign debt and have the matter under control. Official figures show China's foreign debt at a post-1949 record 16 billion dlrs at end-1986. Asked if he had advised China to borrow more francs and U.S. Dollars and less yen, Leveque said he had not offered any advice, but added: \"The yen and the dollar are not stable, but the ECU is stable.\" Asked if his bank has lost any confidence in China after the resignation of Communist Party chief Hu Yaobang in January, he said: \"We have total confidence in the political stability in China. The policies of the open door and economic development outlined in 1979 will not change, although there may be fluctuations in speed.\" REUTER\n\nThe title should be in capital letters and between 6 and 8 words in length. Please provide only the title as output and no other text or explanation.\nassistant\n\"Chinese Foreign Debt Low Given Export Cap, Economy Growth Potential, Political Stability, Credit Lyonnais Chairman Says\"\n","output_type":"stream"}],"execution_count":11},{"id":"7b957a53-7a30-4718-96f5-508796386f8a","cell_type":"markdown","source":"#### Note: \nif you do not provide a lora_path you can check the base model output","metadata":{}},{"id":"0c172050-376b-4bee-b5e9-2f9a85931910","cell_type":"markdown","source":"## Solution","metadata":{}},{"id":"911b21cf-8699-4a28-8a77-bc277b30792e","cell_type":"code","source":"from trlabs.utils import *\n\ndataset = load_from_disk(\"/kaggle/working/labs_AMLD25_Workshop/sessions/4_RLalignment_and_DPO/data/AMLD25_reuters_gentitle_1k\").filter(reuters_cleaning_dataset)\ndataset.save_to_disk(\"AMLD25_reuters_gentitle_0.5k_cleaned\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:34:17.659497Z","iopub.execute_input":"2025-02-17T09:34:17.659838Z","iopub.status.idle":"2025-02-17T09:34:17.820867Z","shell.execute_reply.started":"2025-02-17T09:34:17.659789Z","shell.execute_reply":"2025-02-17T09:34:17.820014Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/987 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"62e7a1bebfe141a29cb5b82b3abbd5bb"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/496 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e0cb72fd12f4a99839119f98404db22"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/446 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f6eb032c0fe0460cbbe3ff70743da32b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Saving the dataset (0/1 shards):   0%|          | 0/196 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5443cd8db70d41b5a1cebd67638ee01e"}},"metadata":{}}],"execution_count":12},{"id":"f036faa4-dab6-49d8-9a67-95aa74298ba2","cell_type":"code","source":"data_params = {\n  \"dataset_name\": \"Mix 1\",\n  \"dataset_mixer\": {\n    f\"AMLD25_reuters_gentitle_0.5k_cleaned\": 1.,\n  },\n  \"dataset_splits\": [\"train\", \"test\"],\n  \"num_eval_samples\": 100,\n  \"seed\": 42\n}","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:34:17.821776Z","iopub.execute_input":"2025-02-17T09:34:17.822121Z","iopub.status.idle":"2025-02-17T09:34:17.825851Z","shell.execute_reply.started":"2025-02-17T09:34:17.822084Z","shell.execute_reply":"2025-02-17T09:34:17.825204Z"}},"outputs":[],"execution_count":13},{"id":"a098c915-0198-42a1-9ff3-164d9137cdb1","cell_type":"code","source":"accelerator = Accelerator()\n\n\ndata_args = DataArguments(**data_params)\ntraining_args =  DPOConfig(**training_params)\nmodel_args = ModelConfig(**model_config)\n\n###################\n# Model & Tokenizer\n###################\ntorch_dtype = (\n    model_args.torch_dtype\n    if model_args.torch_dtype in [\"auto\", None]\n    else getattr(torch, model_args.torch_dtype)\n)\nquantization_config = get_quantization_config(model_args)\nmodel_kwargs = dict(\n    revision=model_args.model_revision,\n    attn_implementation=model_args.attn_implementation,\n    torch_dtype=torch_dtype,\n    use_cache=False if training_args.gradient_checkpointing else True,\n    device_map=get_kbit_device_map() if quantization_config is not None else None,\n    quantization_config=quantization_config,\n)\nmodel = AutoModelForCausalLM.from_pretrained(\n    model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, **model_kwargs\n)\npeft_config = get_peft_config(model_args)\nif peft_config is None:\n    ref_model = AutoModelForCausalLM.from_pretrained(\n        model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, **model_kwargs\n    )\nelse:\n    ref_model = None\ntokenizer = AutoTokenizer.from_pretrained(\n    model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code\n)\nif tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token\nif tokenizer.chat_template is None:\n    tokenizer.chat_template = SIMPLE_CHAT_TEMPLATE\n\n################\n# Dataset\n################\ndataset =  get_datasets(data_args, splits=data_args.dataset_splits)\n\n\n\n################\n# Training\n################\ntrainer = DPOTrainer(\n    model,\n    ref_model,\n    args=training_args,\n    train_dataset=dataset[data_args.dataset_train_split],\n    eval_dataset=dataset[data_args.dataset_test_split].shuffle(data_args.seed)\\\n        .take(min(len(dataset[data_args.dataset_test_split]), data_args.num_eval_samples))\\\n        if training_args.eval_strategy != \"no\" else None,\n    processing_class=tokenizer,\n    peft_config=peft_config,\n)\n\ntrainer.train()\n\nif training_args.eval_strategy != \"no\":\n    metrics = trainer.evaluate()\n    trainer.log_metrics(\"eval\", metrics)\n    trainer.save_metrics(\"eval\", metrics)\n\n# Save and push to hub\ntrainer.save_model(training_args.output_dir)\nif training_args.push_to_hub:\n    trainer.push_to_hub(dataset_name=data_args.dataset_name)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:34:17.826530Z","iopub.execute_input":"2025-02-17T09:34:17.826712Z","iopub.status.idle":"2025-02-17T09:42:50.447300Z","shell.execute_reply.started":"2025-02-17T09:34:17.826696Z","shell.execute_reply":"2025-02-17T09:42:50.446269Z"}},"outputs":[{"name":"stderr","text":"PyTorch: setting up devices\nThe default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\nUsing the `WANDB_DISABLED` environment variable is deprecated and will be removed in v5. Use the --report_to flag to control the integrations used for logging result (for instance --report_to none).\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/model.safetensors\nInstantiating Qwen2ForCausalLM model under default dtype torch.bfloat16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\nAll model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.1,\n  \"temperature\": 0.7,\n  \"top_k\": 20,\n  \"top_p\": 0.8\n}\n\nloading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/merges.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Extracting prompt from train dataset:   0%|          | 0/446 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"751d9b1dce6a47c88692eec8ca6b0ad8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to train dataset:   0%|          | 0/446 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"578a34f0e35d4c2191d0aed8055daeea"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Extracting prompt from eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8dcb6e4ca6164bb99b7ca70db4656558"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Applying chat template to eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ee6f6f389f02443291db3a78929eb857"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing train dataset:   0%|          | 0/446 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e7d829ee9d846e38c4adbf964f89c2c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Tokenizing eval dataset:   0%|          | 0/100 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5f4fc74e21e401f880274593671fd08"}},"metadata":{}},{"name":"stderr","text":"The following columns in the training set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n***** Running training *****\n  Num examples = 446\n  Num Epochs = 1\n  Instantaneous batch size per device = 1\n  Training with DataParallel so batch size has been adjusted to: 2\n  Total train batch size (w. parallel, distributed & accumulation) = 16\n  Gradient Accumulation steps = 8\n  Total optimization steps = 27\n  Number of trainable parameters = 4,325,376\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='27' max='27' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [27/27 07:26, Epoch 0/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n      <th>Rewards/chosen</th>\n      <th>Rewards/rejected</th>\n      <th>Rewards/accuracies</th>\n      <th>Rewards/margins</th>\n      <th>Logps/chosen</th>\n      <th>Logps/rejected</th>\n      <th>Logits/chosen</th>\n      <th>Logits/rejected</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>8</td>\n      <td>5.531200</td>\n      <td>0.641328</td>\n      <td>0.173828</td>\n      <td>0.061768</td>\n      <td>0.720000</td>\n      <td>0.112793</td>\n      <td>-56.000000</td>\n      <td>-58.750000</td>\n      <td>-3.062500</td>\n      <td>-3.046875</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>5.392400</td>\n      <td>0.530469</td>\n      <td>0.867188</td>\n      <td>0.335938</td>\n      <td>0.710000</td>\n      <td>0.531250</td>\n      <td>-49.000000</td>\n      <td>-56.000000</td>\n      <td>-3.015625</td>\n      <td>-3.000000</td>\n    </tr>\n    <tr>\n      <td>24</td>\n      <td>3.973500</td>\n      <td>0.495313</td>\n      <td>0.707031</td>\n      <td>-0.025391</td>\n      <td>0.780000</td>\n      <td>0.734375</td>\n      <td>-50.500000</td>\n      <td>-59.500000</td>\n      <td>-3.015625</td>\n      <td>-3.000000</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"The following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\nSaving model checkpoint to qwen_ex1_output/checkpoint-27\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\ntokenizer config file saved in qwen_ex1_output/checkpoint-27/tokenizer_config.json\nSpecial tokens file saved in qwen_ex1_output/checkpoint-27/special_tokens_map.json\n\n\nTraining completed. Do not forget to share your model on huggingface.co/models =)\n\n\nThe following columns in the evaluation set don't have a corresponding argument in `PeftModelForCausalLM.forward` and have been ignored: origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected. If origin_response_c_r, chosen_reward, date, prompt, chosen, rejected_reward, rejected are not expected by `PeftModelForCausalLM.forward`,  you can safely ignore this message.\n\n***** Running Evaluation *****\n  Num examples = 100\n  Batch size = 2\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='50' max='50' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [50/50 00:48]\n    </div>\n    "},"metadata":{}},{"name":"stderr","text":"Saving model checkpoint to qwen_ex1_output\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"bfloat16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\ntokenizer config file saved in qwen_ex1_output/tokenizer_config.json\nSpecial tokens file saved in qwen_ex1_output/special_tokens_map.json\n","output_type":"stream"},{"name":"stdout","text":"***** eval metrics *****\n  epoch                   =     0.9686\n  eval_logits/chosen      =    -3.0156\n  eval_logits/rejected    =       -3.0\n  eval_logps/chosen       =     -50.75\n  eval_logps/rejected     =     -59.75\n  eval_loss               =     0.4952\n  eval_rewards/accuracies =       0.78\n  eval_rewards/chosen     =     0.7031\n  eval_rewards/margins    =     0.7383\n  eval_rewards/rejected   =    -0.0342\n  eval_runtime            = 0:00:48.88\n  eval_samples_per_second =      2.046\n  eval_steps_per_second   =      1.023\n","output_type":"stream"}],"execution_count":14},{"id":"c6d7da55-56e4-4fe4-a8ad-68b1b2120f0d","cell_type":"code","source":"from trlabs.utils import dataset_creation, not_relevant_data\n\nSYSTEM_PROMPT = 'You are an advanced AI system specialised in providing Reuters News title given a body text of the news.'\nINSTRUCTION = \"The title should be in capital letters and between 6 and 8 words in length. Please provide only the title as output and no other text or explanation.\"\n\ndataset = load_dataset(\"ucirvine/reuters21578\", 'ModApte', trust_remote_code=True)\ndataset = dataset.filter(not_relevant_data).shuffle(seed=42).map(dataset_creation, fn_kwargs={\"system_prompt\": SYSTEM_PROMPT, \"instruction\": INSTRUCTION})","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:42:50.448355Z","iopub.execute_input":"2025-02-17T09:42:50.448675Z","iopub.status.idle":"2025-02-17T09:42:51.052851Z","shell.execute_reply.started":"2025-02-17T09:42:50.448646Z","shell.execute_reply":"2025-02-17T09:42:51.052091Z"}},"outputs":[],"execution_count":15},{"id":"ef2c6d58-f6ad-4ccd-b23f-56f4422a849c","cell_type":"code","source":"from trlabs.rl.eval import setup_model_and_lora, generate\n\nindex =15\nprompt = dataset[\"test\"][index][\"system\"]+dataset[\"test\"][index][\"messages\"]\n\nmodel, tokenizer = setup_model_and_lora(\n    base_model_name = model_config[\"model_name_or_path\"], \n    lora_path = training_params[\"output_dir\"]\n)\n\nresponse = generate(prompt, model, tokenizer)\nprint(response)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-02-17T09:42:51.053787Z","iopub.execute_input":"2025-02-17T09:42:51.054046Z","iopub.status.idle":"2025-02-17T09:42:55.518336Z","shell.execute_reply.started":"2025-02-17T09:42:51.054024Z","shell.execute_reply":"2025-02-17T09:42:55.517396Z"}},"outputs":[{"name":"stderr","text":"loading file vocab.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/vocab.json\nloading file merges.txt from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/merges.txt\nloading file tokenizer.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer.json\nloading file added_tokens.json from cache at None\nloading file special_tokens_map.json from cache at None\nloading file tokenizer_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/tokenizer_config.json\nloading file chat_template.jinja from cache at None\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nloading configuration file config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/config.json\nModel config Qwen2Config {\n  \"_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n  \"architectures\": [\n    \"Qwen2ForCausalLM\"\n  ],\n  \"attention_dropout\": 0.0,\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645,\n  \"hidden_act\": \"silu\",\n  \"hidden_size\": 896,\n  \"initializer_range\": 0.02,\n  \"intermediate_size\": 4864,\n  \"max_position_embeddings\": 32768,\n  \"max_window_layers\": 24,\n  \"model_type\": \"qwen2\",\n  \"num_attention_heads\": 14,\n  \"num_hidden_layers\": 24,\n  \"num_key_value_heads\": 2,\n  \"rms_norm_eps\": 1e-06,\n  \"rope_scaling\": null,\n  \"rope_theta\": 1000000.0,\n  \"sliding_window\": null,\n  \"tie_word_embeddings\": true,\n  \"torch_dtype\": \"float16\",\n  \"transformers_version\": \"4.48.1\",\n  \"use_cache\": true,\n  \"use_sliding_window\": false,\n  \"vocab_size\": 151936\n}\n\nloading weights file model.safetensors from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/model.safetensors\nInstantiating Qwen2ForCausalLM model under default dtype torch.float16.\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"eos_token_id\": 151645\n}\n\nAll model checkpoint weights were used when initializing Qwen2ForCausalLM.\n\nAll the weights of Qwen2ForCausalLM were initialized from the model checkpoint at Qwen/Qwen2-0.5B-Instruct.\nIf your task is similar to the task the model of the checkpoint was trained on, you can already use Qwen2ForCausalLM for predictions without further training.\nloading configuration file generation_config.json from cache at /root/.cache/huggingface/hub/models--Qwen--Qwen2-0.5B-Instruct/snapshots/c540970f9e29518b1d8f06ab8b24cba66ad77b6d/generation_config.json\nGenerate config GenerationConfig {\n  \"bos_token_id\": 151643,\n  \"do_sample\": true,\n  \"eos_token_id\": [\n    151645,\n    151643\n  ],\n  \"pad_token_id\": 151643,\n  \"repetition_penalty\": 1.1,\n  \"temperature\": 0.7,\n  \"top_k\": 20,\n  \"top_p\": 0.8\n}\n\nloading file vocab.json\nloading file merges.txt\nloading file tokenizer.json\nloading file added_tokens.json\nloading file special_tokens_map.json\nloading file tokenizer_config.json\nloading file chat_template.jinja\nSpecial tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\nYou are resizing the embedding layer without providing a `pad_to_multiple_of` parameter. This means that the new embedding dimension will be 151646. This might induce some performance reduction as *Tensor Cores* will not be available. For more details about this, or help on choosing the correct value for resizing, refer to this guide: https://docs.nvidia.com/deeplearning/performance/dl-performance-matrix-multiplication/index.html#requirements-tc\n","output_type":"stream"},{"name":"stdout","text":"system\nYou are an advanced AI system specialised in providing Reuters News title given a body text of the news.\nuser\nChina's foreign debt is very low given its export capability, the size of its economy and its growth potential and the country is politically stable, Jean-Maxime Leveque, chairman of Credit Lyonnais, told reporters. Leveque, who has met the heads of most of China's banks including the president of its central bank during a visit here, said the Chinese authorities are very attentive to its foreign debt and have the matter under control. Official figures show China's foreign debt at a post-1949 record 16 billion dlrs at end-1986. Asked if he had advised China to borrow more francs and U.S. Dollars and less yen, Leveque said he had not offered any advice, but added: \"The yen and the dollar are not stable, but the ECU is stable.\" Asked if his bank has lost any confidence in China after the resignation of Communist Party chief Hu Yaobang in January, he said: \"We have total confidence in the political stability in China. The policies of the open door and economic development outlined in 1979 will not change, although there may be fluctuations in speed.\" REUTER\n\nThe title should be in capital letters and between 6 and 8 words in length. Please provide only the title as output and no other text or explanation.\nassistant\n\"Credit Lyonnais Chairman Levesque Says China Has Stable Political Stability, Will Not Borrow More Francs, US Dollars, or Exchange Rate\"\n","output_type":"stream"}],"execution_count":16}]}