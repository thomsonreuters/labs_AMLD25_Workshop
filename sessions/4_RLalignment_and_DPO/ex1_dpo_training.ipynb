{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ad34dcf5-1f4e-411f-a2a9-0f6ed9c54896",
   "metadata": {},
   "source": [
    "# Exercise 1: Smart Dataset Sampling for Optimal Model Performance ðŸŽ¯\n",
    "In this exercise, we'll explore how intelligent dataset sampling can significantly impact model performance. We'll learn how to create high-quality training sets by implementing strategic sampling techniques.\n",
    "\n",
    "ðŸŒŸ The Challenge\n",
    "Training on our entire dataset without proper sampling can lead to:\n",
    "\n",
    "* **Noisy Training Signals**: Not all data contributes equally to model learning\n",
    "* **Suboptimal Performance**: Quantity doesn't always mean quality\n",
    "* **Inefficient Learning**: Model might focus on redundant or low-quality examples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba72fff1-f380-4532-8218-af46c889d3cc",
   "metadata": {},
   "source": [
    "### Install dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "070a77fe-0f8a-4977-b183-cbc5d2653c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -r requirements.txt\n",
    "! pip install flash-attn==2.7.3 --no-build-isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08f4b693-0cd8-45d4-afaf-ee32b75ad9d5",
   "metadata": {},
   "source": [
    "### Testing GPU\n",
    "Please check if python recognize that you have GPU allocated, if not please go in `Settings`>`Accelerator`>`GPU T4 x 2` "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ff8e48b9-1001-42a5-8bfa-632b585584bb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os, sys\n",
    "\n",
    "# from tensorflow.python.client import device_lib\n",
    "repo_folder = os.getcwd().split('labs_AMLD25_Workshop')[0][:-1]+\"/labs_AMLD25_Workshop/src\" \n",
    "sys.path.append(repo_folder)\n",
    "\n",
    "# UNCOMMENT TO CHECK GPU HW\n",
    "# device_lib.list_local_devices()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af42e06e-970a-495d-893c-3ec156ece907",
   "metadata": {},
   "source": [
    "if you get two GPUs you can manually assign them using env variables. This step is optional since they should be automatically recognized by pytorch "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0147144f-ee68-4958-b824-b1ec9ba9d87f",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"WANDB_DISABLED\"] = \"true\" ## turning off WandB logging\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = \"0,1\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "c6b5f5d1-fed7-4e3d-9c26-9e702e98c518",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from typing import Optional, List, Dict\n",
    "import datasets\n",
    "from datasets import (\n",
    "    load_dataset, \n",
    "    load_from_disk, \n",
    "    DatasetDict,\n",
    "    concatenate_datasets\n",
    ")\n",
    "\n",
    "from accelerate import Accelerator, PartialState\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "from trl import (\n",
    "    ModelConfig,\n",
    "    DPOTrainer,\n",
    "    DPOConfig,\n",
    "    TrlParser,\n",
    "    get_kbit_device_map,\n",
    "    get_peft_config,\n",
    "    get_quantization_config,\n",
    ")\n",
    "\n",
    "from trlabs.rl.data import (\n",
    "    get_datasets, \n",
    "    DataArguments\n",
    ")\n",
    "\n",
    "from trlabs.utils import *\n",
    "\n",
    "from trl.trainer.utils import SIMPLE_CHAT_TEMPLATE\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67c5edfd-ffd7-4d44-b44b-d02c71d53a65",
   "metadata": {},
   "source": [
    "### Model Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5cfac9f8-8a42-45ec-a885-997178768a99",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_config = {\n",
    "    \"model_name_or_path\": \"Qwen/Qwen2-0.5B-Instruct\",\n",
    "    \"torch_dtype\": \"bfloat16\",\n",
    "    \"use_peft\": True, \n",
    "    \"lora_r\": 64,        \n",
    "    \"lora_alpha\": 32,    # Stronger updates\n",
    "    \"lora_dropout\": 0.1, # Prevent overfitting\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b12b23-7568-481d-b367-c19adc2a9100",
   "metadata": {},
   "source": [
    "### Data Config\n",
    "You can leverage the preference dataset for this task located in `data/AMLD25_reuters_gentitle_1k`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "236d9b51-e84a-4f97-b9f9-c97dd6a7a4f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_params = {\n",
    "  \"dataset_name\": \"Mix 1\",\n",
    "  \"dataset_mixer\": {\n",
    "    \"./data/AMLD25_reuters_gentitle_1k\": 1.,\n",
    "  },\n",
    "  \"dataset_splits\": [\"train\", \"test\"],\n",
    "  \"num_eval_samples\": 100,\n",
    "  \"seed\": 42\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e67c3af0-2d92-4814-9aad-2039f79347ca",
   "metadata": {},
   "source": [
    "### Training Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a64d3765-75a0-4679-8d91-fe94a8c60b80",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_params =  {\n",
    "    ## General\n",
    "    \"output_dir\": f\"{model_config['model_name_or_path'].split('/')[0].lower()}_ex1_output\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"beta\": 0.1,\n",
    "    \"eval_strategy\": \"steps\",\n",
    "    \"eval_steps\": 8,\n",
    "    \"per_device_train_batch_size\": 1,\n",
    "    \"per_device_eval_batch_size\": 1,\n",
    "    \"gradient_accumulation_steps\": 8,\n",
    "    #@ context length and max length (max_new_token = max_length - max_prompt_length)\n",
    "    \"max_length\": 768,\n",
    "    \"max_prompt_length\":512,\n",
    "    ## Optimizer\n",
    "    \"optim\": \"adamw_torch\",\n",
    "    \"learning_rate\": 2.0e-4,\n",
    "    \"weight_decay\": 0.001,\n",
    "    \"adam_epsilon\": 1.0e-8,\n",
    "    \"adam_beta1\": 0.9,\n",
    "    \"adam_beta2\": 0.999,\n",
    "    \"max_grad_norm\": 1.0,\n",
    "    ## Scheduler ##\n",
    "    \"warmup_steps\": 10,\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    ## Logging\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_first_step\": True,\n",
    "    \"logging_steps\": 10\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03223d88-b7cf-4326-a7c1-d908886b47fa",
   "metadata": {},
   "source": [
    "### DPO Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "0d7c6796-a16a-4fb5-bae8-e68f2c6028f1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "accelerator = Accelerator()\n",
    "\n",
    "\n",
    "data_args = DataArguments(**data_params)\n",
    "training_args =  DPOConfig(**training_params)\n",
    "model_args = ModelConfig(**model_config)\n",
    "\n",
    "###################\n",
    "# Model & Tokenizer\n",
    "###################\n",
    "torch_dtype = (\n",
    "    model_args.torch_dtype\n",
    "    if model_args.torch_dtype in [\"auto\", None]\n",
    "    else getattr(torch, model_args.torch_dtype)\n",
    ")\n",
    "quantization_config = get_quantization_config(model_args)\n",
    "model_kwargs = dict(\n",
    "    revision=model_args.model_revision,\n",
    "    attn_implementation=model_args.attn_implementation,\n",
    "    torch_dtype=torch_dtype,\n",
    "    use_cache=False if training_args.gradient_checkpointing else True,\n",
    "    device_map=get_kbit_device_map() if quantization_config is not None else None,\n",
    "    quantization_config=quantization_config,\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, **model_kwargs\n",
    ")\n",
    "peft_config = get_peft_config(model_args)\n",
    "if peft_config is None:\n",
    "    ref_model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code, **model_kwargs\n",
    "    )\n",
    "else:\n",
    "    ref_model = None\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    model_args.model_name_or_path, trust_remote_code=model_args.trust_remote_code\n",
    ")\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.pad_token = tokenizer.eos_token\n",
    "if tokenizer.chat_template is None:\n",
    "    tokenizer.chat_template = SIMPLE_CHAT_TEMPLATE\n",
    "\n",
    "################\n",
    "# Dataset\n",
    "################\n",
    "dataset =  get_datasets(data_args, splits=data_args.dataset_splits)\n",
    "\n",
    "\n",
    "\n",
    "################\n",
    "# Training\n",
    "################\n",
    "trainer = DPOTrainer(\n",
    "    model,\n",
    "    ref_model,\n",
    "    args=training_args,\n",
    "    train_dataset=dataset[data_args.dataset_train_split],\n",
    "    eval_dataset=dataset[data_args.dataset_test_split].shuffle(data_args.seed)\\\n",
    "        .take(min(len(dataset[data_args.dataset_test_split]), data_args.num_eval_samples))\\\n",
    "        if training_args.eval_strategy != \"no\" else None,\n",
    "    processing_class=tokenizer,\n",
    "    peft_config=peft_config,\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "\n",
    "if training_args.eval_strategy != \"no\":\n",
    "    metrics = trainer.evaluate()\n",
    "    trainer.log_metrics(\"eval\", metrics)\n",
    "    trainer.save_metrics(\"eval\", metrics)\n",
    "\n",
    "# Save and push to hub\n",
    "trainer.save_model(training_args.output_dir)\n",
    "if training_args.push_to_hub:\n",
    "    trainer.push_to_hub(dataset_name=data_args.dataset_name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3502d66e-7c3f-418e-9c12-1bb1a780602f",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Your Turn!\n",
    "We randomly selected a subset, choosing **X** as the fraction. See below\n",
    "\n",
    "```python\n",
    "data_params = {\n",
    "  \"dataset_name\": \"Mix 1\",\n",
    "  \"dataset_mixer\": {\n",
    "    \"./data/AMLD25_reuters_gentitle_1k\": X,\n",
    "  },\n",
    "  \"dataset_splits\": [\"train\", \"test\"],\n",
    "  \"num_eval_samples\": 100,\n",
    "  \"seed\": 42\n",
    "}\n",
    "```\n",
    "\n",
    "Can we do a better selection? \n",
    "\n",
    "**Hint(s)**: \n",
    "1. Please give a deep look to the max context length (768) and `chosen` and `rejected` features\n",
    "2. Please check the other columns\n",
    "\n",
    "<details>\n",
    "<summary> <b>Solution Spoiler!</b> </summary>\n",
    "  Search in <code>src/trgpt/utils.py</code> for the solution (functions: <code>reuters_cleaning_dataset</code> and <code>not_relevant_data</code>) \n",
    "</details>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e215ec9-a0c1-4336-8806-fe4c7829b7df",
   "metadata": {},
   "source": [
    "## Give a look to the Model Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a52f7a8e-aeb6-42f4-ad81-4fe295434a85",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trlabs.utils import dataset_creation, not_relevant_data\n",
    "\n",
    "SYSTEM_PROMPT = 'You are an advanced AI system specialised in providing Reuters News title given a body text of the news.'\n",
    "INSTRUCTION = \"The title should be in capital letters and between 6 and 8 words in length. Please provide only the title as output and no other text or explanation.\"\n",
    "\n",
    "dataset = load_dataset(\"ucirvine/reuters21578\", 'ModApte', trust_remote_code=True)\n",
    "dataset = dataset.filter(not_relevant_data).shuffle(seed=42).map(dataset_creation, fn_kwargs={\"system_prompt\": SYSTEM_PROMPT, \"instruction\": INSTRUCTION})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "522d09df-b9d7-427e-8d14-71da31972d14",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trlabs.rl.eval import setup_model_and_lora, generate\n",
    "\n",
    "index =15\n",
    "prompt = dataset[\"test\"][index][\"system\"]+dataset[\"test\"][index][\"messages\"]\n",
    "\n",
    "model, tokenizer = setup_model_and_lora(\n",
    "    base_model_name = model_config[\"model_name_or_path\"], \n",
    "    lora_path = training_params[\"output_dir\"]\n",
    ")\n",
    "\n",
    "response = generate(prompt, model, tokenizer)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7b957a53-7a30-4718-96f5-508796386f8a",
   "metadata": {},
   "source": [
    "#### Note: \n",
    "if you do not provide a lora_path you can check the base model output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
