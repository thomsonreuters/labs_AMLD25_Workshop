{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create an AI pipeline for quiz generation, using LangGraph\n",
    "\n",
    "One way to boost customer engagement in the news industry is with news quizzes. The quiz would ask questions about the articles the user recently read.\n",
    "\n",
    "In this hands-on exercise, we will implement a quiz generator. To achieve this, we will use generative AI, retrieval augmented generation, and the LangGraph library. \n",
    "\n",
    "Here is an overview for the AI pipeline:\n",
    "- The user specifies a **topic** of interest\n",
    "- Search for corresponding news articles in the **Reuters dataset**\n",
    "- The user selects 1 from the top-3 new articles\n",
    "- Generate quiz which will have well-defined format\n",
    "\n",
    "![AI pipeline](assets/ai_pipeline.png \"AI pipeline for quiz generation\")"
   ],
   "id": "5f4b2ff659593dfc"
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": [
    "# Install necessary dependencies\n",
    "# - this takes ~3 minutes, give it some patience\n",
    "# - the imports can show error messages, you can ignore them\n",
    "\n",
    "!pip install unsloth\n",
    "!pip install langchain\n",
    "!pip install langgraph\n",
    "!pip install langchain_huggingface"
   ],
   "id": "e722463062e67bf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Do all necessary imports\n",
    "\n",
    "from typing import List, Dict\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from IPython.display import Image, display\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import PreTrainedTokenizer, PreTrainedModel"
   ],
   "id": "a76421ffb2a0b9b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare LLM for answer generation\n",
    "\n",
    "We use [Unsloth](https://docs.unsloth.ai) for LLM inference. If you prefer to use an LLM API instead, feel free to adjust the Notebook accordingly. Note that other parts of this workshop will also use Unsloth. "
   ],
   "id": "34ba6a439e1d6d65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate Unsloth model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ],
   "id": "2a5f23925a2447dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We use this helper function for LLM inference. You can leave it as-is.\n",
    "\n",
    "def llm_inference(\n",
    "        messages: List[Dict],\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    :param messages: Messages for the LLM \n",
    "    :param model: The initialized unsloth LLM\n",
    "    :param tokenizer: The pretrained tokenizer\n",
    "    :return: The LLM answer as raw string\n",
    "    \"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    input_tokens = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "    input_len = len(input_tokens.tokens())\n",
    "    output_tokens = model.generate(**input_tokens)\n",
    "    output_clipped = output_tokens[:, input_len:-1]\n",
    "    result = tokenizer.batch_decode(output_clipped)\n",
    "    return result[0]"
   ],
   "id": "d371046bff56f49e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Generate your first quiz\n",
    "\n",
    "To start off, we'll create our first quiz. For demo purposes, we'll use one hard-coded news article. The more important part is the LLM prompt. In the prompt, we give instructions for quiz generation. Moreover, we define the output format, which the LLM hopefully adheres to during quiz generation. "
   ],
   "id": "91b8eb7abcc3381d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a helper function for creating the LLM messages.\n",
    "def create_messages(news_article: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    :param news_article: The news article \n",
    "    :return: A quiz about the news article\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"Please generate one multiple choice quiz for the provided news article.\n",
    "            \n",
    "            The quiz should have the following format:\n",
    "            \n",
    "            [Question]\n",
    "            \n",
    "            [Choice 1]\n",
    "            [Choice 2]\n",
    "            [Choice 3]\n",
    "            \n",
    "            [Solution]\n",
    "            \"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Here is the news article: {news_article}\"\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ],
   "id": "ecdcc9ed9b6c63ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create quiz\n",
    "news_article = \"U.S. Agriculture Secretary Richard Lyng said he would not agree to an extension of the 18-month whole dairy herd buyout program set to expire later this year. Speaking at the Agriculture Department to representatives of the U.S. National Cattlemen\\'s Association, Lyng said some dairymen asked the program be extended. But he said the Reagan administration, which opposed the whole herd buyout program in the 1985 farm bill, would not agree to an extension. The program begun in early 1986, is to be completed this summer. U.S. cattlemen bitterly opposed the scheme, complaining that increased dairy cow slaughter drove cattle prices down last year. Reuter\"\n",
    "# TODO: generate messages for LLM\n",
    "# Use the above defined function \"create_messages()\".\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "messages: str = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# TODO: query the LLM for quiz generation\n",
    "# Use the above defined function \"llm_inference()\"\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "llm_response: str = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# TODO: Investigate the generated quiz. Does it have the desired scope and format?"
   ],
   "id": "bb9183d70f3869b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prompt adjustments\n",
    "\n",
    "Next, we'll try to tweek the messages for the LLM."
   ],
   "id": "c85a0600b0c90371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: adjust prompt\n",
    "def create_messages_more_answers(news_article: str) -> List[Dict]:\n",
    "    # Adjust the messages for the LLM, such that the generated quiz has 5 instead of 3 answers.\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    pass\n",
    "    # <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "messages: str = create_messages_more_answers(news_article)\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# TODO: re-query the LLM for quiz generation\n",
    "# Use the above defined function \"llm_inference()\"\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "llm_response: str = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<"
   ],
   "id": "f2db39bef365ffd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Can you think of any other way to adjust quiz generation?\n",
    "# - Increase difficulty\n",
    "# - ... any other ideas?"
   ],
   "id": "78d67718fd39aeb",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Reuters dataset\n",
    "\n",
    "Throughout this notebook, we'll be using the [Reuters news dataset](https://huggingface.co/datasets/ucirvine/reuters21578) from Hugging Face.\n",
    "We download it below. This dataset contains short articles from Reuters' financial newswire service from 1987. "
   ],
   "id": "1ddfe1dceca25493"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load dataset\n",
    "# - if asked to run custom code, type \"y\" for YES.\n",
    "reuters_ds = load_dataset('ucirvine/reuters21578','ModHayes')\n",
    "news_raw = reuters_ds[\"train\"].to_pandas()\n",
    "print(f\"Loaded {len(news_raw)} news articles.\")"
   ],
   "id": "94a44ec9bd1a3bae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocess news articles\n",
    "\n",
    "First we perform some preprocessing on the news data. We'll store all articles in a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). For each article, we keep the actual news text, plus the news title. On the resulting strings, we remove unwanted characters."
   ],
   "id": "118e60c1c8169d06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge title and text, drop unnecessary columns\n",
    "news_raw[\"title_and_text\"] = news_raw['title'] + ' | ' + news_raw['text']\n",
    "news = news_raw[[\"title_and_text\", \"date\", \"places\"]]"
   ],
   "id": "305fba345d5aa905",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clean up text, remove unnecessary characters\n",
    "pd.options.mode.chained_assignment = None\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\n\", \" \"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\\\\"\", \"\\\"\"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: \" \".join(x[\"title_and_text\"].split()), axis=1)\n",
    "news.head()"
   ],
   "id": "7fbc942723a24187",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create RAG database\n",
    "\n",
    "We want to support quiz generations for a user specified topic. For this reason, we create a vector store, where one can query news articles by topic."
   ],
   "id": "ecc7e44f22fbd204"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup RAG vector store\n",
    "# - This can take ~ 1 minute, give it some patience\n",
    "texts_to_encode = news['title_and_text'].to_list()\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "vectorstore = InMemoryVectorStore.from_texts(texts=texts_to_encode, embedding=embedder)"
   ],
   "id": "5a3eac69e5af7e84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Search articles about a given topic\n",
    "query = \"An article on agriculture\"\n",
    "k=3\n",
    "# TODO: use the vector store above to search for 3 news articles corresponding to \"agriculture\"\n",
    "# Hint: use InMemoryVectorStore's \"similarity_search()\" function\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "results: List[Document] = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# TODO: verify the articles are really about \"agriculture\""
   ],
   "id": "3cf224b658d7bb43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Check whether you can search for articles about \"coffee\"\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<"
   ],
   "id": "bf6568628279da42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create AI pipeline with LangGraph\n",
    "\n",
    "We now create a graph for quiz generation. Note that the graph is deterministic and doesn't use tools.\n",
    "Since time is short, we provide all code for creating the graph. Checkout more about LangGraph in their [official documentation](https://langchain-ai.github.io/langgraph/tutorials/introduction/).\n",
    "Our graph leverages the following features:\n",
    "- Maintain a conversation state\n",
    "- Human in the loop"
   ],
   "id": "26a878970cf65cfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"The state during graph traversal.\"\"\"\n",
    "    topic: str\n",
    "    documents_from_lookup: List[Document]\n",
    "    selected_document: Document\n",
    "    quiz: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    \"\"\"Search for news articles according to the topic selected by the user.\"\"\"\n",
    "    documents_from_lookup = vectorstore.similarity_search(state[\"topic\"], k=3)\n",
    "    return {\"documents_from_lookup\": documents_from_lookup}\n",
    "\n",
    "def human_feedback(state: State):\n",
    "    \"\"\"Let the user choose one article, on which the quiz will be based.\"\"\"\n",
    "    article_selection = interrupt(\"Let user choose article\")\n",
    "    selected_document=state[\"documents_from_lookup\"][int(article_selection)]\n",
    "    return {\"selected_document\": selected_document}\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\"Generate a quiz.\"\"\"\n",
    "    news_article = state[\"selected_document\"].page_content\n",
    "    messages = create_messages(news_article)\n",
    "    llm_response = llm_inference(messages, model, tokenizer)\n",
    "    return {\"quiz\": llm_response}\n",
    "\n",
    "# Build the graph with all nodes and edges\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"retrieve\", retrieve)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"generate\", generate)\n",
    "builder.add_edge(START, \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"human_feedback\")\n",
    "builder.add_edge(\"human_feedback\", \"generate\")\n",
    "builder.add_edge(\"generate\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "e5598541addf00d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Run the graph\n",
    "\n",
    "We now run the AI pipeline to generate the quiz. Note this is interactive. The user first needs to select a topic, and later needs to choose an article from a list of proposals."
   ],
   "id": "8ef9012d1c786b66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run the graph from the start, until user selection step\n",
    "topic = input(\"Please select your topic: \")\n",
    "initial_input = {\"topic\": topic}\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"updates\"):\n",
    "    pass\n",
    "# Display article options\n",
    "article_options=graph.get_state(config=thread).values['documents_from_lookup']\n",
    "print(\"Article candidates:\")\n",
    "for id, doc in enumerate(article_options):\n",
    "    content = doc.page_content\n",
    "    print(f\"[{id}] {content}\")"
   ],
   "id": "d5b39e98d03c7ae5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: have a look at the retrieved news articles, which one is most suitable for quiz generation?",
   "id": "5ab038fa90b94cc0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get human feedback\n",
    "human_feedback = input(\"Please select which article you'd like to use [0,1,2]: \")\n",
    "\n",
    "# Continue the graph execution\n",
    "for event in graph.stream(\n",
    "        Command(resume=human_feedback), thread, stream_mode=\"updates\"\n",
    "):\n",
    "    pass\n",
    "\n",
    "# Show final quiz\n",
    "quiz=graph.get_state(config=thread).values['quiz']\n",
    "print(quiz)"
   ],
   "id": "76a67528accf8327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "# TODO: Review the quiz",
   "id": "678c2fbcd55eaab2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adjust AI pipeline",
   "id": "48b2d96b86fb7942"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Make the quiz more entertaining. Here are some improvement ideas:\n",
    "# - Show the original news article above the quiz, when displaying the quiz to the user.\n",
    "# - The final output should include the year of the news article."
   ],
   "id": "c46274aca36e06a4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
