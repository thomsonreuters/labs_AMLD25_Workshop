{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5f4b2ff659593dfc",
   "metadata": {},
   "source": [
    "# Quiz generation with LangGraph\n",
    "\n",
    "One way to boost customer engagement in the news industry is via custom quizzes. A generated quiz would ask questions about the articles the user recently read.\n",
    "\n",
    "In this hands-on exercise, we will implement a quiz generator. To achieve this, we will use generative AI, retrieval augmented generation (RAG), and the [LangGraph library](https://langchain-ai.github.io/langgraph/tutorials/introduction/).\n",
    "\n",
    "Here is an overview for the AI pipeline:\n",
    "- The user specifies a **topic** of interest\n",
    "- Search for corresponding news articles in the **Reuters dataset**\n",
    "- The user selects 1 from the top-3 new articles\n",
    "- Generate a multiple-choice quiz\n",
    "\n",
    "![AI pipeline](assets/ai_pipeline.png \"AI pipeline for quiz generation\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3828bac1",
   "metadata": {},
   "source": [
    "## Instructions for workshop participants\n",
    "\n",
    "Ensure you understand the project description above. If you have any questions, reach out to a workshop host.\n",
    "\n",
    "Next, we start with the implementation of the RAG system. Make sure you understand the content of each notebook cell, and execute one cell after another.\n",
    "\n",
    "In some places, there are open tasks that you should work on. There tasks are marked as follows:\n",
    "\n",
    "```\n",
    "# >>>>>>>>>>>>>>>>>>\n",
    "# TODO: <Some instructions ...>\n",
    "# <your code should go here>\n",
    "# <<<<<<<<<<<<<<<<<<\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e722463062e67bf4",
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "outputs": [],
   "source": [
    "# Install necessary dependencies\n",
    "# - Note this can take a minute, give it some patience\n",
    "# - Note there can be warning / error messages during installation, you can ignore those\n",
    "\n",
    "!pip install langchain\n",
    "!pip install langgraph\n",
    "!pip install langchain_huggingface\n",
    "!pip install datasets==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a76421ffb2a0b9b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all necessary imports\n",
    "\n",
    "from typing import List\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34ba6a439e1d6d65",
   "metadata": {},
   "source": [
    "## Setup function to call Google AI Studio\n",
    "\n",
    "Next, we prepare the Python code to call the LLM API of Google. Make sure you have your API key ready, as explained in `sessions/1_prompt_engineering_and_rag/README.md`.\n",
    "\n",
    "We use the [GenAI library from Google](https://pypi.org/project/google-genai/) for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "181f9c1f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM API key\n",
    "# >>>>>>>>>>>>>>>>>>\n",
    "# TODO: add your LLM API key here. You can get your key from Google AI Studio.\n",
    "# Note: More instructions for creating the API key can be found in `sessions/1_prompt_engineering_and_rag/README.md`\n",
    "google_llm_api_key: str = \"<key goes here>\"\n",
    "# <<<<<<<<<<<<<<<<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a5f23925a2447dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare client for Google LLM API\n",
    "client = genai.Client(api_key=google_llm_api_key)\n",
    "\n",
    "\n",
    "def llm_generate_response(system_message: str, user_message: str) -> str:\n",
    "    \"\"\"\n",
    "    Use an LLM to answer a question.\n",
    "\n",
    "    :param system_message: The general instructions for the LLM, which shapes the AI's general behavior.\n",
    "    :param user_message: The question from the user.\n",
    "    :return: The response from the LLM.\n",
    "    \"\"\"\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-preview-04-17\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_message),\n",
    "        contents=user_message\n",
    "    )\n",
    "    return response.text\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b8eb7abcc3381d",
   "metadata": {},
   "source": [
    "## Generate your first quiz\n",
    "\n",
    "To start off, we'll create our first quiz. For demo purposes, we'll use a fix news article about a dairy herd buyout program.\n",
    "\n",
    "The most important part here is the LLM prompt. In the prompt, we give instructions for quiz generation. Moreover, we define the output format, which the LLM hopefully adheres to during quiz generation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ecdcc9ed9b6c63ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quiz(news_article: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a quiz based on the provided news article.\n",
    "    Put the news article in the LLM context windows.\n",
    "    For LLM answer generation, use the Google AI Studio API.\n",
    "\n",
    "    :param news_article: The news article \n",
    "    :return: A quiz about the news article\n",
    "    \"\"\"\n",
    "    system_message = \"\"\"Please generate one multiple choice quiz for the provided news article.\n",
    "\n",
    "The quiz should have the following format:\n",
    "\n",
    "[Question]\n",
    "\n",
    "[Choice 1]\n",
    "[Choice 2]\n",
    "[Choice 3]\n",
    "\n",
    "[Solution]\n",
    "\"\"\"\n",
    "    user_message = f\"Here is the news article: {news_article}\"\n",
    "\n",
    "    return llm_generate_response(user_message, system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb9183d70f3869b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create quiz for one new article\n",
    "news_article = \"U.S. Agriculture Secretary Richard Lyng said he would not agree to an extension of the 18-month whole dairy herd buyout program set to expire later this year. Speaking at the Agriculture Department to representatives of the U.S. National Cattlemen\\'s Association, Lyng said some dairymen asked the program be extended. But he said the Reagan administration, which opposed the whole herd buyout program in the 1985 farm bill, would not agree to an extension. The program begun in early 1986, is to be completed this summer. U.S. cattlemen bitterly opposed the scheme, complaining that increased dairy cow slaughter drove cattle prices down last year. Reuter\"\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>\n",
    "# TODO: generate quiz for the above news article\n",
    "# Note: Use the above defined function \"generate_quiz()\".\n",
    "quiz: str = ...\n",
    "# <<<<<<<<<<<<<<<<<<\n",
    "\n",
    "# Display the generated quiz\n",
    "print(quiz)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "748258a7",
   "metadata": {},
   "source": [
    "## TODO: Investigate the generated quiz.\n",
    "\n",
    "Does it have the desired scope and format?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c85a0600b0c90371",
   "metadata": {},
   "source": [
    "## Prompt adjustments\n",
    "\n",
    "Next, we'll try to tweek the format of the generated quiz. Let's create 5 potential answers, rather than 3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2db39bef365ffd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_quiz_5_options(news_article: str) -> str:\n",
    "    \"\"\"\n",
    "    Generate a quiz with 5 candidate answers.\n",
    "\n",
    "    :param news_article: The news article \n",
    "    :return: A quiz about the news article\n",
    "    \"\"\"\n",
    "\n",
    "    # >>>>>>>>>>>>>>>>>>\n",
    "    # TODO: define a system message.\n",
    "    # Make sure the system message contains the necessary instructions for the quiz format.\n",
    "    # The generated quiz should have 5 candidate answers, not 3.\n",
    "    # Use the previously defined system message as a reference.\n",
    "    system_message: str = ...\n",
    "    # <<<<<<<<<<<<<<<<<<\n",
    "\n",
    "    user_message = f\"Here is the news article: {news_article}\"\n",
    "\n",
    "    return llm_generate_response(user_message, system_message)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d02eb486",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, we're ready to create a quiz with 5 candidate answers.\n",
    "quiz = generate_quiz_5_options(news_article)\n",
    "print(quiz)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97afb250",
   "metadata": {},
   "source": [
    "## TODO: Analyze generated quiz\n",
    "\n",
    "Does the generated quiz have the desired format?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fc9a97a1",
   "metadata": {},
   "source": [
    "## TODO: More prompt engineering\n",
    "\n",
    "Can you think of any other way to adjust quiz generation?\n",
    "- Increase quiz difficulty\n",
    "- anything else?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfe1dceca25493",
   "metadata": {},
   "source": [
    "## Load Reuters dataset\n",
    "\n",
    "Throughout this notebook, we'll be using the [Reuters news dataset](https://huggingface.co/datasets/ucirvine/reuters21578) from Hugging Face.\n",
    "We download it below. This dataset contains short articles from Reuters' financial newswire service from 1987. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94a44ec9bd1a3bae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# - if asked to run custom code, type \"y\" for YES.\n",
    "reuters_ds = load_dataset('ucirvine/reuters21578','ModHayes')\n",
    "news_raw = reuters_ds[\"train\"].to_pandas()\n",
    "print(f\"Loaded {len(news_raw)} news articles.\")\n",
    "news_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "118e60c1c8169d06",
   "metadata": {},
   "source": [
    "## Preprocess news articles\n",
    "\n",
    "First we perform some preprocessing on the news data. We'll store all articles in a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "We do the following data processing:\n",
    "- Concatenate the news text with the news title\n",
    "- Remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "305fba345d5aa905",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge title and text, drop unnecessary columns\n",
    "news_raw[\"title_and_text\"] = news_raw['title'] + ' | ' + news_raw['text']\n",
    "news = news_raw[[\"title_and_text\", \"date\", \"places\"]]\n",
    "\n",
    "# Clean up text, remove unnecessary characters\n",
    "pd.options.mode.chained_assignment = None\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\n\", \" \"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\\\\"\", \"\\\"\"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: \" \".join(x[\"title_and_text\"].split()), axis=1)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc7e44f22fbd204",
   "metadata": {},
   "source": [
    "## Create RAG database\n",
    "\n",
    "We want to support quiz generations for a user specified topic. For this reason, we create a vector store, where one can query news articles by topic."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a3eac69e5af7e84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Setup RAG vector store\n",
    "# - Make sure you have a GPU available to incease the embedding speed.\n",
    "# - This can take a minute, give it some patience\n",
    "\n",
    "texts_to_encode = news['title_and_text'].to_list()\n",
    "# This is a small transformer for computing text embeddings.\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "# This is the RAG database.\n",
    "vectorstore = InMemoryVectorStore.from_texts(texts=texts_to_encode, embedding=embedder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cf224b658d7bb43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search articles about agriculture\n",
    "query = \"An article on agriculture\"\n",
    "k=3\n",
    "result = vectorstore.similarity_search(query, k=k)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1dd001f2",
   "metadata": {},
   "source": [
    "## TODO: Verify the articles are really about \"agriculture\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "595f7dd8",
   "metadata": {},
   "source": [
    "## Search for articles about \"coffee\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6568628279da42",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>\n",
    "# TODO: Check whether you can find articles about \"coffee\"\n",
    "# Retrieve the top-10 articles from our RAG database.\n",
    "# Use the code above as a reference.\n",
    "# <<<<<<<<<<<<<<<<<<"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26a878970cf65cfd",
   "metadata": {},
   "source": [
    "## Create a quiz generation pipeline with LangGraph\n",
    "\n",
    "We are now ready to create our AI pipeline with LangGraph. We'll create a simple graph, which should be traversed in linear fashion, and perform all sub-steps necessary for quiz generation.\n",
    "Since time is short, we provide all code for creating the graph. Checkout more about LangGraph in their [official documentation](https://langchain-ai.github.io/langgraph/tutorials/introduction/).\n",
    "Our graph leverages the following features:\n",
    "- Maintain a conversation state\n",
    "- Human in the loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5598541addf00d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class State(TypedDict):\n",
    "    \"\"\"The state during graph traversal.\"\"\"\n",
    "    topic_from_user: str\n",
    "    relevant_articles_from_reuters: List[Document]\n",
    "    article_selected_by_user: Document\n",
    "    quiz_result: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    \"\"\"Search for news articles in the RAG database.\"\"\"\n",
    "    relevant_articles_from_reuters = vectorstore.similarity_search(state[\"topic_from_user\"], k=3)\n",
    "    return {\"relevant_articles_from_reuters\": relevant_articles_from_reuters}\n",
    "\n",
    "def human_feedback(state: State):\n",
    "    \"\"\"Let the user choose one article. The generated quiz will be about this article.\"\"\"\n",
    "    article_selection = interrupt(\"Let user choose article\")\n",
    "    article_selected_by_user=state[\"relevant_articles_from_reuters\"][int(article_selection)]\n",
    "    return {\"article_selected_by_user\": article_selected_by_user}\n",
    "\n",
    "def generate(state: State):\n",
    "    \"\"\"Generate a quiz for one news article.\"\"\"\n",
    "    news_article = state[\"article_selected_by_user\"].page_content\n",
    "    quiz_result = generate_quiz(news_article)\n",
    "    return {\"quiz_result\": quiz_result}\n",
    "\n",
    "# Build the graph with all nodes and edges\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"retrieve\", retrieve)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"generate\", generate)\n",
    "builder.add_edge(START, \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"human_feedback\")\n",
    "builder.add_edge(\"human_feedback\", \"generate\")\n",
    "builder.add_edge(\"generate\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ef9012d1c786b66",
   "metadata": {},
   "source": [
    "## Run the graph\n",
    "\n",
    "We now run the AI pipeline to generate the quiz. Note this is interactive. The user first needs to select a topic, and later needs to choose an article from a list of proposals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5b39e98d03c7ae5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the graph from the start.\n",
    "# The execution will pause for human feedback. In this step, the user has to select from a list of article proposals.\n",
    "topic_from_user = input(\"Please select a topic for news article generation:\")\n",
    "initial_state = {\"topic_from_user\": topic_from_user}\n",
    "for event in graph.stream(initial_state, thread, stream_mode=\"updates\"):\n",
    "    pass\n",
    "# Display article options on console\n",
    "relevant_articles_from_reuters=graph.get_state(config=thread).values['relevant_articles_from_reuters']\n",
    "print(\"Article candidates:\")\n",
    "for id, doc in enumerate(relevant_articles_from_reuters):\n",
    "    content = doc.page_content\n",
    "    print(f\"[{id}] {content}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cd36489",
   "metadata": {},
   "source": [
    "## TODO: select article\n",
    "\n",
    "Have a look at the retrieved news articles in the console. Which one should serve as base for the quiz generation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76a67528accf8327",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Continue the execution of the graph\n",
    "# As a next step, we ask for human feedback\n",
    "# The user selects a news article, by provividing an ID (0, 1, or 2)\n",
    "article_selection = input(\"Please select which article you'd like to use [0,1,2]: \")\n",
    "\n",
    "# Finish graph execution\n",
    "for event in graph.stream(\n",
    "        Command(resume=article_selection), thread, stream_mode=\"updates\"\n",
    "):\n",
    "    pass\n",
    "\n",
    "# Show final quiz\n",
    "quiz_result=graph.get_state(config=thread).values['quiz_result']\n",
    "print(quiz_result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df652930",
   "metadata": {},
   "source": [
    "## TODO: Review the quiz\n",
    "\n",
    "Is the quiz about the chosen news article?\n",
    "Does the quiz have the expected format?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48b2d96b86fb7942",
   "metadata": {},
   "source": [
    "## TODO: Extend and improve the AI pipeline\n",
    "\n",
    "Make the quiz more entertaining. Here are some improvement ideas:\n",
    "- Show the original news article above the quiz, when displaying the quiz to the user.\n",
    "- Include the year of the news in the final quiz"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
