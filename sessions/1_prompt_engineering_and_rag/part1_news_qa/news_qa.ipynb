{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Create a RAG-based question answering system\n",
    "\n",
    "In this Notebook, we will create a RAG-based Q&A system.\n",
    "\n",
    "Our goal is to leverage the [Reuters News dataset](https://huggingface.co/datasets/ucirvine/reuters21578) and answer some questions around the [JAL airplane crash](https://en.wikipedia.org/wiki/Japan_Air_Lines_Flight_123). Japan Air Lines Flight 123 was a 1985 flight which left Tokyo towards Osaka. Initially it was unclear why the airplane crashed, and various theories emerged over time. We'll try to leverage RAG and an LLM to find out more about the root cause.\n",
    "\n",
    "![JAL airplane](assets/jal_airplane.png \"JAL airplane\")\n",
    "\n",
    "We will create a RAG database as follows:\n",
    "- Fetch the Reuters news dataset from Hugging Face\n",
    "- Do some data preprocessing and cleaning\n",
    "- Embed each news article using a sentence transformer\n",
    "- Implement a search function with cosine similarity\n",
    "\n",
    "The Q&A pipeline works as follows:\n",
    "- The user can ask any news-related question\n",
    "- The question gets embedded with the same sentence transformer as above\n",
    "- Find the news article most similar to the question\n",
    "- Query an LLM with the user question, and provide relevant news article in the context window\n",
    "\n",
    "![Question answering](assets/question_answering.png \"A question answering system using RAG\")"
   ],
   "id": "61b8262a3c0617ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install necessary dependencies\n",
    "# - this takes ~3 minutes, give it some patience\n",
    "# - the imports can show error messages, you can ignore them\n",
    "\n",
    "!pip install unsloth"
   ],
   "id": "a95272e1ceeba843",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2cdbd233",
   "metadata": {},
   "source": [
    "# Do all necessary imports\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Prepare LLM for answer generation\n",
    "\n",
    "We use [Unsloth](https://docs.unsloth.ai) for LLM inference. If you prefer to use an LLM API instead, feel free to adjust the Notebook accordingly. Note that other parts of this workshop will also use Unsloth. "
   ],
   "id": "8bccc36c46c1691c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate Unsloth model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ],
   "id": "e1a2ed9d3068323",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# We use this helper function for LLM inference. You can leave it as-is.\n",
    "\n",
    "def llm_inference(\n",
    "        messages: List[Dict],\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer\n",
    ") -> str:\n",
    "    \"\"\"\n",
    "    :param messages: Messages for the LLM \n",
    "    :param model: The initialized unsloth LLM\n",
    "    :param tokenizer: The pretrained tokenizer\n",
    "    :return: The LLM answer as raw string\n",
    "    \"\"\"\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    input_tokens = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "    input_len = len(input_tokens.tokens())\n",
    "    output_tokens = model.generate(**input_tokens)\n",
    "    output_clipped = output_tokens[:, input_len:-1]\n",
    "    result = tokenizer.batch_decode(output_clipped)\n",
    "    return result[0]"
   ],
   "id": "95afad295ccb742a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Reuters dataset\n",
    "\n",
    "Throughout this notebook, we'll be using the [Reuters news dataset](https://huggingface.co/datasets/ucirvine/reuters21578) from Hugging Face.\n",
    "We download it below. This dataset contains short articles from Reuters' financial newswire service from 1987. "
   ],
   "id": "e978269eb4f5465b"
  },
  {
   "cell_type": "code",
   "id": "b0752f05",
   "metadata": {},
   "source": [
    "# Load dataset\n",
    "# - if asked to run custom code, type \"y\" for YES.\n",
    "reuters_ds = load_dataset('ucirvine/reuters21578','ModHayes')\n",
    "news_raw = reuters_ds[\"train\"].to_pandas()\n",
    "print(f\"Loaded {len(news_raw)} news articles.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Preprocess news articles\n",
    "\n",
    "First we perform some preprocessing on the news data. We'll store all articles in a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). For each article, we keep the actual news text, plus the news title. On the resulting strings, we remove unwanted characters."
   ],
   "id": "367662ece741e082"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge title and text, drop unnecessary columns\n",
    "news_raw[\"title_and_text\"] = news_raw['title'] + ' | ' + news_raw['text']\n",
    "news = news_raw[[\"title_and_text\", \"date\", \"places\"]]"
   ],
   "id": "bf6484bcb778181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clean up text, remove unnecessary characters\n",
    "pd.options.mode.chained_assignment = None\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\n\", \" \"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\\\\"\", \"\\\"\"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: \" \".join(x[\"title_and_text\"].split()), axis=1)\n",
    "news.head()"
   ],
   "id": "50459df97e9ee19c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Semantic embedding\n",
    " \n",
    "Next, we embed each news article using a pretrained sentence transformer. You can use any transformer, e.g. the Hugging Face [SentenceTransformer](https://huggingface.co/sentence-transformers) `all-MiniLM-L6-v2`."
   ],
   "id": "163dbf0d26628f5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Embed all news articles\n",
    "\n",
    "texts_to_encode = news['title_and_text'].to_list()\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# TODO: Compute the news article embeddings.\n",
    "# Use the SentenceTransformer function `encode()`.\n",
    "# expected result: `semantic_embeddings` as Numpy array with dimensions (number of articles, embedding features)\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "semantic_embeddings = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<"
   ],
   "id": "aff5b9732075642",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Search in the embedding space\n",
    "\n",
    "Next, we implement a function `semantic_search()` which can find the most relevant news articles for a given query.\n",
    "The function should return the best articles with the highest cosine similarity.  "
   ],
   "id": "bbf0f72bd7d7a63b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def semantic_search(query: str,\n",
    "                    top_k: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform semantic search for a given query.\n",
    "    \n",
    "    :param query: The question we'll try to answer. We use the question to search for relevant news articles.  \n",
    "    :param top_k: Search for the top k most suitable news articles\n",
    "    :return: A pandas DataFrame with the most similar article texts and their respective semantic scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Write results to new DataFrame\n",
    "    results = news.copy()\n",
    "    # TODO: Encode the query\n",
    "    # Encode the query using the same sentence transformer as for embedding the news articles.\n",
    "    # The resulting Numpy array should have dimensions (1, number of embedding features).\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    query_embedding = ...\n",
    "    # <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    # TODO: Calculate cosine similarity\n",
    "    # Each vector inside \"semantic_embeddings\" should be compared to the vector \"query_embedding\".\n",
    "    # Do some research in the scikit-learn library reference to find an appropriate function for computing cosine similarity.\n",
    "    # The resulting Numpy array should have dimensions (number of news articles, )\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    semantic_similarities = ...\n",
    "    # <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    # Add semantic similarity score as column to the DataFrame\n",
    "    results['semantic_score'] = semantic_similarities\n",
    "    # Get indices of top-k results\n",
    "    top_k_indices = np.argsort(semantic_similarities)[-top_k:][::-1]\n",
    "    # Only keep top-k results\n",
    "    results = results.iloc[top_k_indices]\n",
    "    return results"
   ],
   "id": "723fc448e532e416",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Search for an article related to the JAL airplane crash\n",
    "\n",
    "query = \"What caused the crash of the JAL plane?\"\n",
    "search_results = semantic_search(query=query, top_k=5)\n",
    "search_results.head()"
   ],
   "id": "9b732f430f062812",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Review the semantic search results\n",
    "# Take a look at the 5 search results. Which of them are really relevant to the query? "
   ],
   "id": "a763d0b83e7b7558",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ae13e78",
   "metadata": {},
   "source": [
    "## Combine semantic search with GenAI\n",
    "\n",
    "Use 1-shot RAG to answer the user question about the JAL airplane crash. Combine the strengths of information retrieval with the astonishing capabilities of generative artificial intelligence. Ground the LLM in facts, by providing relevant news articles in the context window.  "
   ]
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define a helper function for creating the LLM messages.\n",
    "def create_messages(context: List[str], question: str) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    :param context: A list of news articles \n",
    "    :param question: The user question\n",
    "    :return: The messages ready to be fed to the LLM API\n",
    "    \"\"\"\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a assistant specialized in answering questions about the news. Answer the questions provided by the user as requested based on the provided articles. Provide the long form answer based on them explaining and summarizing all the details. The related news are following: {context}\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"{question}\"\n",
    "        },\n",
    "    ]\n",
    "    return messages"
   ],
   "id": "ebe6212585be7b4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6fe695f3",
   "metadata": {},
   "source": [
    "# TODO: Perform retrieval augmented generation\n",
    "# 1-shot RAG: We only retrieve the top rated news article\n",
    "# First, grab the top-1 news article from the \"search_results\"\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "context: List[str] = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# Next, create the LLM messages, using the function defined in the previous notebook cell\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "messages: List[Dict] = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# Finally, query the LLM, using the function \"llm_inference()\".\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "llm_response: str = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# TODO: Investigate the result, is it grounded in truth?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8a1caae",
   "metadata": {},
   "source": [
    "## Few-shot RAG\n",
    "\n",
    "Next, we want to improve the answer to our question. Above, the LLM was only grounded using 1 news article. This limits the factual details for the LLM to give an extensive reply.\n",
    "We now switch to using the top-5 news articles, and add them to the context windows for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "id": "d186d688",
   "metadata": {},
   "source": [
    "# TODO: Perform few-shot RAG, leveraging 5 news articles\n",
    "# Grab the 5 most relevant news articles from \"search_results\".\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "context = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# Proceed as before, create the LLM messages, and then query the LLM with a larger context\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "messages = ...\n",
    "llm_response = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# Investigate the results. Do we get a more detailed answer now? What has changed?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Limitations of semantic search\n",
    "\n",
    "We managed to find the root cause of the JAL airplane crash. Now we switch to a new topic.\n",
    "\n",
    "We now increase the difficulty of the question. The new question goes as follows:\n",
    "> \"What will be constructed in Marne-la-Vallee?\"\n",
    "\n",
    "The new query contains a very specific geographical location. Semantic search can fail in such circumstances."
   ],
   "id": "85faac1ae1473a48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Demo of a semantic search that fails\n",
    "\n",
    "query = \"What will be constructed in Marne-la-Vallee?\"\n",
    "search_results = semantic_search(query=query, top_k=5)\n",
    "search_results.head()"
   ],
   "id": "14e85bfde86d598a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analyze the results\n",
    "\n",
    "TODO: Do you see any relevant articles in the results about \"Marne-la-Vallee\"?"
   ],
   "id": "b57e8c6559877951"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## What happens when the LLM doesn't get relevant information\n",
    "\n",
    "Let us now investigate what happens when we perform RAG, and the top-rated articles are not answering the user question. How does the LLM react when it doesn't have enough information?\n",
    "Until some months ago, most LLMs would start hallucinating when they don't have enough information to answer a very specific question.\n",
    "Nowadays, LLMs are becoming better and can frequently spot this situation. If they do, they clarify they need more up to date information, and refuse to answer the question."
   ],
   "id": "950edfa7c4c8ad3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Perform few-shot RAG for the case where semantic search fails\n",
    "# Grab the 5 most relevant news articles from \"search_results\".\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "context = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# Create the LLM messages and query the LLM\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "messages = ...\n",
    "llm_response = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# TODO: Investigate the results. Does the LLM try to answer the question, even without up-to-date information?"
   ],
   "id": "afdedff0f279a754",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c275001",
   "metadata": {},
   "source": [
    "# Hybrid search: use semantics plus word frequencies\n",
    "\n",
    "As seen above, some questions are harder to answer than others. When asking about the construction of a new town called [Marne-la-Vallée](https://en.wikipedia.org/wiki/Val_d%27Europe) by Walt Disney in France,\n",
    "the semantic search fails. To improve the search, we'll use a combination of semantics plus word frequencies ([TF-IDF](https://en.wikipedia.org/wiki/Tf–idf)).\n",
    "\n",
    "As an FYI, Marne-la-Vallée was a joint project between the French government and the Walt Disney Company. The project started 1987 and included six municipalities, a Disneyland Park, and a shopping center.\n",
    "\n",
    "![Marne-la-Vallee](assets/marne_la_vallee.png \"Marne-la-Vallee\")\n",
    "\n",
    "As mentioned, we will improve the news article search by leveraging TF-IDF. With the new data pipeline, the news article search will work as follows:\n",
    "- Embed the news articles and the question with a sentence transformer. Find the news articles with the most similar embedding.\n",
    "- Encode the news articles and the question with TF-IDF. Again, find the best news articles matches, this time based on TF-IDF encodings.\n",
    "- Each of the above encoding methods yields a similarity rank for every news articles.\n",
    "- Use [Reciprocal rank fusion](https://dl.acm.org/doi/abs/10.1145/1571941.1572114) (RRF) to turn the two ranks into one final rank.\n",
    "\n",
    "As we shall see, the new search mechanism will find better results for the following question:\n",
    "\n",
    "\"What will be constructed in Marne-la-Vallee?\"\n",
    "\n",
    "![Hybrid search](assets/hybrid_search.png \"Search with sentence transformer plus TF-IDF\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Encode news articles with TF-IDF\n",
    "\n",
    "We'll compute word frequencies for every news article, leveraging [scikit-learns's TF-IDF implementation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ],
   "id": "5ba7dc3bb273aaca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Initialize a TF-IDF object\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True, stop_words=\"english\"\n",
    ")\n",
    "# TODO: Compute TF-IDF encodings for every news article\n",
    "# Encode all news articles, which we stored in the variable \"texts_to_encode\".\n",
    "# Study the documentation for scikit-learns TfidfVectorizer class, if necessary.\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "tfidf_corpus = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<"
   ],
   "id": "3c808d43bcfbf31f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hybrid search\n",
    "\n",
    "We now combine semantic search with TF-IDF similarity. Let's create a function which performs a lookup in our news database.\n",
    "The function should use \"Reciprocal rank fusion\" for combining the semantic ranks and the word frequency ranks."
   ],
   "id": "8289389b9dd15c87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def hybrid_search(query: str, top_k: int, rrf_k = 60.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform a hybrid search, using both semantics and word frequencies.\n",
    "    \n",
    "    :param query: The question we'll try to answer. We use the question to search for relevant news articles.  \n",
    "    :param top_k: Search for the top k most suitable news articles\n",
    "    :param rrf_k: A hyper-parameter for Reciprocal rank fusion\n",
    "    :return: A pandas dataframe with the most relevant news articles and their RRF rank\n",
    "    \"\"\"\n",
    "\n",
    "    # Write results to new DataFrame\n",
    "    results = news.copy()\n",
    "    # TODO: Encode query and compute cosine score for semantic similarities\n",
    "    # Note: You have already implemented this in \"semantic_search()\". You can copy your code here.\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    query_embedding = ...\n",
    "    semantic_similarities = ...\n",
    "    # <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    # Add semantic similarity score as column to the DataFrame\n",
    "    results['semantic_score'] = semantic_similarities\n",
    "    \n",
    "    # TODO: Compute TF-IDF encoding of query\n",
    "    # Note: you've already encoded the news articles with TF-IDF, do the same here for the query\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    tfidf_encoding = ...\n",
    "    # <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    # TODO: Compute cosine similarities, this time for TF-IDF encodings\n",
    "    # The comparison should happend between \"tfidf_corpus\" and \"tfidf_similarities\"\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    tfidf_similarities = ...\n",
    "    # <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    results['tfidf_score'] = tfidf_similarities\n",
    "\n",
    "    # Compute the semantic and TF-IDF ranks.\n",
    "    # Note: Ranks start at 1, which is the best rank\n",
    "    semantic_ranks = np.argsort(-semantic_similarities.ravel()).argsort() + 1\n",
    "    tfidf_ranks = np.argsort(-tfidf_similarities.ravel()).argsort() + 1\n",
    "    # TODO: Calculate RRF ranks, which combine the semantic and word frequency rank\n",
    "    # Use the formula from today's presentation for the RRF rank.\n",
    "    # Note: higher means better.\n",
    "    # >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "    rrf_rank = ...\n",
    "    # <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "    results['rrf_rank'] = rrf_rank\n",
    "\n",
    "    # Get top-k results\n",
    "    top_k_indices = np.argsort(-rrf_rank)[:top_k]\n",
    "    results = results.iloc[top_k_indices]\n",
    "    return results"
   ],
   "id": "d3b32292a685b557",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Test the hybrid search\n",
    "\n",
    "top_k = 5\n",
    "query = \"What will be constructed in Marne-la-Vallee?\"\n",
    "# TODO: Run a hybrid search\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "search_results = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<"
   ],
   "id": "89edeec0086394de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analyze the search results\n",
    "\n",
    "TODO: Do the new search results contain more relevant information about \"Marne-la-Vallee\"? Are there some irrelevant articles in the results?"
   ],
   "id": "60d919fc92cd04dc"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Perform RAG, this time with hybrid search",
   "id": "f9379d21b87dbdf"
  },
  {
   "cell_type": "code",
   "id": "9bbdd0b1",
   "metadata": {},
   "source": [
    "# TODO: Look up relevant articles with hybrid search, add the articles to the LLM context window, and let the LLM answer the question about Marne-la-Vallee.\n",
    "# Use 5-short RAG. We already have the search results available in \"search_results\".\n",
    "# >>>>>>>>>>>>>>>>>>>>>>>>>\n",
    "context = ...\n",
    "messages = ...\n",
    "llm_response = ...\n",
    "# <<<<<<<<<<<<<<<<<<<<<<<<<\n",
    "# TODO: Analyze the LLM response. Is it able to answer the question factually? Does the LLM manage to ignore irrelevant news articles?"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Further improvements to the search algorithm\n",
    "\n",
    "Can you think of any additional ways for improving our RAG pipeline? Here are some ideas:\n",
    "- Give more weight to news **titles** than **texts**\n",
    "- Leverage article **release dates**, put more weight on the most recent article\n",
    "- Use a more sophisticated LLM for semantic embedding\n",
    "- ... any other ideas?"
   ],
   "id": "482f7741639c5561"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
