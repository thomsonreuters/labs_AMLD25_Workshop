{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b8262a3c0617ed",
   "metadata": {},
   "source": [
    "# Create a RAG-based question answering system\n",
    "\n",
    "In this Notebook, we will create a RAG-based Q&A system. The user can ask any question, and we leverage a Reuters news dataset to create a grounded answer.\n",
    "\n",
    "RAG (retrieval-augmented generation) is a technique to help an AI system generate more accurate answers. For this purpose, we take the user question, search through a large database, retrieve relevant information, and then provide said relevant information to the AI system.\n",
    "\n",
    "Our goal is to leverage the [Reuters News dataset](https://huggingface.co/datasets/ucirvine/reuters21578) and answer some questions around the [JAL airplane crash](https://en.wikipedia.org/wiki/Japan_Air_Lines_Flight_123). Japan Air Lines Flight 123 was a 1985 flight which left Tokyo towards Osaka. Initially it was unclear why the airplane crashed, and various theories emerged over time. We'll try to leverage RAG and an LLM to find out more about the root cause.\n",
    "\n",
    "![JAL airplane](assets/jal_airplane.png \"JAL airplane\")\n",
    "\n",
    "We will create a RAG database as follows:\n",
    "- Fetch the Reuters news dataset from Hugging Face\n",
    "- Do some data preprocessing\n",
    "- Compute an embedding for each news article. An embedding is a high dimensional vector.\n",
    "- Implement a search function, which takes in a question, and returns a news article which contains the answer to the question.\n",
    "\n",
    "The Q&A pipeline works as follows:\n",
    "- The user can ask any news-related question\n",
    "- The question gets embedded to a high dimension vector\n",
    "- Search for related news articles in the RAG database. For this purpose, compare the embedding of the question to the embeddings of all news articles.\n",
    "- Take both the user question and the most relevant news articles, put them both in a prompt, and query an LLM (large language model).\n",
    "- The LLM will answer the user question in text form, and the answer will be grounded in facts from the news database.\n",
    "\n",
    "![Question answering](assets/question_answering.png \"A question answering system using RAG\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f54e2f0f",
   "metadata": {},
   "source": [
    "## Instructions for workshop participants\n",
    "\n",
    "Ensure you understand the project description above. If you have any questions, reach out to a workshop host.\n",
    "\n",
    "Next, we start with the implementation of the RAG system. Make sure you understand the content of each notebook cell, and execute one cell after another.\n",
    "\n",
    "In some places, there are open tasks that you should work on. There tasks are marked as follows:\n",
    "\n",
    "```\n",
    "# >>>>>>>>>>>>>>>>>>\n",
    "# TODO: <Some instructions ...>\n",
    "# <your code should go here>\n",
    "# <<<<<<<<<<<<<<<<<<\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ad5a23c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install necessary dependencies\n",
    "\n",
    "!pip install datasets==3.5.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdbd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all necessary imports\n",
    "\n",
    "import math\n",
    "import tqdm\n",
    "from typing import List\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from google import genai\n",
    "from google.genai import types"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccc36c46c1691c",
   "metadata": {},
   "source": [
    "## Setup function to call Google AI Studio\n",
    "\n",
    "Next, we prepare the Python code to call the LLM API of Google. Make sure you have your API key ready, as explained in `sessions/1_prompt_engineering_and_rag/README.md`.\n",
    "\n",
    "We use the [GenAI library from Google](https://pypi.org/project/google-genai/) for this purpose."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "505d8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up LLM API key\n",
    "# >>>>>>>>>>>>>>>>>>\n",
    "# TODO: add your LLM API key here. You can get your key from Google AI Studio.\n",
    "# Note: More instructions for creating the API key can be found in `sessions/1_prompt_engineering_and_rag/README.md`\n",
    "google_llm_api_key: str = \"<key goes here>\"\n",
    "# <<<<<<<<<<<<<<<<<<"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Prepare client for Google LLM API\n",
    "client = genai.Client(api_key=google_llm_api_key)\n",
    "\n",
    "\n",
    "def llm_generate_response(system_message: str, user_message: str, print_prompt: bool = False) -> str:\n",
    "    \"\"\"\n",
    "    Use an LLM to answer a question.\n",
    "\n",
    "    :param system_message: The general instructions for the LLM, which shapes the AI's general behavior.\n",
    "    :param user_message: The question from the user.\n",
    "    :return: The response from the LLM.\n",
    "    \"\"\"\n",
    "    if print_prompt:\n",
    "        print(\"=================\")\n",
    "        print(\"Prompt for LLM:\")\n",
    "        print(f\"[System message]: [{system_message}]\")\n",
    "        print(f\"[User message]: [{user_message}]\")\n",
    "        print(\"=================\")\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-preview-04-17\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_message),\n",
    "        contents=user_message\n",
    "    )\n",
    "    return response.text\n",
    "\n",
    "\n",
    "def llm_generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    \"\"\"\n",
    "    Use an LLM to generate embeddings for news articles.\n",
    "    :param texts: A list of news articles to generate embeddings for.\n",
    "    :return: A list of embeddings for the news articles.\n",
    "    \"\"\"\n",
    "    response = client.models.embed_content(\n",
    "        model=\"text-embedding-004\",\n",
    "        contents=texts,\n",
    "        config=types.EmbedContentConfig(\n",
    "            output_dimensionality=128\n",
    "        )\n",
    "    )\n",
    "    return [np.array(embedding.values) for embedding in response.embeddings]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978269eb4f5465b",
   "metadata": {},
   "source": [
    "## Load Reuters dataset\n",
    "\n",
    "Throughout this notebook, we'll be using the [Reuters news dataset](https://huggingface.co/datasets/ucirvine/reuters21578) from Hugging Face.\n",
    "We download it below. This dataset contains short articles from Reuters' financial newswire service from 1987. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0752f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# - if asked to run custom code, type \"y\" for YES.\n",
    "reuters_ds = load_dataset('ucirvine/reuters21578','ModHayes')\n",
    "news_raw = reuters_ds[\"train\"].to_pandas()\n",
    "print(f\"Loaded {len(news_raw)} news articles.\")\n",
    "news_raw.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367662ece741e082",
   "metadata": {},
   "source": [
    "## Preprocess news articles\n",
    "\n",
    "First we perform some preprocessing on the news data. We'll store all articles in a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html).\n",
    "\n",
    "We do the following data processing:\n",
    "- Concatenate the news text with the news title\n",
    "- Remove unwanted characters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6484bcb778181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge title and text, drop unnecessary columns\n",
    "news_raw[\"title_and_text\"] = news_raw['title'] + ' | ' + news_raw['text']\n",
    "news = news_raw[[\"title_and_text\", \"date\", \"places\"]]\n",
    "\n",
    "# Clean up text, remove unnecessary characters\n",
    "pd.options.mode.chained_assignment = None\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\n\", \" \"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\\\\"\", \"\\\"\"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: \" \".join(x[\"title_and_text\"].split()), axis=1)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163dbf0d26628f5f",
   "metadata": {},
   "source": [
    "## Semantic embedding\n",
    " \n",
    "Next, we embed each news article using an embedding model from Google AI Studio.\n",
    "\n",
    "After this step, we'll have all of the following data:\n",
    "- `texts_to_encode`: A list of news articles\n",
    "- `semantic_embeddings`: A list of embeddings. Each embedding is a high dimensional vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ecca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo for embedding 1 news article\n",
    "news_samples = news[\"title_and_text\"].iloc[0]\n",
    "article_embedding = llm_generate_embeddings([news_samples])\n",
    "print(f\"News article: {news_samples},\\nEmbedding vector: {article_embedding[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Next, we will generate embeddings for all news articles in the dataset.\n",
    "# Expect some delay here, please give it ~4 minutes\n",
    "# Since the LLM API has an input size limit, we will process the news articles in batches.\n",
    "\n",
    "# Get the content of each news article (title + text)\n",
    "texts_to_encode = news['title_and_text'].to_list()\n",
    "batch_size = 100\n",
    "# Keep all embedding results in this list\n",
    "semantic_embeddings = []\n",
    "# Calculate the number of batches needed\n",
    "num_batches = math.ceil(len(texts_to_encode)/batch_size)\n",
    "# Process each batch\n",
    "for i in tqdm.tqdm(range(num_batches)):\n",
    "    # Get a batch of news articles\n",
    "    news_articles_this_batch = texts_to_encode[i * batch_size:(i+1) * batch_size]\n",
    "\n",
    "    # >>>>>>>>>>>>>>>>>>\n",
    "    # TODO: Call the LLM API to generate embeddings for this batch\n",
    "    # Note: you can use the previously defined function `llm_generate_embeddings()`\n",
    "    batch_embeddings = llm_generate_embeddings(news_articles_this_batch)\n",
    "    # <<<<<<<<<<<<<<<<<<\n",
    "    \n",
    "    # Keep a list of the embeddings from ALL batches\n",
    "    semantic_embeddings.extend(batch_embeddings)\n",
    "# Verify the final result\n",
    "assert len(semantic_embeddings) == len(texts_to_encode)\n",
    "assert semantic_embeddings[0].shape == (128,)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0f72bd7d7a63b",
   "metadata": {},
   "source": [
    "## Search in the embedding space\n",
    "\n",
    "Next, we implement a function `semantic_search()` which can find the most relevant news articles for a given query.\n",
    "The function should return the best articles with the highest cosine similarity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fc448e532e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str,\n",
    "                    top_k: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform semantic search for a given query.\n",
    "    \n",
    "    :param query: The question we'll try to answer. We use the question to search for relevant news articles.  \n",
    "    :param top_k: Search for the top_k most suitable news articles\n",
    "    :return: A pandas DataFrame with the most similar article texts and their respective semantic scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Parepare new DataFrame for the results\n",
    "    results = news.copy()\n",
    "\n",
    "    # >>>>>>>>>>>>>>>>>>\n",
    "    # TODO: Call the LLM API to generate the embedding for this query\n",
    "    # Note: you can use the previously defined function `llm_generate_embeddings()`\n",
    "    # Note: Store the result in `query_embedding`, which contains 1 embedding.\n",
    "    #       The embedding itself is a numpy array of dimension (1, number of embedding features).\n",
    "    query_embedding = llm_generate_embeddings([query])\n",
    "    # <<<<<<<<<<<<<<<<<<\n",
    "\n",
    "    # Next, we need to compare the embedding of the query with the embeddings of all news articles.\n",
    "    # We will use the cosine similarity for this purpose.\n",
    "    # The resulting Numpy array should have dimensions (number of news articles, )\n",
    "    semantic_similarities = cosine_similarity(\n",
    "        query_embedding,\n",
    "        semantic_embeddings\n",
    "    )[0].tolist()\n",
    "    # Add semantic similarity score as column to the DataFrame\n",
    "    results['semantic_score'] = semantic_similarities\n",
    "    # Get indices of top-k results\n",
    "    top_k_indices = np.argsort(semantic_similarities)[-top_k:][::-1]\n",
    "    # Only keep top-k results\n",
    "    results = results.iloc[top_k_indices]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b732f430f062812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for an article related to the JAL airplane crash\n",
    "\n",
    "query = \"What caused the crash of the JAL plane?\"\n",
    "search_results = semantic_search(query=query, top_k=5)\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ec421fd",
   "metadata": {},
   "source": [
    "## TODO: Review the semantic search results\n",
    "\n",
    "Take a look at the 5 top search results. Which of them are really relevant to the query? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae13e78",
   "metadata": {},
   "source": [
    "## Retrieval augmented generation: 1-shot RAG\n",
    "\n",
    "Next, we combine our search functionality and the text generation capability of LLMs to answer the user question. We ground the LLM in facts, by providing relevant news articles in the context window.  \n",
    "\n",
    "Here, we use 1-shot RAG, which means we provide the top-1 news article as context to the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6212585be7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function for RAG (retrieval-augmented generation)\n",
    "def answer_news_question(question: str, relevant_news: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    :param question: The user question about the news articles\n",
    "    :param context: A list of relevant news articles which should help to answer the question\n",
    "    \n",
    "    :return: The response generated by the LLM.\n",
    "    \"\"\"\n",
    "    user_message = question\n",
    "    system_message = f\"You are an assistant specialized in answering questions about news. Answer the question provided by the user as requested based on the provided articles. The answer should start with a one-sentence summary, then go into more details for about 5 sentences. The related news are here: {relevant_news}.\"\n",
    "    return llm_generate_response(system_message, user_message, print_prompt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe695f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform RAG with 1 news article as context\n",
    "# First, grab the top-1 news article\n",
    "relevant_news = search_results.iloc[0][\"title_and_text\"]\n",
    "# Next, we prepare the question\n",
    "question = query\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>\n",
    "# TODO: Take the user question and the relevant news articles, and query an LLM to generate a response\n",
    "# Note: you can use the previously defined function `answer_news_question()`\n",
    "llm_response = answer_news_question(\n",
    "    question=question,\n",
    "    relevant_news=[relevant_news]\n",
    ")\n",
    "# <<<<<<<<<<<<<<<<<<\n",
    "\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8480cca2",
   "metadata": {},
   "source": [
    "## TODO: Investigate the result, is it grounded in truth?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1caae",
   "metadata": {},
   "source": [
    "## Retrieval augmented generation: 5-shot RAG\n",
    "\n",
    "Next, we want to improve the answer to our question. Above, the LLM was only grounded using 1 news article. This limits the factual details for the LLM to give an extensive reply.\n",
    "We now switch to using the top-5 news articles, and add them to the context windows for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform few-shot RAG, leveraging 5 news articles\n",
    "# First, grab the top-5 news article\n",
    "relevant_news = search_results.iloc[:5][\"title_and_text\"]\n",
    "# Next, prepare the question\n",
    "question = query\n",
    "# Finally, query the LLM with relevant context\n",
    "llm_response = answer_news_question(question, relevant_news)\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b648cbab",
   "metadata": {},
   "source": [
    "## TODO: Investigate the 5-shot result, is it better than the 1-shot result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85faac1ae1473a48",
   "metadata": {},
   "source": [
    "## Limitations of semantic search\n",
    "\n",
    "We managed to find the root cause of the JAL airplane crash. Now we switch to a new topic.\n",
    "\n",
    "We now increase the difficulty of the question. The new question goes as follows:\n",
    "> \"What are politicians planning for Marne-la-Vallee?\"\n",
    "\n",
    "The new query contains a very specific geographical location. Semantic search can fail in such circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e85bfde86d598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo of a semantic search that fails\n",
    "\n",
    "query = \"What are politicians planning for Marne-la-Vallee?\"\n",
    "search_results = semantic_search(query=query, top_k=3)\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112f3b3d",
   "metadata": {},
   "source": [
    "## TODO: Investigate all 3 top results.\n",
    "Do the results contain plans by politicians for Marne-la-Vallee?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950edfa7c4c8ad3a",
   "metadata": {},
   "source": [
    "## Answer generation without relevant facts\n",
    "\n",
    "Next, we test RAG for the use case where the LLM is not grounded in facts.\n",
    "As we've seen above, the top-rated articles are not answering the user question. We want to investigate how the LLM reacts when it doesn't have enough information.\n",
    "Until some months ago, most LLMs would start hallucinating in this situation.\n",
    "Nowadays, LLMs are becoming better and refuse to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdedff0f279a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform few-shot RAG for the case where semantic search fails\n",
    "# Grab the 3 most relevant news articles from \"search_results\".\n",
    "relevant_news = search_results.iloc[:3][\"title_and_text\"]\n",
    "# Next, prepare the question\n",
    "question = query\n",
    "# Finally, query the LLM with relevant context\n",
    "llm_response = answer_news_question(question, relevant_news)\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2edbf7",
   "metadata": {},
   "source": [
    "## TODO: Investigate the results.\n",
    "\n",
    "Does the LLM try to answer the question, even without up-to-date information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c275001",
   "metadata": {},
   "source": [
    "# Hybrid search: use semantics plus word frequencies\n",
    "\n",
    "As seen above, some questions are harder to answer than others. When asking about the construction of a new town called [Marne-la-Vallée](https://en.wikipedia.org/wiki/Val_d%27Europe) by Walt Disney in France,\n",
    "the semantic search fails. To improve the search, we'll use a combination of semantics plus word frequencies ([TF-IDF](https://en.wikipedia.org/wiki/Tf–idf)).\n",
    "\n",
    "As an FYI, Marne-la-Vallée was a joint project between the French government and the Walt Disney Company. The project started 1987 and included six municipalities, a Disneyland Park, and a shopping center.\n",
    "\n",
    "![Marne-la-Vallee](assets/marne_la_vallee.png \"Marne-la-Vallee\")\n",
    "\n",
    "As mentioned, we will improve the news article search by leveraging TF-IDF. With the new data pipeline, the search algorithm will work as follows:\n",
    "- Embed both the news articles and the user question. Find the news articles with the most similar embedding.\n",
    "- Encode the news articles and the question with TF-IDF. Again, find the best news articles matches, this time based on TF-IDF encodings.\n",
    "- Each of the above encoding methods yields a similarity rank for every news articles.\n",
    "- Use [Reciprocal rank fusion](https://dl.acm.org/doi/abs/10.1145/1571941.1572114) (RRF) to merge the two ranks into one final rank.\n",
    "\n",
    "![Hybrid search](assets/hybrid_search.png \"Search with sentence transformer plus TF-IDF\")\n",
    "\n",
    "As we shall see, the new search mechanism will find better results for the following question:\n",
    "\n",
    "> What are politicians planning for Marne-la-Vallee?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba7dc3bb273aaca",
   "metadata": {},
   "source": [
    "## Encode news articles with TF-IDF\n",
    "\n",
    "We'll compute word frequencies for every news article, leveraging [scikit-learns's TF-IDF implementation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c808d43bcfbf31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TF-IDF object\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True, stop_words=\"english\"\n",
    ")\n",
    "# Compute TF-IDF encodings for every news article\n",
    "tfidf_corpus = tfidf_vectorizer.fit_transform(texts_to_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289389b9dd15c87",
   "metadata": {},
   "source": [
    "## Hybrid search\n",
    "\n",
    "We now combine semantic search with TF-IDF similarity. Let's create a function which performs a lookup in our news database.\n",
    "The function should use \"Reciprocal rank fusion\" for combining the semantic ranks and the word frequency ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b32292a685b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str, top_k: int, rrf_k = 60.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform a hybrid search, using both semantics and word frequencies.\n",
    "    \n",
    "    :param query: The question we'll try to answer. We use the question to search for relevant news articles.  \n",
    "    :param top_k: Search for the top k most suitable news articles\n",
    "    :param rrf_k: A hyper-parameter for Reciprocal rank fusion\n",
    "    :return: A pandas dataframe with the most relevant news articles and their RRF rank\n",
    "    \"\"\"\n",
    "\n",
    "    # Write results to new DataFrame\n",
    "    results = news.copy()\n",
    "    # Encode query and compute cosine score for semantic similarities\n",
    "    # Note: You have already implemented this in \"semantic_search()\". You can copy your code here.\n",
    "    query_embedding = llm_generate_embeddings([query])[0]\n",
    "    semantic_similarities = cosine_similarity(\n",
    "        semantic_embeddings,\n",
    "        [query_embedding]\n",
    "    )\n",
    "    # Add semantic similarity score as column to the DataFrame\n",
    "    results['semantic_score'] = semantic_similarities\n",
    "    \n",
    "    # Compute TF-IDF encoding of query\n",
    "    # Note: you've already encoded the news articles with TF-IDF, do the same here for the query\n",
    "    tfidf_encoding = tfidf_vectorizer.transform([query])\n",
    "    # Compute cosine similarities, this time for TF-IDF encodings\n",
    "    # The comparison should happend between \"tfidf_corpus\" and \"tfidf_similarities\"\n",
    "    tfidf_similarities = cosine_similarity(tfidf_corpus, tfidf_encoding)\n",
    "    results['tfidf_score'] = tfidf_similarities\n",
    "\n",
    "    # Compute the semantic and TF-IDF ranks.\n",
    "    # Note: Ranks start at 1, which is the best rank\n",
    "    semantic_ranks = np.argsort(-semantic_similarities.ravel()).argsort() + 1\n",
    "    tfidf_ranks = np.argsort(-tfidf_similarities.ravel()).argsort() + 1\n",
    "    # Calculate RRF ranks, which combine the semantic and word frequency rank\n",
    "    # Use the formula from today's presentation for the RRF rank.\n",
    "    # Note: higher means better.\n",
    "    rrf_rank = (1 / (semantic_ranks + rrf_k) + 1 / (tfidf_ranks + rrf_k))\n",
    "    results['rrf_rank'] = rrf_rank\n",
    "\n",
    "    # Get top-k results\n",
    "    top_k_indices = np.argsort(-rrf_rank)[:top_k]\n",
    "    results = results.iloc[top_k_indices]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89edeec0086394de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the hybrid search\n",
    "\n",
    "top_k = 3\n",
    "query = \"What are politicians planning for Marne-la-Vallee?\"\n",
    "\n",
    "# >>>>>>>>>>>>>>>>>>\n",
    "# TODO: Use hybrid search to find relevant information about \"Marne-la-Vallee\"\n",
    "# Note: you can use the previously defined function `hybrid_search()`\n",
    "search_results = hybrid_search(query=query, top_k=top_k)\n",
    "# <<<<<<<<<<<<<<<<<<\n",
    "\n",
    "# Show the top-3 results\n",
    "search_results.head(3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d919fc92cd04dc",
   "metadata": {},
   "source": [
    "## TODO: Analyze the search results\n",
    "\n",
    "Do the hybrid search results contain more relevant information about \"Marne-la-Vallee\"? Are there some irrelevant articles in the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9379d21b87dbdf",
   "metadata": {},
   "source": [
    "## Retrieval augmented generation, this time with hybrid search\n",
    "\n",
    "Let's generate an answer to our question:\n",
    "> \"What are politicians planning for Marne-la-Vallee?\"\n",
    "\n",
    "This time, we use hybrid search to find relevant entries in our knowledge database. By combining semantic search with TF-IDF, we achieve better results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdd0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# >>>>>>>>>>>>>>>>>>\n",
    "# TODO: Leverage relevant articles from hybrid search for RAG. Add the articles to the LLM context window, and let the LLM answer the question about Marne-la-Vallee.\n",
    "# First, grab the top-3 news articles\n",
    "# Note: we have already performed hybrid search, and have the results ready in \"search_results\"\n",
    "relevant_news = search_results.iloc[:3][\"title_and_text\"]\n",
    "# Define the question\n",
    "question = query\n",
    "# Finally, query the LLM with relevant context\n",
    "# Note: you can use the previously defined function `answer_news_question()`\n",
    "llm_response = answer_news_question(\n",
    "    question=question,\n",
    "    relevant_news=relevant_news\n",
    ")\n",
    "# <<<<<<<<<<<<<<<<<<\n",
    "\n",
    "# Show the RAG result\n",
    "print(llm_response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b572cc",
   "metadata": {},
   "source": [
    "## TODO: Analyze the RAG result from hybrid search.\n",
    "Is the LLM able to answer the question factually? Does the LLM manage to ignore irrelevant news articles?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f7741639c5561",
   "metadata": {},
   "source": [
    "## TODO: Add further improvements to the search algorithm\n",
    "\n",
    "Can you think of any additional ways for improving our RAG pipeline? Here are some ideas:\n",
    "- Give more weight to news **titles** than **texts**\n",
    "- Leverage article **release dates**, put more weight on the most recent article\n",
    "- ... any other ideas?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
