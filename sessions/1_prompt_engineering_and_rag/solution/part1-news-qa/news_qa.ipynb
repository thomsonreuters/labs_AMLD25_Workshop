{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b8262a3c0617ed",
   "metadata": {},
   "source": [
    "## Create a RAG-based question answering system\n",
    "\n",
    "In this Notebook, we will create a RAG-based Q&A system.\n",
    "\n",
    "Our goal is to leverage the [Reuters News dataset](https://huggingface.co/datasets/ucirvine/reuters21578) and answer some questions around the [JAL airplane crash](https://en.wikipedia.org/wiki/Japan_Air_Lines_Flight_123). Japan Air Lines Flight 123 was a 1985 flight which left Tokyo towards Osaka. Initially it was unclear why the airplane crashed, and various theories emerged over time. We'll try to leverage RAG and an LLM to find out more about the root cause.\n",
    "\n",
    "![JAL airplane](assets/jal_airplane.png \"JAL airplane\")\n",
    "\n",
    "We will create a RAG database as follows:\n",
    "- Fetch the Reuters news dataset from Hugging Face\n",
    "- Do some data preprocessing and cleaning\n",
    "- Embed each news article using a sentence transformer\n",
    "- Implement a search function with cosine similarity\n",
    "\n",
    "The Q&A pipeline works as follows:\n",
    "- The user can ask any news-related question\n",
    "- The question gets embedded with the same sentence transformer as above\n",
    "- Find the news article most similar to the question\n",
    "- Query an LLM with the user question, and provide relevant news article in the context window\n",
    "\n",
    "![Question answering](assets/question_answering.png \"A question answering system using RAG\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cdbd233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Do all necessary imports\n",
    "\n",
    "from datasets import load_dataset\n",
    "import numpy as np\n",
    "from typing import List\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from google import genai\n",
    "from google.genai import types\n",
    "import math\n",
    "import tqdm\n",
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccc36c46c1691c",
   "metadata": {},
   "source": [
    "## Setup LLM client for Google AI Studio\n",
    "\n",
    "<some description>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1a2ed9d3068323",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepare LLM client\n",
    "env_file_path = Path('../.env')\n",
    "load_dotenv(dotenv_path=env_file_path)\n",
    "google_llm_api_key = os.environ.get('GOOGLE_LLM_API_KEY')\n",
    "client = genai.Client(api_key=google_llm_api_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e6aaae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function for running LLM in autoregressive mode\n",
    "def llm_generate_response(user_message: str, system_message: str) -> str:\n",
    "\n",
    "    response = client.models.generate_content(\n",
    "        model=\"gemini-2.5-flash-preview-04-17\",\n",
    "        config=types.GenerateContentConfig(\n",
    "            system_instruction=system_message),\n",
    "        contents=user_message\n",
    "    )\n",
    "    return response.text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acc1787b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to get the embeddings\n",
    "\n",
    "def llm_generate_embeddings(texts: List[str]) -> List[np.ndarray]:\n",
    "    response = client.models.embed_content(\n",
    "        model=\"text-embedding-004\",\n",
    "        contents=texts,\n",
    "        config=types.EmbedContentConfig(\n",
    "            output_dimensionality=128\n",
    "        )\n",
    "    )\n",
    "    return [np.array(embedding.values) for embedding in response.embeddings]\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e978269eb4f5465b",
   "metadata": {},
   "source": [
    "## Load Reuters dataset\n",
    "\n",
    "Throughout this notebook, we'll be using the [Reuters news dataset](https://huggingface.co/datasets/ucirvine/reuters21578) from Hugging Face.\n",
    "We download it below. This dataset contains short articles from Reuters' financial newswire service from 1987. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0752f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load dataset\n",
    "# - if asked to run custom code, type \"y\" for YES.\n",
    "reuters_ds = load_dataset('ucirvine/reuters21578','ModHayes')\n",
    "news_raw = reuters_ds[\"train\"].to_pandas()\n",
    "print(f\"Loaded {len(news_raw)} news articles.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "367662ece741e082",
   "metadata": {},
   "source": [
    "## Preprocess news articles\n",
    "\n",
    "First we perform some preprocessing on the news data. We'll store all articles in a [pandas DataFrame](https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.html). For each article, we keep the actual news text, plus the news title. On the resulting strings, we remove unwanted characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf6484bcb778181",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Merge title and text, drop unnecessary columns\n",
    "news_raw[\"title_and_text\"] = news_raw['title'] + ' | ' + news_raw['text']\n",
    "news = news_raw[[\"title_and_text\", \"date\", \"places\"]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50459df97e9ee19c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up text, remove unnecessary characters\n",
    "pd.options.mode.chained_assignment = None\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\n\", \" \"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\\\\"\", \"\\\"\"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: \" \".join(x[\"title_and_text\"].split()), axis=1)\n",
    "news.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "163dbf0d26628f5f",
   "metadata": {},
   "source": [
    "## Semantic embedding\n",
    " \n",
    "Next, we embed each news article using an embedding model from Google AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3ecca48",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo for embedding one news article\n",
    "news_samples = [news[\"title_and_text\"].iloc[0]]\n",
    "article_embedding = llm_generate_embeddings(news_samples)\n",
    "print(f\"Embedding for article 0: {article_embedding[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2bc1970",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed all news articles\n",
    "# Expect some delay here, please give it ~4 minutes\n",
    "\n",
    "texts_to_encode = news['title_and_text'].to_list()\n",
    "# We need to use batching, as the LLM requests have a size limit\n",
    "batch_size = 100\n",
    "semantic_embeddings = []\n",
    "# Calculate the number of batches needed\n",
    "num_batches = math.ceil(len(texts_to_encode)/batch_size)\n",
    "# Process each batch\n",
    "for i in tqdm.tqdm(range(num_batches)):\n",
    "    current_batch = texts_to_encode[i * batch_size:(i+1) * batch_size]\n",
    "    batch_embeddings = llm_generate_embeddings(current_batch)\n",
    "    semantic_embeddings.extend(batch_embeddings)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbf0f72bd7d7a63b",
   "metadata": {},
   "source": [
    "## Search in the embedding space\n",
    "\n",
    "Next, we implement a function `semantic_search()` which can find the most relevant news articles for a given query.\n",
    "The function should return the best articles with the highest cosine similarity.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723fc448e532e416",
   "metadata": {},
   "outputs": [],
   "source": [
    "def semantic_search(query: str,\n",
    "                    top_k: int) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform semantic search for a given query.\n",
    "    \n",
    "    :param query: The question we'll try to answer. We use the question to search for relevant news articles.  \n",
    "    :param top_k: Search for the top k most suitable news articles\n",
    "    :return: A pandas DataFrame with the most similar article texts and their respective semantic scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Write results to new DataFrame\n",
    "    results = news.copy()\n",
    "    # Encode the query using the same LLM as for embedding the news articles.\n",
    "    # The resulting Numpy array should have dimensions (1, number of embedding features).\n",
    "    query_embedding = llm_generate_embeddings([query])[0]\n",
    "    # Calculate cosine similarity\n",
    "    # Each vector inside \"semantic_embeddings\" should be compared to the vector \"query_embedding\".\n",
    "    # The resulting Numpy array should have dimensions (number of news articles, )\n",
    "    semantic_similarities = cosine_similarity(\n",
    "        [query_embedding],\n",
    "        semantic_embeddings\n",
    "    )[0].tolist()\n",
    "    # Add semantic similarity score as column to the DataFrame\n",
    "    results['semantic_score'] = semantic_similarities\n",
    "    # Get indices of top-k results\n",
    "    top_k_indices = np.argsort(semantic_similarities)[-top_k:][::-1]\n",
    "    # Only keep top-k results\n",
    "    results = results.iloc[top_k_indices]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b732f430f062812",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Search for an article related to the JAL airplane crash\n",
    "\n",
    "query = \"What caused the crash of the JAL plane?\"\n",
    "search_results = semantic_search(query=query, top_k=5)\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a763d0b83e7b7558",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Review the semantic search results\n",
    "# Take a look at the 5 search results. Which of them are really relevant to the query? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ae13e78",
   "metadata": {},
   "source": [
    "## Retrieval augmented generation: Combine semantic search with GenAI\n",
    "\n",
    "Use 1-shot RAG to answer the user question about the JAL airplane crash. Combine the strengths of information retrieval with the astonishing capabilities of generative artificial intelligence. Ground the LLM in facts, by providing relevant news articles in the context window.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebe6212585be7b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a helper function for RAG (retrieval-augmented generation)\n",
    "def answer_news_question(question: str, relevant_news: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    :param question: The user question about the news articles\n",
    "    :param context: A list of relevant news articles which should help to answer the question\n",
    "    \n",
    "    :return: The answer to the question\n",
    "    \"\"\"\n",
    "    user_message = question\n",
    "    system_message = f\"You are a assistant specialized in answering questions about the news. Answer the questions provided by the user as requested based on the provided articles. Provide the long form answer based on them explaining and summarizing all the details. The related news are following: {relevant_news}.\"\n",
    "    return llm_generate_response(user_message, system_message)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe695f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform retrieval augmented generation\n",
    "# 1-shot RAG: We only retrieve the top rated news article\n",
    "# First, grab the top-1 news article from the \"search_results\"\n",
    "relevant_news = search_results.iloc[0][\"title_and_text\"]\n",
    "# Next, prepare the question\n",
    "question = query\n",
    "# Finally, query the LLM with relevant context\n",
    "llm_response = answer_news_question(question, relevant_news)\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb10fd4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Investigate the result, is it grounded in truth?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a1caae",
   "metadata": {},
   "source": [
    "## Few-shot RAG\n",
    "\n",
    "Next, we want to improve the answer to our question. Above, the LLM was only grounded using 1 news article. This limits the factual details for the LLM to give an extensive reply.\n",
    "We now switch to using the top-5 news articles, and add them to the context windows for the LLM."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d186d688",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform few-shot RAG, leveraging 5 news articles\n",
    "# Grab the 5 most relevant news articles from \"search_results\".\n",
    "relevant_news = search_results.iloc[:5][\"title_and_text\"]\n",
    "# Next, prepare the question\n",
    "question = query\n",
    "# Finally, query the LLM with relevant context\n",
    "llm_response = answer_news_question(question, relevant_news)\n",
    "print(llm_response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa110af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Investigate the 5-shot result, is it better than the 1-shot result?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85faac1ae1473a48",
   "metadata": {},
   "source": [
    "## Limitations of semantic search\n",
    "\n",
    "We managed to find the root cause of the JAL airplane crash. Now we switch to a new topic.\n",
    "\n",
    "We now increase the difficulty of the question. The new question goes as follows:\n",
    "> \"What will be constructed in Marne-la-Vallee?\"\n",
    "\n",
    "The new query contains a very specific geographical location. Semantic search can fail in such circumstances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14e85bfde86d598a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Demo of a semantic search that only partially works\n",
    "\n",
    "query = \"What are politicians planning for Marne-la-Vallee?\"\n",
    "search_results = semantic_search(query=query, top_k=3)\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b30a2c19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Investigate all 3 top results. Do they contain plans by politicians for Marne-la-Vallee?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "950edfa7c4c8ad3a",
   "metadata": {},
   "source": [
    "## What happens when the LLM doesn't get relevant information\n",
    "\n",
    "Let us now investigate what happens when we perform RAG, and the top-rated articles are not answering the user question. How does the LLM react when it doesn't have enough information?\n",
    "Until some months ago, most LLMs would start hallucinating when they don't have enough information to answer a very specific question.\n",
    "Nowadays, LLMs are becoming better and can frequently spot this situation. If they do, they clarify they need more up to date information, and refuse to answer the question."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afdedff0f279a754",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform few-shot RAG for the case where semantic search fails\n",
    "# Grab the 3 most relevant news articles from \"search_results\".\n",
    "relevant_news = search_results.iloc[:3][\"title_and_text\"]\n",
    "# Next, prepare the question\n",
    "question = query\n",
    "# Finally, query the LLM with relevant context\n",
    "llm_response = answer_news_question(question, relevant_news)\n",
    "print(llm_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73094080",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Investigate the results. Does the LLM try to answer the question, even without up-to-date information?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c275001",
   "metadata": {},
   "source": [
    "# Hybrid search: use semantics plus word frequencies\n",
    "\n",
    "As seen above, some questions are harder to answer than others. When asking about the construction of a new town called [Marne-la-Vallée](https://en.wikipedia.org/wiki/Val_d%27Europe) by Walt Disney in France,\n",
    "the semantic search fails. To improve the search, we'll use a combination of semantics plus word frequencies ([TF-IDF](https://en.wikipedia.org/wiki/Tf–idf)).\n",
    "\n",
    "As an FYI, Marne-la-Vallée was a joint project between the French government and the Walt Disney Company. The project started 1987 and included six municipalities, a Disneyland Park, and a shopping center.\n",
    "\n",
    "![Marne-la-Vallee](assets/marne_la_vallee.png \"Marne-la-Vallee\")\n",
    "\n",
    "As mentioned, we will improve the news article search by leveraging TF-IDF. With the new data pipeline, the news article search will work as follows:\n",
    "- Embed the news articles and the question with a sentence transformer. Find the news articles with the most similar embedding.\n",
    "- Encode the news articles and the question with TF-IDF. Again, find the best news articles matches, this time based on TF-IDF encodings.\n",
    "- Each of the above encoding methods yields a similarity rank for every news articles.\n",
    "- Use [Reciprocal rank fusion](https://dl.acm.org/doi/abs/10.1145/1571941.1572114) (RRF) to turn the two ranks into one final rank.\n",
    "\n",
    "As we shall see, the new search mechanism will find better results for the following question:\n",
    "\n",
    "\"What will be constructed in Marne-la-Vallee?\"\n",
    "\n",
    "![Hybrid search](assets/hybrid_search.png \"Search with sentence transformer plus TF-IDF\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ba7dc3bb273aaca",
   "metadata": {},
   "source": [
    "## Encode news articles with TF-IDF\n",
    "\n",
    "We'll compute word frequencies for every news article, leveraging [scikit-learns's TF-IDF implementation](https://scikit-learn.org/stable/modules/generated/sklearn.feature_extraction.text.TfidfVectorizer.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c808d43bcfbf31f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize a TF-IDF object\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True, stop_words=\"english\"\n",
    ")\n",
    "# Compute TF-IDF encodings for every news article\n",
    "# Encode all news articles, which we stored in the variable \"texts_to_encode\".\n",
    "# Study the documentation for scikit-learns TfidfVectorizer class, if necessary.\n",
    "tfidf_corpus = tfidf_vectorizer.fit_transform(texts_to_encode)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8289389b9dd15c87",
   "metadata": {},
   "source": [
    "## Hybrid search\n",
    "\n",
    "We now combine semantic search with TF-IDF similarity. Let's create a function which performs a lookup in our news database.\n",
    "The function should use \"Reciprocal rank fusion\" for combining the semantic ranks and the word frequency ranks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3b32292a685b557",
   "metadata": {},
   "outputs": [],
   "source": [
    "def hybrid_search(query: str, top_k: int, rrf_k = 60.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform a hybrid search, using both semantics and word frequencies.\n",
    "    \n",
    "    :param query: The question we'll try to answer. We use the question to search for relevant news articles.  \n",
    "    :param top_k: Search for the top k most suitable news articles\n",
    "    :param rrf_k: A hyper-parameter for Reciprocal rank fusion\n",
    "    :return: A pandas dataframe with the most relevant news articles and their RRF rank\n",
    "    \"\"\"\n",
    "\n",
    "    # Write results to new DataFrame\n",
    "    results = news.copy()\n",
    "    # Encode query and compute cosine score for semantic similarities\n",
    "    # Note: You have already implemented this in \"semantic_search()\". You can copy your code here.\n",
    "    query_embedding = llm_generate_embeddings([query])[0]\n",
    "    semantic_similarities = cosine_similarity(\n",
    "        semantic_embeddings,\n",
    "        [query_embedding]\n",
    "    )\n",
    "    # Add semantic similarity score as column to the DataFrame\n",
    "    results['semantic_score'] = semantic_similarities\n",
    "    \n",
    "    # Compute TF-IDF encoding of query\n",
    "    # Note: you've already encoded the news articles with TF-IDF, do the same here for the query\n",
    "    tfidf_encoding = tfidf_vectorizer.transform([query])\n",
    "    # Compute cosine similarities, this time for TF-IDF encodings\n",
    "    # The comparison should happend between \"tfidf_corpus\" and \"tfidf_similarities\"\n",
    "    tfidf_similarities = cosine_similarity(tfidf_corpus, tfidf_encoding)\n",
    "    results['tfidf_score'] = tfidf_similarities\n",
    "\n",
    "    # Compute the semantic and TF-IDF ranks.\n",
    "    # Note: Ranks start at 1, which is the best rank\n",
    "    semantic_ranks = np.argsort(-semantic_similarities.ravel()).argsort() + 1\n",
    "    tfidf_ranks = np.argsort(-tfidf_similarities.ravel()).argsort() + 1\n",
    "    # Calculate RRF ranks, which combine the semantic and word frequency rank\n",
    "    # Use the formula from today's presentation for the RRF rank.\n",
    "    # Note: higher means better.\n",
    "    rrf_rank = (1 / (semantic_ranks + rrf_k) + 1 / (tfidf_ranks + rrf_k))\n",
    "    results['rrf_rank'] = rrf_rank\n",
    "\n",
    "    # Get top-k results\n",
    "    top_k_indices = np.argsort(-rrf_rank)[:top_k]\n",
    "    results = results.iloc[top_k_indices]\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89edeec0086394de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test the hybrid search\n",
    "\n",
    "top_k = 20\n",
    "query = \"What will be constructed in Marne-la-Vallee?\"\n",
    "# Run a hybrid search\n",
    "search_results = hybrid_search(query=query, top_k=top_k)\n",
    "search_results.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60d919fc92cd04dc",
   "metadata": {},
   "source": [
    "## Analyze the search results\n",
    "\n",
    "TODO: Do the new search results contain more relevant information about \"Marne-la-Vallee\"? Are there some irrelevant articles in the results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9379d21b87dbdf",
   "metadata": {},
   "source": [
    "## Perform RAG, this time with hybrid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bbdd0b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Look up relevant articles with hybrid search, add the articles to the LLM context window, and let the LLM answer the question about Marne-la-Vallee.\n",
    "# Use 5-short RAG. We already have the search results available in \"search_results\".\n",
    "\n",
    "relevant_news = search_results.iloc[:5][\"title_and_text\"]\n",
    "question = query\n",
    "llm_response = answer_news_question(question, relevant_news)\n",
    "print(llm_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eeecf041",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Analyze the LLM response. Is it able to answer the question factually? Does the LLM manage to ignore irrelevant news articles?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "482f7741639c5561",
   "metadata": {},
   "source": [
    "# Further improvements to the search algorithm\n",
    "\n",
    "Can you think of any additional ways for improving our RAG pipeline? Here are some ideas:\n",
    "- Give more weight to news **titles** than **texts**\n",
    "- Leverage article **release dates**, put more weight on the most recent article\n",
    "- Use a more sophisticated LLM for semantic embedding\n",
    "- ... any other ideas?"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
