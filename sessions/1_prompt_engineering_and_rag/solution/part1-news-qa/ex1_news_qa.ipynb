{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Part 1 - Semantic search \n",
    "\n",
    "## Create a RAG-based question answering system\n",
    "\n",
    "In this Notebook, we will create a RAG-based Q&A system.\n",
    "\n",
    "Our goal is to leverage the [Reuters News dataset](https://huggingface.co/datasets/ucirvine/reuters21578) and answer some questions around the [JAL airplane crash](https://en.wikipedia.org/wiki/Japan_Air_Lines_Flight_123). Japan Air Lines Flight 123 was a 1985 flight which left Tokyo towards Osaka. Initially it was unclear why the airplane crashed, and various theories emerged over time. We'll try to leverage RAG and an LLM to find out more about the root cause.\n",
    "\n",
    "![JAL airplane](jal_airplane.png \"JAL airplane\")\n",
    "\n",
    "We will leverage the Reuters Nets dataset to create a vector store as follows:\n",
    "- Fetch the Reuters news dataset from Huggingface\n",
    "- Do some data preprocessing and cleaning\n",
    "- Embed each article using a sentence transformer\n",
    "\n",
    "The Q&A pipeline works as follows:\n",
    "- The user can ask any news-related question\n",
    "- The question gets embedded with the same sentence transformer as above\n",
    "- Find the news article most likely to contain the answer, using cosine similarity\n",
    "- Query an LLM with the user question, the related news article, and a suitable prompt\n",
    "\n",
    "![Question answering](question_answering.png \"A question answering system using RAG\")"
   ],
   "id": "61b8262a3c0617ed"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install necessary dependencies\n",
    "# - this takes ~3 minutes, give it some patience\n",
    "# - the imports can show error messages, you can ignore them\n",
    "\n",
    "!pip install unsloth"
   ],
   "id": "a95272e1ceeba843",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "2cdbd233",
   "metadata": {},
   "source": [
    "# Do all necessary imports\n",
    "\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import PreTrainedTokenizer, PreTrainedModel\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Prepare LLM for answer generation ",
   "id": "8bccc36c46c1691c"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate unsloth model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ],
   "id": "e1a2ed9d3068323",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define function for LLM inference\n",
    "\n",
    "def llm_inference(\n",
    "        messages: List[Dict],\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer\n",
    ") -> str:\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    input_tokens = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "    input_len = len(input_tokens.tokens())\n",
    "    output_tokens = model.generate(**input_tokens)\n",
    "    output_clipped = output_tokens[:, input_len:-1]\n",
    "    result = tokenizer.batch_decode(output_clipped)\n",
    "    return result[0]"
   ],
   "id": "95afad295ccb742a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Reuters dataset\n",
    "\n",
    "Huggingface dataset: https://huggingface.co/datasets/ucirvine/reuters21578"
   ],
   "id": "e978269eb4f5465b"
  },
  {
   "cell_type": "code",
   "id": "b0752f05",
   "metadata": {},
   "source": [
    "# Load dataset from Huggingface - if asked to run custom code, type \"y\" for YES.\n",
    "reuters_ds = load_dataset('ucirvine/reuters21578','ModHayes')\n",
    "news_raw = reuters_ds[\"train\"].to_pandas()\n",
    "print(f\"Loaded {len(news_raw)} news articles.\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocess articles",
   "id": "367662ece741e082"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge title and text, drop unnecessary columns\n",
    "news_raw[\"title_and_text\"] = news_raw['title'] + ' | ' + news_raw['text']\n",
    "news = news_raw[[\"title_and_text\", \"date\", \"places\"]]"
   ],
   "id": "bf6484bcb778181",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clean up text, remove unnecessary characters\n",
    "pd.options.mode.chained_assignment = None\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\n\", \" \"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\\\\"\", \"\\\"\"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: \" \".join(x[\"title_and_text\"].split()), axis=1)\n",
    "news.head()"
   ],
   "id": "50459df97e9ee19c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Compute semantic embedding of articles using a sentence transformer",
   "id": "163dbf0d26628f5f"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get texts to encode\n",
    "texts_to_encode = news['title_and_text'].to_list()\n",
    "embedding_model = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "# Encode texts\n",
    "# - This can take ~ 1 minute, give it some patience\n",
    "print(\"Encoding news articles. This will take ~ 1 minute ...\")\n",
    "semantic_embeddings = embedding_model.encode(\n",
    "    texts_to_encode,\n",
    "    show_progress_bar=True,\n",
    "    batch_size=32,\n",
    "    normalize_embeddings=True\n",
    ")"
   ],
   "id": "aff5b9732075642",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Search by semantics",
   "id": "bbf0f72bd7d7a63b"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def semantic_search(query: str,\n",
    "                    top_k: int = 5) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform semantic search for a given query.\n",
    "    \n",
    "    :param query: The question we'll try to answer. We use the question to search for relevant news articles.  \n",
    "    :param top_k: Search for the top k most suitable news articles\n",
    "    :return: A pandas dataframe with the most similar article texts and semantic scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Write results to new DataFrame\n",
    "    news_copy = news.copy()\n",
    "    # Encode the query\n",
    "    query_embedding = embedding_model.encode(query, normalize_embeddings=True)\n",
    "    # Calculate cosine similarity - higher is better\n",
    "    semantic_similarities = np.dot(semantic_embeddings, query_embedding)\n",
    "    news_copy['semantic_score'] = semantic_similarities\n",
    "    # Get indices of top-k results\n",
    "    top_k_indices = np.argsort(semantic_similarities)[-top_k:][::-1]\n",
    "    # Only keep top-k results\n",
    "    results = news_copy.iloc[top_k_indices]\n",
    "    return results"
   ],
   "id": "723fc448e532e416",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Search for an article related to the JAL airplane crash\n",
    "\n",
    "query = \"What caused the crash of the JAL plane?\"\n",
    "search_results = semantic_search(query=query, top_k=5)\n",
    "search_results.head()"
   ],
   "id": "9b732f430f062812",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "7ae13e78",
   "metadata": {},
   "source": "## Q&A with an LLM + RAG  "
  },
  {
   "cell_type": "markdown",
   "id": "27077cf7",
   "metadata": {},
   "source": "## 1-shot RAG: Only retrieve 1 news article"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_messages(context: List[str], question: str):\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": f\"You are a assistant specialized in answering questions about the news. Answer the questions provided by the user as requested based on the provided articles. Provide the long form answer based on them explaining and summarizing all the details. The related news are following: {context}\",\n",
    "        },\n",
    "        {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "    ]\n",
    "    return messages"
   ],
   "id": "ebe6212585be7b4a",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "id": "6fe695f3",
   "metadata": {},
   "source": [
    "context = [search_results.iloc[0][\"title_and_text\"]]\n",
    "messages = create_messages(context, query)\n",
    "llm_response = llm_inference(messages, model, tokenizer)\n",
    "print(f\"LLM response:\\n{llm_response}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "e8a1caae",
   "metadata": {},
   "source": [
    "## Few-shot: Retrieve 5 news articles\n",
    "\n",
    "Now be expand the search and look for the 5 best matching news articles. We provide all of them as context when querying the LLM."
   ]
  },
  {
   "cell_type": "code",
   "id": "d186d688",
   "metadata": {},
   "source": [
    "top_k = 5\n",
    "context = list(search_results.iloc[:top_k].sort_values(by=\"date\", ascending=True)[\"title_and_text\"])\n",
    "messages = create_messages(context, query)\n",
    "llm_response = llm_inference(messages, model, tokenizer)\n",
    "print(f\"LLM response:\\n{llm_response}\")\n"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Improvements from few-shot RAG\n",
    "\n",
    "In few-short RAG, the LLM has access to more relevant news articles when compared to 1-shot RAG.\n",
    "In the answer above, what additional information do you spot in the answer?"
   ],
   "id": "75f54c64592b28dd"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## A hard question where the RAG might fail\n",
    "\n",
    "We managed to find the root cause of the JAL airplane crash. Now we switch to a new topic.\n",
    "\n",
    "We now increase the difficulty of the question. The new query contains a very specific geographical location. Semantic search can fail in such circumstances."
   ],
   "id": "85faac1ae1473a48"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Demo of a RAG that fails\n",
    "\n",
    "query = \"What will be constructed in Marne-la-Vallee?\"\n",
    "search_results = semantic_search(query=query, top_k=5)\n",
    "search_results.head()"
   ],
   "id": "14e85bfde86d598a",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analyze the results\n",
    "\n",
    "Do you see any relevant articles in the results about \"Marne-la_vallee\"?"
   ],
   "id": "b57e8c6559877951"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Ask the LLM a hard question\n",
    "\n",
    "How does the LLM react when it doesn't have enough information?\n",
    "Until some months, most LLMs would start hallucinating when they don't have enough information to answer a very specific question.\n",
    "Nowadays, LLMs become better at spotting this situation, and they clarify they need more up-to-date information."
   ],
   "id": "950edfa7c4c8ad3a"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "top_k = 5\n",
    "context = list(search_results.iloc[:top_k][\"title_and_text\"])\n",
    "messages = create_messages(context, query)\n",
    "llm_response = llm_inference(messages, model, tokenizer)\n",
    "print(f\"LLM response:\\n{llm_response}\")"
   ],
   "id": "afdedff0f279a754",
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "markdown",
   "id": "1c275001",
   "metadata": {},
   "source": [
    "# Part 2 - Hybrid search: use semantics plus word frequencies\n",
    "\n",
    "As seen above, some questions are harder to answer than others. When asking about the construction of a new town called [Marne-la-Vallée](https://en.wikipedia.org/wiki/Val_d%27Europe) by Walt Disney in France,\n",
    "the semantic search fails. To improve the search, we'll use a combination of semantics plus word frequencies [TF-IDF](https://en.wikipedia.org/wiki/Tf–idf).\n",
    "\n",
    "Marne-la-Vallée is a joint project between the French governement and the Walt Disney Company. The project started 1987 and includes six municipalities, a Disneyland Park, and a shopping center.\n",
    "\n",
    "![Marne-la-Vallee](marne_la_vallee.png \"Marne-la-Vallee\")\n",
    "\n",
    "As mentioned, we will improve the news article search by leveraging TF-IDF. With the new data pipeline, the news article search will work as follows:\n",
    "- Embed the news articles and the question with a sentence transformer. Find the news articles with the most similar embedding.\n",
    "- Encode the news articles and the question with TF-IDF. Again, find the best news articles matches, this time based on TF-IDF encodings.\n",
    "- Each of the above encoding methods yields a similarity rank for every news articles.\n",
    "- Use [Reciprocal rank fusion](https://dl.acm.org/doi/abs/10.1145/1571941.1572114) (RRF) to turn the two ranks into one final rank.\n",
    "\n",
    "As we shall see, the new search mechanism will find better results for the following question:\n",
    "\n",
    "\"What will be constructed in Marne-la-Vallee?\"\n",
    "\n",
    "![Hybrid search](hybrid_search.png \"Search with sentence transformer plus TF-IDF\")"
   ]
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Encode news articles with TF-IDF",
   "id": "5ba7dc3bb273aaca"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Compute TF-IDF vectors of new articles\n",
    "tfidf_vectorizer = TfidfVectorizer(\n",
    "    lowercase=True, stop_words=\"english\"\n",
    ")\n",
    "tfidf_corpus = tfidf_vectorizer.fit_transform(texts_to_encode)"
   ],
   "id": "3c808d43bcfbf31f",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Hybrid search\n",
    "\n",
    "Combine semantic search with TF-IDF similarity."
   ],
   "id": "8289389b9dd15c87"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def hybrid_search(query: str, top_k: int = 5, rrf_k = 60.0) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Perform a hybrid search, using both semantics and word frequencies.\n",
    "    \n",
    "    :param query: The question we'll try to answer. We use the question to search for relevant news articles.  \n",
    "    :param top_k: Search for the top k most suitable news articles\n",
    "    :param rrf_k: A hyper parameter for Reciprocal rank fusion\n",
    "    :return: A pandas dataframe with the most similar article texts and RRF scores\n",
    "    \"\"\"\n",
    "\n",
    "    # Write results to new DataFrame\n",
    "    news_copy = news.copy()\n",
    "    # Compute semantic embedding of query\n",
    "    query_embedding = embedding_model.encode(query, normalize_embeddings=True)\n",
    "    # Calculate semantic similarities - higher is better\n",
    "    semantic_similarities = cosine_similarity(semantic_embeddings, [query_embedding])\n",
    "    news_copy['semantic_score'] = semantic_similarities\n",
    "    \n",
    "    # Compute TF-IDF encoding of query\n",
    "    tfidf_query = tfidf_vectorizer.transform([query])\n",
    "    # Calculate TF-IDF similarities - higher is better\n",
    "    tfidf_similarities = cosine_similarity(tfidf_corpus, tfidf_query)\n",
    "    news_copy['tfidf_score'] = semantic_similarities\n",
    "\n",
    "    # Calculate ranks. Ranks start at 1, which is the best rank\n",
    "    semantic_ranks = np.argsort(-semantic_similarities.ravel()).argsort() + 1\n",
    "    tfidf_ranks = np.argsort(-tfidf_similarities.ravel()).argsort() + 1\n",
    "    # Calculate RRF scores - higher means better\n",
    "    rrf_scores = (1 / (semantic_ranks + rrf_k) + 1 / (tfidf_ranks + rrf_k))\n",
    "    news_copy['rrf_score'] = rrf_scores\n",
    "\n",
    "    # Get top-k results\n",
    "    top_k_indices = np.argsort(-rrf_scores)[:top_k]\n",
    "    results = news_copy.iloc[top_k_indices]\n",
    "    return results"
   ],
   "id": "d3b32292a685b557",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Example usage\n",
    "\n",
    "top_k = 5\n",
    "query = \"What will be constructed in Marne-la-Vallee?\"\n",
    "search_results = hybrid_search(query=query, top_k=top_k)\n",
    "search_results.head()"
   ],
   "id": "89edeec0086394de",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Analyze the search results\n",
    "\n",
    "Do the new search results contain more relevant information about \"Marne-la-Vallee\"?"
   ],
   "id": "60d919fc92cd04dc"
  },
  {
   "cell_type": "code",
   "id": "9bbdd0b1",
   "metadata": {},
   "source": [
    "context = list(search_results[\"title_and_text\"])\n",
    "messages = create_messages(context, query)\n",
    "llm_response = llm_inference(messages, model, tokenizer)\n",
    "print(f\"LLM response:\\n{llm_response}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# TODO: improve hybrid search\n",
    "Do you have any idea how you could improve the search further, so that more relevant results can be found for the question above?"
   ],
   "id": "482f7741639c5561"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "amld",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
