{
 "cells": [
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "# Create an AI pipeline for quiz generation, using langgraph\n",
    "\n",
    "## Overview\n",
    "\n",
    "Create a quiz about a news article, using GenAI, RAG, and LangGraph. \n",
    "\n",
    "## AI pipeline\n",
    "\n",
    "The pipeline includes:\n",
    "- The user specifies a **topic** of interest\n",
    "- Search for corresponding news articles in the **Reuters dataset**\n",
    "- The user selects 1 from the top-3 new articles\n",
    "- Generate quiz with a **prompt template**\n",
    "- The generated quiz will have an output format according to our specifications\n",
    "\n",
    "![AI pipeline](ai-pipeline.png \"AI pipeline for quiz generation\")"
   ],
   "id": "5f4b2ff659593dfc"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Install necessary dependencies\n",
    "# - this takes ~3 minutes, give it some patience\n",
    "# - the imports can show error messages, you can ignore them\n",
    "\n",
    "!pip install unsloth\n",
    "!pip install langchain\n",
    "!pip install langgraph\n",
    "!pip install langchain_huggingface"
   ],
   "id": "e722463062e67bf4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Do all necessary imports\n",
    "\n",
    "from typing import List, Dict\n",
    "from typing_extensions import TypedDict\n",
    "from langchain_core.documents import Document\n",
    "from langchain_core.vectorstores import InMemoryVectorStore\n",
    "from langgraph.types import Command, interrupt\n",
    "from langgraph.graph import START, StateGraph, END\n",
    "from langgraph.checkpoint.memory import MemorySaver\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from IPython.display import Image, display\n",
    "from datasets import load_dataset\n",
    "import pandas as pd\n",
    "from unsloth import FastLanguageModel\n",
    "from transformers import PreTrainedTokenizer, PreTrainedModel"
   ],
   "id": "a76421ffb2a0b9b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Prepare LLM for quiz generation",
   "id": "34ba6a439e1d6d65"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Instantiate unsloth model and tokenizer\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Llama-3.2-1B-Instruct-bnb-4bit\"\n",
    ")\n",
    "FastLanguageModel.for_inference(model)"
   ],
   "id": "2a5f23925a2447dc",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Define function for LLM inference\n",
    "\n",
    "def llm_inference(\n",
    "        messages: List[Dict],\n",
    "        model: PreTrainedModel,\n",
    "        tokenizer: PreTrainedTokenizer\n",
    ") -> str:\n",
    "    prompt = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=True\n",
    "    )\n",
    "    input_tokens = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n",
    "    input_len = len(input_tokens.tokens())\n",
    "    output_tokens = model.generate(**input_tokens)\n",
    "    output_clipped = output_tokens[:, input_len:-1]\n",
    "    result = tokenizer.batch_decode(output_clipped)\n",
    "    return result[0]"
   ],
   "id": "d371046bff56f49e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Define prompt template for quiz generation",
   "id": "91b8eb7abcc3381d"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "def create_messages(news_article: str) -> List[Dict]:\n",
    "    messages = [\n",
    "        {\n",
    "            \"role\": \"system\",\n",
    "            \"content\": \"\"\"Please generate one multiple choice quiz for one provided news article.\n",
    "            \n",
    "            The quiz should have the following format:\n",
    "            \n",
    "            [Question]\n",
    "            \n",
    "            [Choice 1]\n",
    "            [Choice 2]\n",
    "            [Choice 3]\n",
    "            \n",
    "            [Solution]\n",
    "            \"\"\",\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"user\",\n",
    "            \"content\": f\"Here is the news article: {news_article}\"\n",
    "        }\n",
    "    ]\n",
    "    return messages"
   ],
   "id": "ecdcc9ed9b6c63ea",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "news_article = \"U.S. Agriculture Secretary Richard Lyng said he would not agree to an extension of the 18-month whole dairy herd buyout program set to expire later this year. Speaking at the Agriculture Department to representatives of the U.S. National Cattlemen\\'s Association, Lyng said some dairymen asked the program be extended. But he said the Reagan administration, which opposed the whole herd buyout program in the 1985 farm bill, would not agree to an extension. The program begun in early 1986, is to be completed this summer. U.S. cattlemen bitterly opposed the scheme, complaining that increased dairy cow slaughter drove cattle prices down last year. Reuter\"\n",
    "messages = create_messages(news_article)\n",
    "llm_response = llm_inference(messages, model, tokenizer)\n",
    "print(f\"LLM response:\\n{llm_response}\")"
   ],
   "id": "bb9183d70f3869b6",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## TODO: Adjust prompt\n",
    "\n",
    "Try to adjust the prompt template, in order to get the following results:\n",
    "- A quiz with more multiple choice options\n",
    "- A more tricky quiz\n",
    "- ... think of another adjustment ..."
   ],
   "id": "c85a0600b0c90371"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: adjust prompt template\n",
    "# <your code goes here>"
   ],
   "id": "f2db39bef365ffd",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "## Load Reuters dataset\n",
    "\n",
    "Huggingface dataset: https://huggingface.co/datasets/ucirvine/reuters21578"
   ],
   "id": "1ddfe1dceca25493"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Load dataset from Huggingface - if asked to run custom code, type \"y\" for YES.\n",
    "reuters_ds = load_dataset('ucirvine/reuters21578','ModHayes')\n",
    "news_raw = reuters_ds[\"train\"].to_pandas()\n",
    "print(f\"Loaded {len(news_raw)} news articles.\")"
   ],
   "id": "94a44ec9bd1a3bae",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## Preprocess articles",
   "id": "118e60c1c8169d06"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Merge title and text, drop unnecessary columns\n",
    "news_raw[\"title_and_text\"] = news_raw['title'] + ' | ' + news_raw['text']\n",
    "news = news_raw[[\"title_and_text\", \"date\", \"places\"]]"
   ],
   "id": "305fba345d5aa905",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Clean up text, remove unnecessary characters\n",
    "pd.options.mode.chained_assignment = None\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\n\", \" \"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: x[\"title_and_text\"].replace(\"\\\\\\\"\", \"\\\"\"), axis=1)\n",
    "news[\"title_and_text\"] = news.apply(lambda x: \" \".join(x[\"title_and_text\"].split()), axis=1)\n",
    "news.head()\n"
   ],
   "id": "7fbc942723a24187",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Setup RAG vector store\n",
    "# - This can take ~ 1 minute, give it some patience\n",
    "texts_to_encode = news['title_and_text'].to_list()\n",
    "embedder = HuggingFaceEmbeddings(\n",
    "    model_name = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "vectorstore = InMemoryVectorStore.from_texts(texts=texts_to_encode, embedding=embedder)"
   ],
   "id": "5a3eac69e5af7e84",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Search articles about a given topic\n",
    "result = vectorstore.similarity_search(\"An article on agriculture\", k=3)\n",
    "print(result)"
   ],
   "id": "3cf224b658d7bb43",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# TODO: Search for articles on another topic",
   "id": "d22fbde18d68f752"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Search for articles on another subject\n",
    "# Potential subjects include: \"Japan\", \"Taxes\", etc.\n",
    "# < your code goes here >"
   ],
   "id": "bf6568628279da42",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Create graph with LangGraph",
   "id": "26a878970cf65cfd"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "class State(TypedDict):\n",
    "    topic: str\n",
    "    ragged_documents: List[Document]\n",
    "    selected_document: Document\n",
    "    quiz: str\n",
    "\n",
    "def retrieve(state: State):\n",
    "    retrieved_docs = vectorstore.similarity_search(state[\"topic\"], k=3)\n",
    "    return {\"ragged_documents\": retrieved_docs}\n",
    "\n",
    "def human_feedback(state: State):\n",
    "    article_selection = interrupt(\"Let user choose article\")\n",
    "    selected_document=state[\"ragged_documents\"][int(article_selection)]\n",
    "    return {\"selected_document\": selected_document}\n",
    "\n",
    "def generate(state: State):\n",
    "    news_article = state[\"selected_document\"].page_content\n",
    "    messages = create_messages(news_article)\n",
    "    llm_response = llm_inference(messages, model, tokenizer)\n",
    "    return {\"quiz\": llm_response}\n",
    "\n",
    "builder = StateGraph(State)\n",
    "builder.add_node(\"retrieve\", retrieve)\n",
    "builder.add_node(\"human_feedback\", human_feedback)\n",
    "builder.add_node(\"generate\", generate)\n",
    "builder.add_edge(START, \"retrieve\")\n",
    "builder.add_edge(\"retrieve\", \"human_feedback\")\n",
    "builder.add_edge(\"human_feedback\", \"generate\")\n",
    "builder.add_edge(\"generate\", END)\n",
    "\n",
    "memory = MemorySaver()\n",
    "graph = builder.compile(checkpointer=memory)\n",
    "thread = {\"configurable\": {\"thread_id\": \"1\"}}\n",
    "\n",
    "display(Image(graph.get_graph().draw_mermaid_png()))"
   ],
   "id": "e5598541addf00d0",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Run the graph",
   "id": "8ef9012d1c786b66"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Run the graph from the start, until user selection step\n",
    "topic = input(\"Please select your topic: \")\n",
    "initial_input = {\"topic\": topic}\n",
    "for event in graph.stream(initial_input, thread, stream_mode=\"updates\"):\n",
    "    pass\n",
    "# Display article options\n",
    "article_options=graph.get_state(config=thread).values['ragged_documents']\n",
    "print(\"Article candidates:\")\n",
    "for id, doc in enumerate(article_options):\n",
    "    content = doc.page_content\n",
    "    print(f\"[{id}] {content}\")"
   ],
   "id": "d5b39e98d03c7ae5",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Get human feedback\n",
    "human_feedback = input(\"Please select which article you'd like to use [0,1,2]: \")\n",
    "\n",
    "# Continue the graph execution\n",
    "for event in graph.stream(\n",
    "        Command(resume=human_feedback), thread, stream_mode=\"updates\"\n",
    "):\n",
    "    pass"
   ],
   "id": "76a67528accf8327",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Show final quiz\n",
    "quiz=graph.get_state(config=thread).values['quiz']\n",
    "print(quiz)"
   ],
   "id": "678c2fbcd55eaab2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Adjust quiz",
   "id": "48b2d96b86fb7942"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# TODO: Make the quiz more entertaining. Add the following:\n",
    "# The final output should include the year of the news article.\n",
    "# You'll need to go back to the data preparation step, and make sure the year is included in the metadata of the LangChain documents.\n",
    "# Additionally, show the original news article above the quiz, when displaying the quiz to the user.\n",
    "# < your code goes here >"
   ],
   "id": "c46274aca36e06a4",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
