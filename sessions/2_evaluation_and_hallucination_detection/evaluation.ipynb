{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Installation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    !pip uninstall torchvision unsloth -y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    !pip install -Uq torchvision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "    %%capture\n",
    "    !pip install unsloth\n",
    "    !pip install --force-reinstall --no-cache-dir --no-deps git+https://github.com/unslothai/unsloth.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "print(f\"{torch.__version__=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "from logging import getLogger\n",
    "\n",
    "logger = getLogger(__name__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from unsloth import FastLanguageModel\n",
    "from unsloth.chat_templates import get_chat_template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.meteor_score import meteor_score\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('wordnet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jinja2 import Template"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rouge_score import rouge_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bert_score import score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Markdown, display, HTML\n",
    "\n",
    "text = \"The quick ~~brown~~ **red** fox ~~jumps~~ **leaps** over the ~~lazy~~ **sleeping** dog\"\n",
    "display(Markdown(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_html_diff_with_background(\n",
    "    a=\"The quick brown fox jumps over the lazy dog\",\n",
    "    b=\"The quick red fox leaps over the sleeping dog\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trlabs import clean_text, extract_tag_content, create_html_diff_with_background"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from trlabs import generate_text, GenerationConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#task_config = 'ModHayes'\n",
    "#task_config = 'ModLewis'\n",
    "task_config = 'ModApte'\n",
    "print(f\"{task_config=}\")\n",
    "ds = load_dataset(\"ucirvine/reuters21578\", task_config, trust_remote_code=True)\n",
    "ds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = ds['train'].to_pandas()\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.dropna(subset=['text'], inplace=True)\n",
    "train_df.dropna(subset=['title'], inplace=True)\n",
    "train_df['cleaned_title'] = train_df['title'].fillna('').apply(clean_text)\n",
    "train_df['cleaned_text'] = train_df['text'].fillna('').apply(clean_text)\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['topics_flattened'] = train_df['topics'].apply(lambda x: '_'.join([str(item).strip() for item in x]))\n",
    "train_df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Topics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_vc = train_df['topics_flattened'].value_counts()\n",
    "topics_vc.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "topics_vc.head(n=15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_topics = [topic.strip() for topic in topics_vc.head(n=20).index.values.tolist() if topic.strip() and not \"money\" in topic]\n",
    "#top_n_topics = []\n",
    "print(f\"{top_n_topics=}\")\n",
    "len(top_n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_n_topics = ['earn', 'acq', 'crude', 'trade', 'interest', 'ship', 'grain_wheat', 'sugar', 'coffee', 'gold', 'grain_corn', 'gnp', 'cpi', 'cocoa', 'grain']\n",
    "print(f\"{top_n_topics=}\")\n",
    "len(top_n_topics)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = \"unsloth/Meta-Llama-3.1-8B-Instruct\",\n",
    "    max_seq_length = 8192,\n",
    "    load_in_4bit = True,\n",
    "    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = get_chat_template(\n",
    "    tokenizer,\n",
    "    chat_template = \"llama-3.1\",\n",
    "    mapping = {\"role\" : \"from\", \"content\" : \"value\", \"user\" : \"human\", \"assistant\" : \"gpt\"}, # ShareGPT style\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "FastLanguageModel.for_inference(model) # Enable native 2x faster inference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-01-30T12:03:50.543527Z",
     "iopub.status.busy": "2025-01-30T12:03:50.543187Z",
     "iopub.status.idle": "2025-01-30T12:03:50.547146Z",
     "shell.execute_reply": "2025-01-30T12:03:50.545931Z",
     "shell.execute_reply.started": "2025-01-30T12:03:50.543500Z"
    }
   },
   "source": [
    "### 1. Title Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_title_generation = \"[TODO]\"\n",
    "print(f\"{system_message_title_generation=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_title_generation_str = \"\"\"\n",
    "[TODO]\n",
    "\"\"\".strip()\n",
    "print(prompt_template_title_generation_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_title_generation = Template(prompt_template_title_generation_str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_topic = 'coffee'\n",
    "#target_topic = 'sugar'\n",
    "#target_topic = 'ship'\n",
    "#target_topic = 'grain_wheat'\n",
    "#target_topic = 'gold'\n",
    "print(f\"{target_topic=}\")\n",
    "s = train_df.loc[train_df['topics_flattened']==target_topic].sample()\n",
    "s.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#title = s['title'].values.tolist()[0]\n",
    "#print(title)\n",
    "print(s['title'].values.tolist()[0])\n",
    "cleaned_title = s['cleaned_title'].values.tolist()[0]\n",
    "print(cleaned_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(s['text'].values.tolist()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = s['cleaned_text'].values.tolist()[0]\n",
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_title_generation = prompt_template_title_generation.render(article=cleaned_text)\n",
    "print(prompt_title_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generated_titles = generate_text(\n",
    "    prompt_title_generation,\n",
    "    tokenizer,\n",
    "    model,\n",
    "    system_message=system_message_title_generation,\n",
    "    enable_streaming=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Reference: {cleaned_title}\")\n",
    "print(f\"Generated: {generated_titles[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Title Generation (Loop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "reference_titles = []\n",
    "generated_titles = []\n",
    "article_texts = []\n",
    "for target_topic in top_n_topics:\n",
    "    print(f\"{target_topic=}\")\n",
    "    \n",
    "    # Get one sample for this topic\n",
    "    cleaned_title = \"\"\n",
    "    cleaned_text = \"\"\n",
    "    while cleaned_title.strip() == \"\" or cleaned_text.strip() == \"\":  # Fixed comparison operators\n",
    "        s = train_df.loc[train_df['topics_flattened']==target_topic].sample()\n",
    "        cleaned_title = s['cleaned_title'].values.tolist()[0]\n",
    "        cleaned_text = s['cleaned_text'].values.tolist()[0]\n",
    "    assert cleaned_title.strip() != \"\"\n",
    "    assert cleaned_text.strip() != \"\"\n",
    "    print(f\"{cleaned_title=}\")\n",
    "    reference_titles.append(cleaned_title)\n",
    "    article_texts.append(cleaned_text)\n",
    "    \n",
    "    # Fill prompt template\n",
    "    prompt_title_generation = prompt_template_title_generation.render(article=cleaned_text)\n",
    "    \n",
    "    # Generate title\n",
    "    response = generate_text(\n",
    "        prompt_title_generation,\n",
    "        tokenizer,\n",
    "        model,\n",
    "        system_message=system_message_title_generation,\n",
    "    )\n",
    "    assert len(response) == 1\n",
    "    generated_title = response[0]\n",
    "    print(f\"{generated_title=}\")\n",
    "    generated_titles.append(generated_title)\n",
    "    print()\n",
    "\n",
    "print(len(reference_titles), len(generated_titles), len(article_texts))\n",
    "assert len(reference_titles) == len(generated_titles)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Automatic Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ROUGE, BLEU, METEOR Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = {\n",
    "    'rouge': [],\n",
    "}\n",
    "\n",
    "for gen, ref in zip(generated_titles, reference_titles):\n",
    "    # ROUGE scores\n",
    "    rouge_scores = scorer.score(gen, ref)\n",
    "    results['rouge'].append({\n",
    "        'rouge1': rouge_scores['rouge1'].fmeasure,\n",
    "        'rouge2': rouge_scores['rouge2'].fmeasure,\n",
    "        'rougeL': rouge_scores['rougeL'].fmeasure\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### BERT Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results_bert_score = {\n",
    "        'bert_score': [],\n",
    "}\n",
    "# BERTScore (computed in batch)\n",
    "P, R, F1 = score(generated_titles, reference_titles, lang='en', verbose=False)\n",
    "results_bert_score['bert_score'] = F1.numpy()\n",
    "results_bert_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\nEvaluation Results:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "for i in range(len(generated_titles)):\n",
    "    print(f\"\\nPair {i+1}:\")\n",
    "    print(f\"Reference: {reference_titles[i]}\")\n",
    "    print(f\"Generated: {generated_titles[i]}\")\n",
    "    print(\"\\nMetrics:\")\n",
    "    print(f\"ROUGE-1:   {results['rouge'][i]['rouge1']:.3f}\")\n",
    "    print(f\"ROUGE-2:   {results['rouge'][i]['rouge2']:.3f}\")\n",
    "    print(f\"ROUGE-L:   {results['rouge'][i]['rougeL']:.3f}\")\n",
    "    print(f\"BERTScore: {results_bert_score['bert_score'][i]:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate average scores\n",
    "print(\"\\nAverage Scores:\")\n",
    "print(\"-\" * 50)\n",
    "print(f\"Avg ROUGE-1:   {np.mean([r['rouge1'] for r in results['rouge']]):.3f}\")\n",
    "print(f\"Avg ROUGE-2:   {np.mean([r['rouge2'] for r in results['rouge']]):.3f}\")\n",
    "print(f\"Avg ROUGE-L:   {np.mean([r['rougeL'] for r in results['rouge']]):.3f}\")\n",
    "print(f\"Avg BERTScore: {np.mean(results_bert_score['bert_score']):.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### LLM-as-a-Judge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_title_evaluation = \"\"\"\n",
    "[TODO]\n",
    "\"\"\".strip()\n",
    "print(system_message_title_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_title_evaluation_str = \"\"\"\n",
    "[TODO]\n",
    "\"\"\".strip()\n",
    "print(prompt_template_title_evaluation_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#i = 1\n",
    "i = random.randint(0, len(generated_titles))\n",
    "print(f\"{i=}\")\n",
    "reference_title = reference_titles[i]\n",
    "print(f\"Reference: {reference_title}\")\n",
    "generated_title = generated_titles[i]\n",
    "print(f\"Generated: {generated_title}\")\n",
    "print()\n",
    "article_text = article_texts[i]\n",
    "print(article_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r_i = random.randint(0,1)\n",
    "print(f\"{r_i=}\")\n",
    "\n",
    "if r_i:\n",
    "    gold_title = \"A\"\n",
    "    title_A = reference_title\n",
    "    title_B = generated_title\n",
    "else:\n",
    "    gold_title = \"B\"\n",
    "    title_A = generated_title\n",
    "    title_B = reference_title\n",
    "\n",
    "print(f\"{gold_title=}\")\n",
    "print(f\"{title_A=}\")\n",
    "print(f\"{title_B=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_title_evaluation = Template(prompt_template_title_evaluation_str)\n",
    "prompt_title_evaluation = prompt_template_title_evaluation.render(\n",
    "    article_text=article_text,\n",
    "    title_A=title_A,\n",
    "    title_B=title_B,\n",
    ")\n",
    "print(prompt_title_evaluation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_text(prompt_title_evaluation, tokenizer, model, system_message=system_message_title_evaluation)\n",
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "judge = extract_tag_content(text=response[0], tag_name=\"classification\")\n",
    "judge = judge.strip()\n",
    "print(judge, gold_title, judge == gold_title)\n",
    "assert judge in [\"A\", \"B\"]\n",
    "if judge == \"A\":\n",
    "    print(\"Better title:\")\n",
    "    print(title_A)\n",
    "    print()\n",
    "    print(\"Worse title:\")\n",
    "    print(title_B)\n",
    "else:\n",
    "    print(\"Better title:\")\n",
    "    print(title_B)\n",
    "    print()\n",
    "    print(\"Worse title:\")\n",
    "    print(title_A)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_summary_generation = \"[TODO]\"\n",
    "print(f\"{system_message_summary_generation=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt_template_str = \"\"\"\n",
    "[TODO]\n",
    "\"\"\".strip()\n",
    "print(summary_prompt_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(top_n_topics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#target_topic = 'cocoa'\n",
    "target_topic = 'sugar'\n",
    "print(f\"{target_topic=}\")\n",
    "s = train_df.loc[train_df['topics_flattened']==target_topic].sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_title = s['cleaned_title'].values.tolist()[0]\n",
    "print(f\"{cleaned_title=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_text = s['cleaned_text'].values.tolist()[0]\n",
    "print(cleaned_text)\n",
    "assert cleaned_text.strip() != \"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_prompt_template = Template(summary_prompt_template_str)\n",
    "summary_prompt = summary_prompt_template.render(article=cleaned_text)\n",
    "print(summary_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summary = generate_text(summary_prompt, tokenizer, model, system_message=system_message_summary_generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(summary) == list:\n",
    "    assert len(summary) == 1\n",
    "    summary = summary[0]\n",
    "assert type(summary) == str\n",
    "print(summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### \"Corrupt\" Summary Generation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_summary_corruption = \"[TODO]\"\n",
    "print(f\"{system_message_summary_corruption=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_prompt_template_str = \"\"\"\n",
    "[TODO]\n",
    "\"\"\".strip()\n",
    "print(corrupt_prompt_template_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_prompt_template = Template(corrupt_prompt_template_str)\n",
    "corrupt_prompt = corrupt_prompt_template.render(summary=summary)\n",
    "print(corrupt_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corrupt_summary = generate_text(corrupt_prompt, tokenizer, model, system_message=system_message_summary_corruption)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if type(corrupt_summary) == list:\n",
    "    assert len(corrupt_summary) == 1\n",
    "    corrupt_summary = corrupt_summary[0]\n",
    "assert type(corrupt_summary) == str\n",
    "#print(corrupt_summary)\n",
    "#if \"### Summary\" in corrupt_summary:\n",
    "#    corrupt_summary = corrupt_summary.strip(\"### Summary\")\n",
    "#if \"###\" in corrupt_summary:\n",
    "#    corrupt_summary = corrupt_summary.split(\"###\")[0]\n",
    "print(corrupt_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_html_diff_with_background(summary, corrupt_summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Hallucination Detection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(cleaned_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(corrupt_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "system_message_hallucination_detection = \"[TODO]\"\n",
    "print(f\"{system_message_hallucination_detection=}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_hallucination_detection_str = \"\"\"\n",
    "[TODO]\n",
    "\"\"\".strip()\n",
    "print(prompt_template_hallucination_detection_str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_hallucination_detection = Template(prompt_template_hallucination_detection_str)\n",
    "hallucination_detection_prompt = prompt_template_hallucination_detection.render(\n",
    "    article_title=cleaned_title,\n",
    "    article_text=cleaned_text,\n",
    "    summary=summary,\n",
    ")\n",
    "print(hallucination_detection_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_text(hallucination_detection_prompt, tokenizer, model, system_message=system_message_hallucination_detection)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = extract_tag_content(text=response[0], tag_name=\"classification\")\n",
    "print(result)\n",
    "assert result in [\"TRUE\", \"FALSE\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template_hallucination_detection = Template(prompt_template_hallucination_detection_str)\n",
    "hallucination_detection_prompt = prompt_template_hallucination_detection.render(\n",
    "    article_title=cleaned_title,\n",
    "    article_text=cleaned_text,\n",
    "    summary=corrupt_summary, # <- Changed to the corrupted summary\n",
    ")\n",
    "Markdown(hallucination_detection_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = generate_text(hallucination_detection_prompt, tokenizer, model, system_message=system_message_hallucination_detection)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(response[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = extract_tag_content(text=response[0], tag_name=\"classification\")\n",
    "print(result)\n",
    "assert result in [\"TRUE\", \"FALSE\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    import re\n",
    "    import html\n",
    "    import difflib\n",
    "    from IPython.display import HTML\n",
    "    \n",
    "    def clean_text(text: str) -> str:\n",
    "        if not isinstance(text, str):\n",
    "            return text\n",
    "    \n",
    "        # Decode HTML entities (like < > &amp; etc)\n",
    "        text = html.unescape(text)\n",
    "    \n",
    "        # Remove multiple spaces and newlines\n",
    "        text = ' '.join(text.split())\n",
    "    \n",
    "        # Remove 'Reuter' variations at the end\n",
    "        text = text.replace(' Reuter', '').replace(' Reuters', '')\n",
    "    \n",
    "        # Remove any remaining multiple whitespace\n",
    "        text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    \n",
    "        return text\n",
    "    \n",
    "    def extract_tag_content(text: str, tag_name: str) -> str:\n",
    "        pattern = f\"<{tag_name}>(.*?)</{tag_name}>\"\n",
    "        match = re.search(pattern, text, re.DOTALL)\n",
    "        return match.group(1) if match else \"\"\n",
    "    \n",
    "    def create_html_diff_with_background(a: str, b: str) -> HTML:\n",
    "        \"\"\"\n",
    "        Create HTML diff between two strings with background highlighting\n",
    "        \"\"\"\n",
    "        # Create a diff object\n",
    "        d = difflib.Differ()\n",
    "        diff = list(d.compare(a.split(), b.split()))\n",
    "        \n",
    "        # Create HTML with colored backgrounds\n",
    "        html = []\n",
    "        for word in diff:\n",
    "            if word.startswith('  '):  # unchanged\n",
    "                html.append(word[2:])\n",
    "            elif word.startswith('- '):  # deletion\n",
    "                html.append(f'<span style=\"background-color: #ffb3ba\">{word[2:]}</span>')\n",
    "            elif word.startswith('+ '):  # addition\n",
    "                html.append(f'<span style=\"background-color: #88c8f7\">{word[2:]}</span>')\n",
    "        \n",
    "        return HTML(' '.join(html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "    from typing import List, Optional\n",
    "    import torch\n",
    "    from transformers import PreTrainedTokenizer, PreTrainedModel, TextStreamer\n",
    "    from logging import getLogger\n",
    "    \n",
    "    logger = getLogger(__name__)\n",
    "    \n",
    "    def generate_text(\n",
    "        prompt: str,\n",
    "        tokenizer: PreTrainedTokenizer,\n",
    "        model: PreTrainedModel,\n",
    "        config: Optional['GenerationConfig'] = None,\n",
    "        device: str = \"cuda\",\n",
    "        enable_streaming: bool = False,\n",
    "        system_message: Optional[str] = None,\n",
    "    ) -> List[str]:\n",
    "        \"\"\"\n",
    "        Generate text based on a given prompt using a pre-trained model.\n",
    "        Args:\n",
    "            prompt: Input text prompt for generation\n",
    "            tokenizer: Hugging Face tokenizer instance\n",
    "            model: Hugging Face model instance\n",
    "            config: Generation configuration parameters\n",
    "            device: Computing device (\"cuda\" or \"cpu\")\n",
    "            enable_streaming: Whether to enable text streaming\n",
    "            system_message: Optional system message to prepend to the conversation\n",
    "        Returns:\n",
    "            List of generated text responses\n",
    "        \"\"\"\n",
    "        if not prompt.strip():\n",
    "            raise ValueError(\"Prompt cannot be empty\")\n",
    "        if device == \"cuda\" and not torch.cuda.is_available():\n",
    "            raise RuntimeError(\"CUDA device requested but not available\")\n",
    "        \n",
    "        config = config or GenerationConfig()\n",
    "        \n",
    "        try:\n",
    "            # Prepare messages format\n",
    "            messages = []\n",
    "            if system_message:\n",
    "                messages.append({\"role\": \"system\", \"content\": system_message})\n",
    "            messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "            \n",
    "            # Tokenize input\n",
    "            inputs = tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=True,\n",
    "                add_generation_prompt=True,\n",
    "                return_tensors=\"pt\"\n",
    "            )\n",
    "            \n",
    "            # Rest of the code remains the same...\n",
    "            attention_mask = torch.ones_like(inputs)\n",
    "            inputs = inputs.to(device)\n",
    "            attention_mask = attention_mask.to(device)\n",
    "            \n",
    "            streamer = TextStreamer(tokenizer) if enable_streaming else None\n",
    "            \n",
    "            response = model.generate(\n",
    "                input_ids=inputs,\n",
    "                attention_mask=attention_mask,\n",
    "                streamer=streamer,\n",
    "                max_new_tokens=config.max_new_tokens,\n",
    "                use_cache=config.use_cache,\n",
    "                temperature=config.temperature,\n",
    "                do_sample=config.do_sample,\n",
    "            )\n",
    "            \n",
    "            generated_texts = [\n",
    "                tokenizer.decode(\n",
    "                    resp[inputs.shape[1]:],\n",
    "                    skip_special_tokens=True,\n",
    "                )\n",
    "                for resp in response\n",
    "            ]\n",
    "            return generated_texts\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during text generation: {str(e)}\")\n",
    "            raise\n",
    "    \n",
    "    class GenerationConfig:\n",
    "        \"\"\"Configuration for text generation parameters.\"\"\"\n",
    "        def __init__(\n",
    "            self,\n",
    "            max_new_tokens: int = 1024,\n",
    "            temperature: float = 0.0,\n",
    "            use_cache: bool = True,\n",
    "            do_sample: bool = False\n",
    "        ):\n",
    "            self.max_new_tokens = max_new_tokens\n",
    "            self.temperature = temperature\n",
    "            self.use_cache = use_cache\n",
    "            self.do_sample = do_sample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "nvidiaTeslaT4",
   "dataSources": [],
   "dockerImageVersionId": 30840,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
